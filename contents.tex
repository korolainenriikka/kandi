\chapter{Introduction}

%1st paragraph/section: context summary: why
\section{context summary, why, and motivation}
%Topic in a sentence: knowledge retention including into a system. The system does machine learning on extreme-scale data

One of the main paradigm shifts currently ongoing within machine learning is developing models that can handle evolving tasks and environments. While the literature on algorithms able to retain knowledge is increasing, systems approaches to this problem are lagging behind, and systems presented in literature systematically lack reasoning on why the design decisions presented were made.

To contribute to filling this gap, an inspection of systems approaches for enabling long model lifetimes within the context of maritime is conducted. The case in question is highly geographically distributed, voluminous in data, and does not require instant response times. The aim is to thoroughly compare and assess approaches for dealing with evolving model environments and discuss which approaches suit the case and its set of requirements. Then, it is discussed what kind of big data system design would optimally enable implementing the knowledge retention approach chosen.

In addition to presenting a system proposal for the specific case, as a conclusion is also discussed, which requirements led to which design decisions to elaborate how the insights derived in this case can be transferred to another case.

The way in which this work differs form many other system presentations is that the goal is not only to find a solution that works, but to find a solution that has the highest chance of succeeding in operation. This means that some less technical considerations are also included. These are simplicity and maturity of the solution, as these traits arguably increase the chance that a system working in theory actually works in practice.

% outline: topic, backgound summary, motivation

% things to mention:
% the paradigm shift
% lack of systems literature addressing whys
% maritime requirements
% the intention: find vesselai optimal solutions
% further intention: find applicability to other cases
% take into account: maturity, simplicity. Not only find a system that works but the one that has the highest probability of succeeding in operation

% to-adds: that maturity is a factor that ended up being the very one reason many promising-seeming approaches cannot be adopted in this case


%2nd paragraph/section: question, results, methodology
\section{research questions, results summary and methodology}

primary question: Which training and inference setup would be optimal for enabling lifelong learning / incremental learning / retraining in the specific context of vesselai?

dealing with concept drift

secondary questions:

where (location (edge, fog, cloud) & type of hardware (not exact but like supercomputer, regular computer or mini device like raspberry pi/arduino)) should the models be deployed on?

things to exclude: details on stream processing, inference system details

aspects to consider: version management in central vs edge inference, lifelong learning management, concept drift in iot: how to avoid (concept drift still fully unresearched!)

how big is the messaging overhead in a highly distributed system

do the specialities of this context, the low end to end latency requirement, very high geographical and no private data (no private from d10.1) impact something we consider so much that an ''unconventional'' design decision is justified? (privacy matters affect design a lot \cite{iotsystems})

a point of motivation from D1.1 moving beyond the state of the art section: ''the time is ripe  to  rethink  whether  cloud  computing  is  the  only  architecture  able  to  support  IoT  applications, especially  in  the  case  of  smart applications,  where  static  and  mobile  IoT  devices  will  be  widely embedded  in  infrastructures.  It  is  worth  investigating  an  overall  orchestration  of  the  computational resources  available  today  that  can  take  advantage  of  the  edge-fog-cloud  continuum'' addition: to make lifelong learning possible

methology

one point to mention / keep in mind: in comparing different systems existing only those handling some kind of geospatial sensor data are considered. In other systems the requirements already differ so much that not a lot can be derived from them. Exception to this are the mature super-big systems like the googles and facebooks, which give good lessons learned in general.


results summary

\section{Chapter roles}

% this should maybe be shortened? in many papers this section is much shorter and only outlines the highest level, ie here chapters maybe

The rest of this thesis is organized as follows:

Chapter 2 presents the necessary background knowledge to the reader. First, a general introduction into distributed sensor systems with machine learning will be given. The most important design decisions; edge-fog-or-cloud deployments and incoming data processing scheme as batch, micro-batch or stream, will be briefly presented.

A more thorough state-of-the-art inspection is given in the domain of machine learning on the methods for extending the lifetime of a model. The main challenges in this, the concept drift and the stability-plasticity-dilemma are defined, and then the existing solutions for addressing the challenges are presented.

Then follows a quick interlude into high performance computing, where the power and application requirements of the newest HPC systems are given.

Lastly, the general context for the VesselAI project will be described. The specialities of the maritime domain and the nature of the data sources will be dissected, after which the project pilots and general goals will be presented. From these the general requirements will be listed and the main challenge of the project is identified: how to organize a workflow for efficient re-training of the models.

Chapter 3 analyzes and compares the various systems approaches presented previously while taking great consideration the point of view of the case studied. The main abstract components of the system, stream processing, training workflow, and deployment scheme are each inspected for the optimal trade-offs. After this, the VesselAI system proposal is presented, alongside reasoning why these decision should be made. It is also considered whether there is enough evidence to make the presented conclusions, or if there are several good-seeming options to choose from.

The paper is concluded with discussion on if the conclusions reached with the case study are applicable to other systems with similar requirements. Especially applicability to other cases with highly voluminous data combined with a long model lifetime is emphasized.

\section{acm classification,  keyword ideas}

acm:
Information systems → Information systems applications →
Spatial-temporal systems → Data streaming (from \cite{uprctrajectorysystem})

machine learning + maritime + inference + lifelong learning

adaptive learning (not sure about the exact definition)

high performance computing

Distributed Machine Learning, Internet of Things, Training, Serving (from \cite{mliot})

title wordplay:

Sensor data systems at extreme scale / for scale

Enabling knowledge retention in a sensor-based machine learning big data system: a case study

%knowledge retention is a bad term for this because we are not really aiming for retaining knowledge but enabling the models to be accurate across an extended lifespan

\chapter{Context and Background}

\section{Knowledge retention techniques for machine learning}

\subsection{Definitions}

% this section: I should have more sources and double-check that I got the definitions correct.

% mention the types of adaptive learning earlier

% find referenve to "There are two broad ways of retaining knowledge"

In machine learning based systems, the term \textbf{knowledge retention} means extending the span of time of a machine learning model from months to years while preserving its performance. Here ''knowledge'' means the model parameters that resulted from training a machine learning model of a specific machine learning task. There are two broad ways of retaining knowledge. Firstly, one can use a model for a specific task in an environment that is shifting over time. This is called \textbf{adaptive learning} \cite{conceptdriftsurvey}. Secondly, a model can be used in multiple tasks; the knowledge gained from training for one task can be transferred to similar problem domains. This is referred to with \textbf{transfer learning} \cite{lmlsystems}.

The paper at hand is dealing with the problem of adaptive learning. However, as terms can be easily confused and misunderstood, some terms regarding transfer learning are defined next.

\textbf{Lifelong learning} is a machine learning paradigm of ''learning many tasks over a lifetime from one or more domains'' \cite{lmlsystems}. This means that the knowledge gained from one task is processed in a way that it can be applied to a related task. As of date lifelong learning is implemented for simple classification tasks, and the new tasks are usually introduced by including unseen classes of data for the testing data set \cite{lmlinneuralnets}. A new environment could also perhaps be interpreted as a new task, but as the paradigm is currently still new and tackling very simple problems \cite{lmlinneuralnets}, it is too early for its adoption to a real-world scenario with complex models.

The general challenges arising in knowledge retention are the following. Especially within transfer learning \textbf{the stability-plasticity dilemma} is central. This refers to the need of finding an optimal balance on when to retain the old knowledge (stability) and when to replace it with new insight (plasticity) in order to perform well in predicting for both the old tasks seen and the new ones emerging later. Overdoing plasticity, i. e. replacing old parameters too easily may sometimes lead to \textbf{catastrophic forgetting}, which means that the ability to predict accurately for the old tasks is lost completely or for a big part.

Within adaptive learning the central problem to address is called \textbf{concept drift}. This means, in technical terms, that ''the relation between the input data and the target variable changes over time'' \cite{conceptdriftsurvey}. This is usually the result of changes in the environment in which learning is conducted. For example, in the maritime domain, it is possible that over the years vessels' speeds increase, and as a result the old trajectory forecaster predicts systematically false future locations for vessels.

The concept drift can include in itself a need for balancing stability and plasticity, depending on the use case. In autonomous cars, for example, an adaptive learning algorithm could change its predictions based on shifts in lighting and terrain conditions, but it should still be able to operate in the conditions it has seen before the changes \cite{conceptdriftsurvey}. In contrast, in maritime, it is not necessary for the models to perform well in an environment of the past after marine traffic conditions have shifted over time.

% maybe add here the definition of feature drift: "A feature drift occurs when a subset of features becomes, or ceases to be, relevant to the learning task [10]" (from mlflorstreamingsurvey) -> feature evolution will be a rare occurence in vesselai, can be omitted? also streamminingchallenges stated that feature drift is an important problem to address!

% maybe add: also hyperparams need to evolve! \cite{mlflorstreamingsurvey}

% maybe add: there is gradual and abrupt drift \cite{streamminingchallenges} \cite{conceptdriftsurvey}

\subsection{Implementing adaptive learning}

In the following section the ways of mitigating the effects of concept drift whilst maintaining performance on old tasks, if necessary, are presented.

The ways of doing adaptive: retraining, incremental learning, online learning. Then there was the ways of preserving old data and re-feeding it to models. Then there are the ensemble learning techniques, putting models to sleep state, blind&informed adaptation, ways for feedback mechanisms

-> there are a lot of ways, many ways to classify the ways also. Make more sense of what each means and how they should be presented. Find more sources than \cite{conceptdriftsurvey} to find more insight.

definitions
 retraining: taking a new instance of the model and completely training it again from scratch
	    incremental learning: a model is trained with each batch coming in from a data source and updated after each training. a way of doing adaptive learning.
	    online learning: the model is updated with each data entry coming in from a data stream and is simultaneously constantly in operation. a way of doing adaptive learning.
	    ensemble learning: adapting through predicting with a combination of models

\section{On Big Data Systems With Machine Learning}

% maybe edit the heading to also include sensor/IoT

to add here: a high-enough abstraction / "charting of the field" of big data systems in general

also some genaral principles on bd systems. Tradeoffs is big: usually to achieve better results in some respect simplicity is sacrificed. The goal is to do complex in right places where real benefits can be received, and keep it simple elsewhere. Also point on this is that the big mature big data systems stated that simplicity or ease of use or maintainability, basically variants of simplicity, is the most imporant trait or one of the most important \cite{facebook} \cite{storm@twitter}.

the high level components, d1.2 p 32 has one good starting point

definition of a user request in this context: the human is not considered, they use the ui. user is the application making requests to the system. these might be preprocessing-related or actual human user requests

%\subsection{the Lambda and Kappa architectures}
% this is probably off topic.
% this is the content on lambda & kappa that is probably needed content-wise. only needs better writing.

\subsection{Data ingestion: reference architectures for stream processing}

the initial motivation behind lambda: to beat cap theorem's inherent need to tradeoff between consistency and availability \cite{lambdakappa}

nathan marz presenting a system that supposedly beats cap. how: treat data as immutable, emphasize importance of preparing for human error, and splitting a database into two systems, batch and speed layer. The batch layer does order of hours long preprocessing of queries, speed layer has  abstracted within itself a complex system for handling the data from the latest hours to make also that available for querying. Fault tolerance comes mainly from the fact that the speed layer is possible to empty to the batch queue and restart in case of failure or overload.

the most known criticism to lambda, a blog post by Jay Kreps \cite{questioninglambda}, acknowledged its strengths in seeing data as immutable and highlighting the possibility that data may be needed to reprocess due to changes in application code. The main points of criticism were the argument that lambda does not actually beat cap but still compromises some consistency, that deeming real-time stream processing as inherently faulty and approximate "does not make sense" and is only a fault in the current tools \cite{questioninglambda}. The post along with other sources deem the most important flaw in lambda to be the fact that maintaining and syncronizing separate code bases for batch and stream layer to be redundantly complex \cite{uber} \cite{facebook}. At the same time multiple system analyses name maintainability the most or one of the most important trait about a big data system \cite{facebook} \cite{storm@twitter}.

As an alternative approach to solve these problems, Kreps proposed a different architecture called the Kappa architecture. It is otherwise similar to lambda but only has either batch or speed layer, usually speed layer in a for of highly parallelized stream processor. This mitigates the duplication problem while keeping other strengths from lambda, but the ability of giving fast answers to queries is lost.

(\cite{D1.1} states that both lambda and kappa mitigate human errors ''by  updating  the  code  and  running  it  again  on  the historical  data '')

% problems with both lambda&kappa, structure needs reorder because this does not make sense before vesselai requirement specs

fundamental assumptions of both systems:  the need almost instant execution times is assumed \cite{lambdakappa}. This is not required in vesselai. also, not stated in the sources on lambda and kappa but still assumed: that we are able to tell before the user query what exactly is going to be queried. If in vesselai the model may change and with that the query as well, or more importantly the queries regard only a mini fraction of geographical space so preprocessing the full globe and needing a percent of the preprocessed has a potential of being a grande waste of computation in the maritime case (my judgement). for the latter point it is possible that neither lambda or kappa are good.

- all the apache systems: what they are what is each used for, a very short description

\subsection{On machine learning IoT systems: the inference paradigms}

here:

edge vs fog vs cloud

OCF is The Open Connectivity Foundation. add below + the proper source

The definition of fog computing is the following, as stated by the the OCF (in \cite{fogsurvey} reference [42]): “Fog computing is a horizontal, system-level architecture that distributes computing, storage, control and networking functions closer to the users along a cloud-to-thing continuum.” (but not only entirely to the edge). Or, put differently by \cite{fogsurvey} for a clearer mental image: "The Fog is a Cloud closer to the ground."


%\section{the evaluation dimensions}

this sect should be removed and added as a part of the introduction.

For this thesis, dimensions are defined as traits by which various systems can be evaluated. These are:

latency: the time it takes from user input until they get their desired output,

scalability: at which data volume per time unit the system faces bottlenecks,

accuracy: how true the predictions made by the system are and how much information the system is able to give its user about the predictions' accuracy.

throughput: how many user requests the system is able to handle (this trait may not be necessary)

a word for how easy something is to improve on...: multiple mature big data systems descriptions mentioned this as the most of one of the most important trait in the system \cite{uber} \cite{facebook}.

\section{High-performance Computing}

something on how fast is fast, how powerful the hpc systems are compared to laptops

what are raspberry pi and arduino and how their power compares to a standard laptop

\section{The case: context and requirements for machine learning in the maritime domain}

Maritime as a domain of application for a sensor-based machine learning system is in a few ways significantly different from many others. The most obvious difference is its global nature; domains like smart cities also have to take into account the fact that there is some geographical distribution, but with the seas covering x\% of the earths' surface, the extent of geographical distribution in the maritime context is on a whole new level.

With the vastness of the seas and its vital nature for world trade, the amount of vessel traffic is large even in comparison to other big data applications. Due to international regulation vessels have to send update signals on their status from every few minutes to every 2 seconds, depending on their speed and course \cite{maritimeinformatics}. With approximately 100, 000 ships sailing the world oceans daily (a source from \cite{maritimeinformatics} states this), this leads to several billion messages sent each day. To this exceptional amount of data we refer to as \textit{extreme-scale data} in this work. In addition to this, in order to provide valid maritime intelligence, other types of data such as geographical information and weather updates, are needed. Therefore, the data is not only highly voluminous, but also heterogeneous, coming in both static and dynamic forms at different velocities.

Added to scale, another specific of this context is speed. Depending on the size and type of the vessel, it takes from minutes to up to an hour to change the course of the vessel (todo: fact-check this). This means that for the AI services targeted for navigational purposes the end-to-end latencies need not to be faster than the order of minutes, and execution times of up to 30-60 minutes can be acceptable. For other AI services end-to-end times in order of seconds may be aimed for, but achieving sub-second latencies as is often the goal in literature (citations...), is unnecessary, even detrimental to the success of the system operation: for speed other more important traits would have to be deprioritised, which in this case would do more harm than good.

The main data source, the status signal data sent by vessels, called \textit{automatic identification system} (AIS) data, also has its specialities. AIS is a form of sensor data and has two types: static messages containing information such as name, destination and ship characteristics, and dynamic messages with information on the vessels' location, speed, heading, and rate of turn. The challenge with AIS data is both its highly fluctuating reporting intervals and especially its unreliability.  Things such as manually written destinations, faulty timestamping, lack of universal identifiers, misreported locations, and even illegal traffic camouflaging their operations, make identifying and correcting erroneous data both difficult and computationally intensive.

The goal on the application level of the VesselAI project is to provide the following four pilot services to users: vessel route forecasting for traffic monitoring and management, design of optimal ship energy systems, operating autonomous ships in short sea transport, and weather-optimized routing for long-distance voyages. More abstract goals are to find a system that is suitable for both managing extreme-scale data and facilitating long model lifetimes. The goal is also to be able to fully utilize the most modern high performance computing infrastructures in an optimal way.

With this context specification in mind, the following guidelines on the system aimed for can be stated: the high volume and relatively low quality of the data sets high demands on the training part of the pipeline, whereas the inference side has less strict requirements. The fact that the pilot cases are clearly defined and at least as of date there are no plans for additional applications, the primary challenge is maintaining and updating the models for these specific problem, not being ready to cope with the possibility of new problems being added, as is sometimes the case in other systems (find citations here...).  Summarized: the primary challenge for this system is to find a way of organising the training workflow to maintain accuracy throughout the model lifespan. This is the primary topic of inspection in the following chapters.

% TO ADD HERE: the data processed is not personal in any way and the case studied is in this sense very different from many others. dunno id d10 should be cited because it was marked confidential

% TO ADD (MAYBE): this is sensor stuff so it is kind of IoT like but it is different in many respects to what is usually thought of when talking about IoT like smart ovens and all those

% TO ADD: the data used is spatiotemporal. Kind of mentioned kind of not mentioned. Could be emphasized more if needed in an argument

% could emphasize even more not overdoing latency, not doing speed for the sake of speed because it's cool

%outline:

%maritime
%	global
%	geo distributed
%	volume is huge (ais message rates & amount of traffic: give numbers)
%	time scales: it takes minutes to an hour to turn a cargo vessel

%ais
%	what is AIS
%	a lot of features that are unreliable
%	identifying the erroneus might be very difficult: smugglers' disguising ais example

%the project
%	the pilots I-V: what each one aims for
%		these ml problems are difficult
%	abstract aims
%		deal with extreme scale data
%		facilitate continuous learning
%		utilize modern HPC
%
%the system
%	we combine different data sets
%		static and dynamic, fast and slow paced
%			examples: map data, weather data, sensor data
%	many models only need a small fraction of the data
%

%combine these: the requirements
%we need to know what we need and what we do not need for sure to know which system design makes sense.

%we need throughput, not individual request answer speed (millisecond speed unnecessary)
%pilots defined: we need to keep models up to date, not be ready to expand to new models as much
%the machine learning paradigm and workflow of choice needs to be mature enough for industry implementation
%	in a complex scenario.
%inference time windows are big. the challenge is facilitating training.

%then: present abstract illustration of the architecture and state: THIS is %the spot which we investigate.


\chapter{vesselai-like systems}

\section{facilitating knowledge retention: training approaches}

include also here: data cleaning

central vs federated learning: learning location

the approach of doing adaptive learning in a unsupervised / semi-supervised setting

Here: give the +'s and -'s of each approach. This information on how knowledge retention is likely to be applied affects the discussions on the other things in consideration

blind mechanisms

+ no need for a feedbacck system

identifying concept drift approaches

+ more accuracy compared to blind methods \cite{conceptdriftsurvey}

- a feedback mechanism needs to be in the system

online learning

- no way for abrupt adaptation, only gradual \cite{conceptdriftsurvey}

ensemble

+ a mature approach for the problem \cite{mlforstreamingsurvey}

approaches: stable&reactive models, weighted average, model sleeping \cite{conceptdriftsurvey}

data window comparisons

- old data needs to be stored and takes up lots of memory \cite{conceptdriftsurvey} -> probably not applicable

combinations of approaches that are not researched a lot -> immature

\begin{itemize}
    \item concept drift detection + delayed labels \cite{mlforstreamingsurvey}
    \item transfer learning + online learning \cite{mlforstreamingsurvey}
    \item adaptive learning + delayed labels \cite{mlforstreamingsurvey}
\end{itemize}

\section{inference paradigm}

discuss here: edge v couldlet v fog v cloud

+ and - for fog, from \cite{fogsurvey}

+'s that matter: saves bandwidth, mitigates problems with hostile areas that have fragile net connections (fog is stated to be fundamental in solving this problem), has reached a good enough maturity

+'s that do not matter: saves time (in range 66-302 ms), is somewhat more private, context awareness: able to adjust to location is probably not something the models are able to benefit from because edge training is most likely not computationally feasible

-'s: adds compexity by distribution and heterogenity: " imposes non-trivial orchestration issues", cloud is more energy efficient (fot this check [120] from \cite{fogsurvey}),

so: fog has drawbacks in complexity that are not acceptable considering that the advantages gotten with it are largely irrelevant.


\section{stream processing}

stream has compared to batch the ml pipeline steps ''interleaved'' \cite{mlforstreamingsurvey}

\section{other possible discussions to add}

\begin{itemize}
    \item where to do data cleaning - maybe too much given the amount of to-discuss here already? \cite{mlforstreamingsurvey} has a good sect on this if this will eb considered
    \item algorithm performance feedback mechanism (was presented as a thing to take into account in \cite{streamminingchallenges})
\end{itemize}

\chapter{the vesselai system}

i e the results i get

\chapter{Conclusion}

add here applicability of these conclusions to other systems.

could add here: that the results gotten could not be validated empirically as the scope and level of this thesis will not allow for a thorough experimental inspection.

also add maybe here that the literature on the topic is so vast and quickly evolving that it is possible that the subset of existing literature that I have gone through somehow conveys things in a non-objective way. This is unlikely however.
