\chapter{Introduction}
% 1st sect: GENERAL BACKGROUND: data environments evolve. data stream processing requires being accurate efficiently.
A problem occurring in nearly every application of machine learning is the challenge of handling evolving environments, a problem known as concept drift. Meanwhile, the domain of data stream processing proposes its own set of challenges, the main one being the ability of providing accurate predictions at any time despite large volumes of data and constrained time and memory resources. The combination of these two problems, the need to adapt to evolving data while meeting the systems requirements, is complex, but common.

% there are lots of solutions to this so requirements are needed to select from them
Multitudes of approaches have been proposed as a solution to the problem of handling concept drift efficiently in stream processing systems, each with their own set of strengths. Therefore, to be able to compare the applicability of these solutions, a set of requirements should be used as a starting point. A case study is presented for this purpose, and the solution is searched for given the set of identified assumptions and requirements. 

% the context intro
The thesis at hand uses as context an on-going research project, VesselAI, aiming for applying novel machine learning to the domain of maritime. The domain in question is characteristic in the especially large volume of incoming sensor data and the slowness of vessel traffic in comparison. Additional context specifics include low-quality and heterogenity of data and long delays in model feedback. 

% goal
The goal of this analysis is, given the project context, to find the most optimal system-level workflow for maintaining model accuracy. In other words, from the challenges identified in the VesselAI state-of-the-art analysis~\cite{D1.1}, the problem tackled is `ML model retraining is computationally very heavy: in environments where data evolves, it is urgent to use architectures that manage ML models in order to adapt to new data/tasks and retrain when necessary'. In addition to the suggested approach of lifelong learning, additional insight is gathered from the research fields of concept drift adaptation, deep learning optimization, MLOps and AutoML. This way the literature gap of works addressing the maritime domain and automated model adaptation identified in~\cite{D1.1} is addressed.   

% systems perspective
Traditionally, research addressing the problem of concept drift focuses on types of models able to adapt to a changing environment. This thesis can, from this perspective, also be seen as a systems approach to concept drift. If a model is taken as given, it is discussed how the system organization could make sure the model stays accurate. This perspective also aims to take into account the insight on best practices from the field of big data systems. Especially the lessons of prioritizing simplicity and maturity are taken into account when searching for a system applicable to the real world.

% 2nd section: RESEARCH Q'S, METHODOLOGY, RESULTS SUMMARY

% outline
% main question
% exact definitions of main question
% sub-questions
% note on methodology (literature based but not systematic, non-empirical)
% results summary

% comment: the sentence on the model is a little vague, tailored means that it is made for the case not just taken from somewhere else. I'm unsure what words to mention on the model here.

% research question
The main research question addressed in this work is the following:

\begin{center}
    Given the domain context and the models used, how to organize a workflow for efficient updating of the models in order to meet the demands posed by the use case?
\end{center}

As further elaboration, by updating the model it is meant that the model parameters are changed. The domain context is the maritime domain, and the model used is assumed to be a deep recurrent neural network operating in batch mode. By efficient we mean especially the ability to handle large amounts of data by optimally utilizing high performance computing resources. The main demand posed by the use case is the fact that the accuracy of the models should either stay on a high level or ideally improve, which is referred to as \textit{model maintenance} in this work.

The complementary research questions considered as implications of the primary one are the following:

\begin{itemize}
    \item To which extent is it possible to optimize the training of the models?
    \item Which type of monitoring should be in place in the system?
    \item How much human intervention is required for maintaining the updating schemes; what are the possibilities of automating these workflows?
\end{itemize}

As a note on methodology, the investigation in its entirety is based on literature. Conducting a systematic literature review would be beyond the scope of this thesis, so only material directly relevant to the specific set of requirements is taken into account. Testing the various approaches in practice is also beyond scope; the empirical investigation of these findings is left to future works. This limits both the set of options considered to those already used in the problem domain and the evaluation to analyzig the thoroughness of existing evidence in literature.

% 3rd section: CHAPTER ROLES

The rest of this thesis is organized as follows:

Chapter 2 presents the necessary background knowledge to the reader: First, the phenomenon causing the need of efficient retraining, concept drift, is presented, alongside machine learning approaches aiming to cope with the problem. Then, as context, an overview of the general organization of a big data system is presented. Lastly follows an introduction to the maritime domain and the requirements of the case. Chapter 3 analyzes ways of enabling optimal model maintenance from three points of view. These viewpoints are data management optimization,  ways of speeding up neural network training and optimally timing the updates using concept drift detectors. In addition it is discussed how certain these findings are given the quality and thoroughness of the literature used. The thesis is concluded with discussion on the applicability of the results to similar problems in different domains.

\chapter[Overview on machine learning systems for the maritime domain]{Overview on machine learning\\ systems for the maritime domain}

% ingressi: mitä aiot rakentaa luvussa, mikä tulee esille, miksi se on tärkeää, mihin tällä kokonaisuudella pyritään

The general task of big data systems is that they should be able to handle vast amounts of data and from that be able to provide predictions in a timely manner. To position our problem of model maintenance in this larger domain, this chapter provides an introduction to the necessary background. First, background to deep recurrent neural networks and definitions for the relevant machine learning approaches are provided. Then, the entire big data pipeline and relevant design decisions are presented on an abstract level. Lastly, the requirements of the case used to compare the maintenance approaches are derived from the application domain description.

%online learing survey has lots of definitions that can be checked!
\section[Background on neural networks and machine learning approaches for model maintenance]{Background on neural networks and machine\\ learning approaches for model maintenance}

% intro to rnn+dnn and sgd
\textbf{Neural networks} is a machine learning paradigm that takes its inspiration from the human brain. The composition of a neural network is the following. The basic building blocks are neurons, collected in a layered architecture. A single neuron composes of weights and an activation function. The neuron takes weighted signals from part of the neurons of the previous layer and determines using the activation function which weighted value it passes on neurons connected to it on the next layer. \textbf{Deep neural networks} (DNN) have more than one layer between the first and last layer, whereas in \textbf{Recurrent neural networks} (RNN) there are not only connections to the next layer, but also back-connections within the same layer~\cite{ben-nunDemystifyingParallelDistributed2019}. 

 Training of a deep neural network means that the weights of the neuron-to-neuron-connections are adjusted, usually using the algorithm called \textbf{stochastic gradient descent} (SGD)~\cite{ben-nunDemystifyingParallelDistributed2019}. Predictions to a learning task are determined by the value of the final layer after the signal has passed through the entire network. The case study regarding marine traffic is assumed to use recurrent deep neural networks that are trained using the SGD algorithm.

% approaches opening paragraph: nonestablished terminology
The terminology presented next, regarding machine learning approaches, is somewhat indefinitive. As the terms for various approaches have varying interpretations in literature, the definitions provided in the following paragraphs will define how these terms are used in this work.

% problem: concept drift incl. subtypes + delayed feedback
For the problem space addressed, the central term is \textbf{concept drift}. By concept drift we mean the case where the relation between the input
data and the predicted variable change over time~\cite{conceptdriftsurvey}. It is important to note that this does not mean the natural fluctuation occurring in data when sampling from a probability distribution, but when the probability distribution itself changes. As a simple example, fours repeating when throwing a dice would not be a concept drift, but changing the dice to one with multiple fours on it would characterize a drift. Concept drift can be further divided into types by the duration and scope of the drift. For duration, the terms \textbf{abrupt} and \textbf{gradual concept drift} distinguish cases where drift occurs over a short or a longer period of time~\cite{zliobaiteAdaptiveTrainingSet2010}. For scope, \textbf{global} and \textbf{partial} concept drift are used to distinguish between drifts occurring in all or only in a subset of the incoming data. As a further challenge in this domain, it is common that the correct answers for the prediction tasks are only available after some time~\cite{delayedlabelstreams}, for which the term  \textbf{delayed feedback} will be used. 

% paradigms: what this is & is not
The terms related to machine learning approaches for dealing with concept drifts are the following. The approach regarding models that are capable to learn new tasks using previous training, such as a classifier being able to recognize unseen classes, is called \textbf{transfer learning}~\cite{iotsurvey}. \textbf{Lifelong learning} is a closely related field that explores the ability of models to adapt to new tasks, problems or environments, without losing the accuracy on previously learned tasks. Transfer and lifelong learning, in these definitions, are out of the scope of this thesis. As for approaches that do not enable adjusting to new prediction tasks, \textbf{adaptive learning} refers to models able to adjust to concept drift~\cite{conceptdriftsurvey}. Somewhat synonymously, \textbf{continual learning} is used to refer to models able to operate accurately over longer periods of time. Summarized, both adaptive and continual learning could be used to characterize the problem of this thesis, but only continual learning is used as it more accurately describes the problem at hand.

As for systems settings where machine learning is conducted, the follwing approaches are central. \textbf{Incremental learning} refers to cases where not all input data is available during training~\cite{giraud-carrierNoteUtilityIncremental2000}. As a subtype of this, in an \textbf{online learning} setting data comes in as a stream one unit at a time, and can be processed as such or in \textbf{batches}, meaning chunks of data~\cite{conceptdriftsurvey}.

% closing paragraph: what we are doing using this terminology
With the terminology presented above, our research challenge can be reformulated to be the following: The assumption is that there is a recurrent neural network trained on a small data sample. The aim is to enable retraining on large-scale data while mitigating concept drifts using continual online learning with batch processing. 

% add here description like this: The online learning setting under consideration is one
%where data can be buffered in batches using a moving window. So we do batch mode online learning.


\section[Best practices in big data systems design: organization and principles]{Best practices in big data systems design:\\ organization and principles}
% this is a long sect... a lot of stuff like lambdakappa and edgefogcloud can be omitted if necessary, they are more so nice to knows

% intro to subsection: why this is explained.
In the following section, the abstract components of a data streaming system are presented to clarify which parts are to be studied in the later chapters. Best practices are presented to have general guidelines for which traits to priorize when comparing model maintaining approaches.

% list the components
% maybe add: this is an abstraction, feedback loops and multitudes of ways of implementation abstracted away
These parts their order of operating are: data extraction, data preparation, data storage, model training, model evaluation and validation, model registry and model inference. It is assumed that the neural network used is readily developed and trained on a sample of data, as model development concerns, however non-trivial, are out of scope of this thesis. For each of the system steps, the main task and general design decisions are presented next.

\textbf{Data extraction} is responsible for handling the incoming data. Usual forms of data are either web logs, coming in from a server, or IoT data, coming in from sensors that are often geographically distant from the data ingesting component.

The data to process is usually coming in as a stream. An important design decision to make is whether the data should be processed as such or in very small chunks, called stream processing, or by collecting data into larger chunks and processing those, called batch processing. The main advantage of using a batch representation is known to be increased throughput%(find citations here)
, while stream processing allows smaller end-to-end latencies~\cite{mci/Feick2018}. %(find citations here)
% the usually interpreted as stream or batch needs a ref too?

To mitigate this trade-off between throughput and latency, a popular solution is called the lambda architecture. This reference architecture has a processing unit for both batch and stream data with the stream component, so-called speed layer, handling requests for the data not yet processed by the batch layer~\cite{beatingcap}. While this approach is widely accepted to solve the problem of handling highly voluminous data fast and mitigating human errors, the architecture has faced criticism for being redundantly complex and forcing code duplication~(\cite{questioninglambda},~\cite{uber},~\cite{facebook}). As an alternative, the kappa architecture has been proposed, only composing of a stream processor~\cite{questioninglambda}. This mitigates duplication, but retains the tradeoff with throughput and latency.

\textbf{Data preparation} encompasses the processes of data cleaning, transformation, and feature engineering. Data cleaning refers to operations identifying and removing faulty data, such as values that are missing or clearly impossible. Data transformation and feature engineering are used interchangeably, both meaning processes that transform data into a format the models can process. A popular example in this is the mapping of plain text into word count matrices.

While the data preparation step is often overlooked in literature,
it is a both challenging and crucial part of the system as preprocessing commonly takes up a majority of the total end-to-end latency of a machine learning system~\cite{adaptivelearningsystems}. As for systems design, the most relevant decision to make is whether to clean data before it is saved to the data storage, or only when it is  needed for training. The main benefit with the first approach is that data only needs to be cleaned once, while the latter allows processing data for different models in different ways and retaining the data in its original form.
% the two approaches and their tradeoffs need a ref...

\textbf{Data storage} is used to save the data needed for model training. The most often used storage methods are called data warehouses and columnar databases. Here it is important to note that due to the volume of big data, storage times are generally short, usually the maximum being a few days (e.g. \cite{uber}). The other extreme is one-pass processing, which means that the processed data is not stored at all.
% data archival needs a ref.

\textbf{Model training} means tuning the model parameters in a way that it fits to the distribution of the incoming data in order to make accurate predictions from data coming in during inference.

% too short sentences, töksähtelee
The main design decision for this component regards infrastructure: distribution and parallelization. Centralized training, often run in a cloud data center, can run either on one machine or parallelized across multiple cores by splitting the model or the data~\cite{ben-nunDemystifyingParallelDistributed2019}. As the amount of data is large, high-performance clusters and modern general processing units are used to reach required latencies~\cite{iotsurvey}. The highest degree of distribution is called \textbf{federated learning} and refers to a setting where the training is conducted in the edge devices of a sensor network. This aims to solve the issues of moving privacy-sensitive data across the network and being able to adapt to the unique environments of each data source~\cite{iotsurvey}.

\textbf{Model evaluation and validation} refers to the stage where the model is tested; usually various model metrics such as accuracy for classifiers and error statistics for regression tasks are checked~\cite{iotsurvey}. These numbers can also be compared against other models, possibly models already in operation~\cite{googlemlops}. Also testing different data sets and checking compatibility with the rest of the system is conducted at this stage~\cite{googlemlops}.

In addition to testing, models are also optimized for the infrastructure that they are being deployed on. This can include, for example, various performance optimizations, or in case of constrained memory, model compression~\cite{iotsurvey}.

\textbf{Model registry} is a centralized storage for the trained models. As trained models do not require a lot of memory, this stage has little complex design matters.

\textbf{Model inference} refers to the model being in production and answering application requests.
The most important design decision to make for this component is where to deploy the models. The options are a centralized cloud data center, the sensor network edge devices, or the so-called fog, which refers to any devices between the edge and the cloud~\cite{fogsurvey}. The main advantage with the distributed approaches are, similarly to distributed training, reduced network latencies and better privacy~\cite{szeEfficientProcessingDeep2017}.

The components presented above are summarized in Figure~\ref{simplepipeline}.

% the figure is too small... -> not very readable
\begin{figure}[hb]
%\begin{figure}[tbh] t= top, b = bottom, h=here
\newline
\begin{center}
\includegraphics[width=1.0\columnwidth]{simplegoogle.png}
\caption{Abstract components of a big data analytics system~\cite{googlemlops}.}
\label{simplepipeline}
\end{center}
\end{figure}

% model updating within inference, thesis concerns (iotsurvey has monitoring stuff if needed)
% maybe add: this is like CI and CD but CT=continuous training (from~\cite{googlemlops})
The concerns of this study are within the step of model inference. This step requires, due to the evolving nature of data, a scheme for maintaining the accuracy of the model. For this purpose, a retraining pipeline, model monitoring system, and a retraining trigger need to be in place. From these parts, especially the retraining, monitoring, and trigger components are investigated. This general abstraction of the updating scheme, highlighted with the focuses of this thesis, are presented in Figure~\ref{triggerpipeline}.

% the figure is too small... -> not very readable
\begin{figure}[ht]
%\begin{figure}[tbh] t= top, b = bottom, h=here
\ \newline
\begin{center}
\includegraphics[width=1.0\columnwidth]{paivityssykli.png}
\caption{Automated retraining workflow, simplified and highlighted by the author~\cite{googlemlops}.}
\label{triggerpipeline}
\end{center}
\end{figure}


% best practices + automation: mlops and automl
Lastly, the most important best practices in building a successful big data system are the following. These are combined lessons learned from the most mature big data systems of our time, Google MillWheel~\cite{millwheel}, Facebook~\cite{facebook}, Twitter storm~\cite{storm@twitter} and the Uber system~\cite{uber}. As the most important was deemed ease of using and improving the system, which includes modularity and simplicity. The two other principles named important across these systems were scalability and resiliency, the latter including tolerance to failures and exceptions. 

As an important part of system-level ease of use was named the fact that human intervention should be minimized. This is a general trend in big data systems, as in other fields of technology. The field of MLOps aims to, similarly to DevOps, unify machine learning system development and operation, which means that automation and monitoring is advocated~\cite{googlemlops}. Automation and minimizing human intervention is the topic of study in the field of \textbf{AutoML}~\cite{celikAdaptationStrategiesAutomated2021}.


% section conclusion: what was said & how it relates to whats to come
The principles of simplicity, scalability, resiliency and degree of automation will be another point considered when choosing the most appropriate model updating workflow: from the promising-seeming options encountered, the goal is to find one that would meet these best practices as well as possible. From the full systems perspective, this goal means handling one part of the process, model updating, as successfully as possible. Seen this way our goal really is the means to the ends of setting up a full, efficient data processing pipeline.

\section[Context and requirements from the maritime domain]{Context and requirements from the maritime\\ domain}

% why this section: we need to know the context to know the requirements to choose the right approach 
Lastly for the background, this section introduces the maritime domain and elaborates its specialities that are used to find the most optimal model maintenance workflow.

% maritime is global
The domain of application, maritime, differs in a few ways from others from the point or view of a sensor-based machine learning system. The most obvious difference is its global nature: while other spatial domains such as smart cities also have to take geographical distribution into account, the maritime domain is special as it encompasses all of what the earths' surface is mostly covered by: the seas. 
% maritime is extreme heterogeneous data

As another characteristic, equally to the area that is operated in, the amount of traffic is large, which results to data volumes exceeding even what is conventionally noted as big data. Due to international regulation vessels have to send update signals of their status from every few minutes to every two seconds, depending on their speed and course~\cite{maritimeinformatics}. With approximately 100, 000 ships sailing the world oceans daily~\cite{maritimeinformatics}, this leads to billions of messages sent each day. To this exceptional amount of data we refer to as \textit{extreme-scale data} in this work. In addition to this, in order to provide valid maritime intelligence, other types of data such as geographical information and weather reports are needed~\cite{D1.1}. Therefore, the data is not only highly voluminous, but also heterogeneous, coming in both static and dynamic forms at different velocities.

% lack of speed in terms of vessels
Added to scale, another specific of this context is speed, or more specifically, the lack of it. Depending on the size and type of the vessel, it takes from minutes to an hour to change course. This means that for machine learning services in this domain the acceptable end-to-end latencies are measured in seconds, even minutes. This differs greatly from other sensor system applications, where latencies are usually measured in the order of hundreds of milliseconds (e.g.~\cite{facebook},~\cite{edgelatency}). This means that striving for instant response times is redundant, even of detriment, as prioritizing latency inevitably would introduce the need to make compromises in other respects.

% AIS intro
The main data source, the status signal data sent by vessels, called \textbf{automatic identification system} (AIS) data, also has its specialities. AIS is a form of sensor data and has two types: static messages containing information such as name, destination and ship characteristics, and dynamic messages with information on the vessels' location, speed, heading, and rate of turn. The challenge with AIS data is both its highly fluctuating reporting intervals and especially its unreliability.  Things such as manually written destinations, faulty timestamping, lack of universal identifiers, misreported locations, and even illegal traffic camouflaging their operations make identifying and correcting erroneous data both difficult and computationally expensive. In addition to faults in the data itself, another layer of error is added by the traffic system: for example, in busy areas large parts of messages can go lost because of overloaded receivers, and vessels sometimes shut off their transmitters in fear of smugglers~\cite{maritimeinformatics}.

% pilots
The goal of the project studied, VesselAI, is to provide the following four pilot services to users: route forecasting for traffic monitoring and management, design of optimal ship energy systems, operating autonomous ships in short sea transport, and weather-optimized routing for long-distance voyages. In addition to this, the more abstract goals are to find a system that is suitable for both managing extreme-scale data and enabling the models to run in production over extended periods of time. The goal is also to be able to fully utilize the most modern high performance computing infrastructures in an optimal way, and develop new machine learning methods~\cite{D1.1}. The emphasis with these models is in deep learning, which is also the focus of this thesis.

% requirements derived from above
With this context and goal specification in mind, the following system goals and challenges can be stated. The general goal is to be able to provide accurate predictions not in an instant, but still in a time-constrained manner. Two main challenges arise from the combination of the context and required services that most pressingly need to be addressed to reach this goal. Firstly, the nature of data used sets high demands on the training part of the workflow: storage, preprocessing, and training. Secondly, given the environment of operation, concept drift identification poses a challenge. As data quality is low, distinguishing drift from noise is hard, especially as it can be expected that the drifts will be more of the gradual type; abrupt drifts will likely be encountered only within subsets of the data. The time spans of vessel operating mean that the prediction correctness is known only after days, which means that the problem of delayed feedback is very present in the domain.

How to deal with these challenges, in order to build a system able to meet its requirements, is the primary topic of inspection in the following chapters.

%outline:
% 1st paragraph: maritime specifics: geo distr, volume, vessel slowness
% 2nd paragraph: data is heterogeneous and error-prone
% missing data due to lots of traffic -> receivers dont take in everything. senders turned off bc smuggling

% 3rd paragraph: pilots n abstract aims
% the project
%	the pilots I-V: what each one aims for
%		these ml problems are difficult
%	abstract aims
%		deal with extreme scale data
%		facilitate continuous learning
%		utilize modern HPC

%data thoughts
%		static and dynamic, fast and slow paced
%			examples: map data, weather data, sensor data
%	many models only need a small fraction of the data


% combine these: the requirements.
% we want accurate solutions to hard problems not instantly, but it should not take forever either
% security is no obstacle
% obstacles to reach this
% there's a big need for data preprocessing (emphasis on workflow efficiency)
% concept drift is gradual large scale abrupt only in subset of data (napa example: vessels start anchoring somewhere)
% concept drift: noise vs drift is hard to distinguish bc error data
% correct labels to data come delayed, talking about days to couple weeks. hard to get labels also is possible. mention delayed labels


\chapter{Analysis of efficient updating cycles for batch learning}

In this chapter, the existing solutions to the problem of efficiently updating the models are analyzed. Specifically, we assume that the starting point is a recurrent neural network trained on a small sample of input data and that the environment faces occasional concept drifts, mainly of gobal and gradual, or abrupt and partial type.

The challenges to address with enabling efficient update cycles are ensuring and monitoring data quality, enabling training scaling to larger data sets through optimized training and finding the optimal times when the model should be updated. The approaches for addressing these problems are discussed next.

\section[Prerequisite for retraining: efficient data management]{Prerequisite for retraining: efficient data \\management}

What differs machine learning systems from traditional software systems is that the quality of the model does not only depend on the machine learning algorithm used, but also on the data it was trained on~\cite{polyzotisDataLifecycleChallenges2018}. Therefore, there need to be mechanisms ensuring that the data is of high quality during the updates, which is referred to as \textbf{data quality assurance} in this thesis. Secondly, there needs to be knowledge on which data, transformed and cleaned in which ways, was used. This problem is known as \textbf{data versioning}.

The follwing aspects need to be taken into account considering data quality assurance. For data cleaning, it is important to ensure that the format of data is consistent, which means that the data the model was trained on should be in the same format as it will be during inference~\cite{polyzotisDataLifecycleChallenges2018}. It is also important to consider the model used when creating the cleaning processes: as some data noise impacts model performance far more than other noise, the most optimal cleaning scheme needs to take account the specifics of the model~\cite{renggliDataQualityDrivenView2021}. As for the quality of the testing data used after training the model, the same test data set should not be used too many times, as that can lead to the model being adjusted to predict perfectly on the test data set but poorly on other sets, a phenomenon known as over-fitting~\cite{renggliDataQualityDrivenView2021}.

As for the inference stage, it is know that there sometimes occur unexpected changes in the data format, which may deteriorate model performance without model failures. For these cases there needs to be an alerting system that monitors the incoming data and triggers a warning in case data format has changed unexpectedly. Also model accuracy decreasing should be seen as a signal of a potential data error~\cite{polyzotisDataLifecycleChallenges2018}.

For data versioning, the general goal is to be able to keep track of which data was used and how it was transformed before training while avoiding keeping copies of data in memory~\cite{maddoxDecibelRelationalDataset2016}. The usage of traditional version control such as git is unsuitable for data versioning as it does not take into account the size and structuredness of data files. For instance, in order to make changes to a file using git, the user has to copy the data file to their local machine, which is infeasible in cases of large files~\cite{maddoxDecibelRelationalDataset2016}.

Works aiming at creating data-tailored versioning systems, such as Decibel~\cite{maddoxDecibelRelationalDataset2016} and OrpheusDB~\cite{huangEffectiveDataVersioning2019}, aim to minimize storage costs for each data version and the time it takes to retrieve a version from memory. The main advantages are saved memory and no need to preprocess the same data many times. Another approach, implemented by the Shibsted System~\cite{vanderweideVersioningEndtoEndMachine2017}, saves all information regarding the data source and its processing into a tuple, and saves the preprocessed data into a file path hashed from this metadata. This approach is significantly simpler than the previous one.

Given the case requirements, it is likely that a batch of data is needed in the preprocessed format only for a small time window needed to train all models using that data. Therefore, storing the preprocessed version of the training data for longer is likely to be redundant and memory-consuming. However, part of the data will be needed again in the preprocessed format: in case there are indications of data errors, the data used needs to be checked manually. Therefore keeping the original data, identifiers to it and the preprocessing used is necessary to enable reproducibility for all model versions in production. This simple versioning consisting of original data saving and sufficient identifiers for reproducibility without redundantly complex storage techniques is implementable using existing machine learning lifecycle management tools, such as MLflow \cite{mlflow}.

\section[Optimizing retraining: parallelization and approximation in deep neural network training]{Optimizing retraining: parallelization and \\approximation in deep neural network training}

% not mentioned but maybe should: in neural net processing memory access, data moving, and multiply-accumulate ops are the most expensive

% not mentioned: why the highlighted approaches were highlighted (seem most mature for training recurrent networks, a lot of approaches optimize inference and convolutions for cnns)

The goal of optimized retraining from the perspective of concept drift adaptation is that it can be expected that the more often model updates can be executed, the better they can keep up with environment changes without significant fluctuations in model performance. The aim is therefore to be able to minimize the cost of an update cycle so that update frequency can be increased. As deep neural network training can take from hours to days, even weeks~\cite{szeEfficientProcessingDeep2017}, reducing training costs is crucial: update cycles of this time span are unacceptable especially in the cases of abrupt local concept drifts in marine traffic.

% assumption of feeding more data, should this be questioned?
The research question for this section is a generalization of the main question: how can the retraining time of deep recurrent neural networks be reduced? It is assumed that the model is retrained based on the previous trained model on a cloud data center as moving data to the cloud requires time spans measured in hundreds of milliseconds~\cite{edgelatency}, which is a small time given the acceptable response times (up to minutes) of the maritime intelligence case. Assuming these, the approaches of neural network parallelization and reduced precision networks will be discussed next.

\textbf{Distribution and parallelization}

The goal of distributed deep neural network training is to utilize the used hardware as efficiently as possible to reduce the training time of the network. The methods to implement parallelism can coarsely be divided into two types: data and model parallelism~\cite{ben-nunDemystifyingParallelDistributed2019}.

Data parallelism means that the parallel training is conducted by dividing the incoming data batch into parts and executing the training with those on different cores. As the de-facto neural network training algorithm, SGD, is sequential by nature, a way of enabling parallelization is using the so-called mini-batch SGD, where the training on individual minibatches can be distributed~\cite{DBLP:conf/icml/LeNCLPN11}. The common way of introducing more parallelism is through increasing the mini-batch size, which is proven to reduce training times (\cite{shallueMeasuringEffectsData2019},~\cite{smithDonDecayLearning2018}). However, this batch size increasing introduces a trade-off known as the generalization gap~\cite{hofferTrainLongerGeneralize2018}: the accuracy of the model on the testing data set generally decreases when using larger mini-batches. The main research topic in data parallelism is therefore to find techniques for mitigating this gap.

As for model parallelism, the training is distributed through dividing the model into parts, usually layers, and training these parts on individual cores. This has been proven to increase training performanceTÄNNEUUSREFERICSONTILALLE, but after a certain amount of parallelism performance starts to decrease. Increased communication costs between the parallel processes induces this performance deterioration~(\cite{ben-nunDemystifyingParallelDistributed2019},TÄNNEUUSREFERICSONTILALLE).

Given the case requirements and popularity of various distribution methods, the following can be stated about which approach should be preferred. The impact of both of the trade-offs presented, the generalization gap and increasing communicational costs can be reduced, but in general, there is more comprehensive and established methods for improving data parallelism. Furthermore, there is indication that the generalization gap is not inherently caused by the larger batches themselves, but some other trait in the training introduced due to larger batches, such as a too small number of weight updates~\cite{hofferTrainLongerGeneralize2018} or parametrization and hardware used~\cite{shallueMeasuringEffectsData2019}. This means that eliminating this secondary problem could enable larger mini-batches and through that more data parallelism without deteriorating model accuracy.

\textbf{Reduced precision networks}

An efficient technique for optimizing neural network processing is lowering the precision of the operands in the network. There is variation in which operands are rounded, which rounding scheme is used, and what is the resulting precision, ranging from standard 32-bit floating point to binary precision. Although it is a generally known result that in some cases adding noise to the network improves training quality~\cite{murrayEnhancedMLPPerformance1994}, there is indication that this would not apply to noise added by rounded values~\cite{murrayEnhancedMLPPerformance1994} nor to tasks other that classification~\cite{anEffectsAddingNoise1996}. On the contrary: there is a tradeoff between operand precision and resulting accuracy~\cite{courbariauxTrainingDeepNeural2015}. Therefore, the aim is to reduce the computational cost from training as much as can be done with negligible accuracy losses.

It has been proven that the operands in the network can be reduced from 32-bit to 16-bit floating point using either stochastic or deterministic rounding without losses in accuracy for image classification~(\cite{pmlr-v37-gupta15},~\cite{micikeviciusMixedPrecisionTraining2018}). This precision reduction would halve the need of memory usage, which is a major gain as memory operations are named as a bottleneck in neural network processing~\cite{szeEfficientProcessingDeep2017}. Even more reduction to the computational cost have been proposed through the use of binarized neural networks. These networks use only the numbers 1 and -1 as either weights~\cite{courbariauxBinaryConnectTrainingDeep2016} or weights and activations~\cite{courbariauxBinarizedNeuralNetworks2016},  which transforms all multiply-and-accumulate operations, another costly part of DNN processing, to simple additions. This would result in computational efficiency increasing by up to a factor of 3~\cite{courbariauxBinaryConnectTrainingDeep2016}.

As a rule, the reduced precision techniques report the same or very close to the same accuracies as the standard 32-bit floating point on the tasks used for benchmarking. However, changing this task can reduce the usability of the technique even when using a different task in image classification, as is seen with binarized networks: these reported state-of-the-art accuracies using the MNIST, CIFAR or SVHN datasets (\cite{courbariauxBinarizedNeuralNetworks2016},~\cite{courbariauxBinaryConnectTrainingDeep2016}), but using ImageNet, accuracy losses of 19-29\% were reported~\cite{rastegariXNORNetImageNetClassification2016}. This means that the usability of these methods cannot be determined without benchmarking on the specific learning task used in the application domain, and the results can vary greatly depending on the learning task.

\section{Timeliness of retraining: concept drift detection}

% why this discussion is included
In order to optimally utilize the accuracy gains achieved when performing model retraining, the updates should be timed correctly, that is, the updates should be triggered at occurences of concept drifts as accurately as possible. This is especially important in the maritime case where the computational intensiveness of retraining caused by extreme-scale data would make unnecessary updates especially wasteful.

% drift type specs
For concept drift detection, the types of drifts expected are important. In the case studied it can be expected that drifts are either of gradual and global or of abrupt and partial type: it is highly unlikely that all marine traffic would change abruptly across the globe. Additionally, the abrupt drifts can be expected to be geographically concentrated, such as traffic conditions changing suddenly, and the global drifts can be expected to occur very slowly, as marine traffic is slow by nature.

% methods: window, statistic, monitoring based
Roughly, drift detectors can be divided into three classes. The data-inspecting approaches are statistical methods that trigger drifts based on perceived data distribution changes and windowing techniques that compare two windows of data and trigger change if they differ sufficiently. The model-based approach is to monitor the model and mark sufficient changes in model metrics as drift occurring~\cite{faithfullUnsupervisedChangeDetection2018}.

% my argument: abrupt with statistics in geo distributed manner (drifts likely to have a physical location, windowing mainly for classification, model feedback stuffs slow)
Considering the nature of the case studied, it is necessary to detect the abrupt drifts: in cases where traffic in an area suddenly changes, for instance when a common route is shut for some reason, the performance of navigational services would deteriorate crucially unless the models are quickly updated. Additionally, due to the locality of abrupt drifts and the fact that one area represents only a small fraction of the globe, these drifts can only be detected from data that is grouped by geographical location. From the drift detection types presented above, model metric based approach would be too slow for these drifts: the lag coming from receiving the model feedback, computing metrics and inspecting those would make the drift alarm outdated. As windowing is generally only used for classification tasks, statistical tests seem most useful for detecting abrupt drifts. Which statistical test should be applied is very case dependent~\cite{faithfullUnsupervisedChangeDetection2018}, so benchmarking on the data used is needed to determine the right statistic and change triggering threshold values. Thorough surveys on the possible methods and their evaluation are presented in~\cite{conceptdriftsurvey} and~\cite{faithfullUnsupervisedChangeDetection2018}.

% also monitoring is needed
As for the gradual drifts, it is known that concept drift detection is overall better suitable for adaptation to abrupt drift as gradual drifts are hard to be distinguished from noise~\cite{zliobaiteLearningConceptDrift2010}. However, it is suggested that model monitoring should be in place in every large-scale big data system~\cite{googlemlops}, as that serves as the ground truth on if the model performance actually deteriorates or not. Therefore, also the popular drift detectors utilizing model metrics such as the drift detection method (DDM)~\cite{gamaLearningDriftDetection2004} would be of benefit to the system.

% short summary
Summarized, combining two drift detection approaches would most likely be able to detect all occurring concept drifts. These are a statistical method inspecting geographically grouped data and a more generic model metric inspector. For both of these, the actual implementation and trigger value needs further experimentation. Overall, the trigger value is a tradeoff between sensitiveness to drifts and rate of false alarms~\cite{faithfullUnsupervisedChangeDetection2018}, which is also a matter of preference.

%mikä on se tarinan osa joka ei tule muualla esille
\section{Synthesis}

This section combines the mechanisms needed for efficient updates: the mechanisms required are listed, alongside the benchmarking needed to know how they should exactly be implemented. Finally it is analyzed to which degree these approaches can be automated.

% outline
% orthogonal but interdependent
% what needs to be in place to enable efficient updates
% what needs to be benchmarked
% what can be automated

% orthogonal but interdependent
The approaches for enabling efficient update cycles outlined in the previous sections are orthogonal in nature: all approach the problem from different directions and can therefore be implemented somewhat independently. On the other hand, they also partly impact one another. For instance, the speed of retraining impacts how often retraining can be triggered, which has an impact on how sensitive the monitoring system should be. This means that the approaches have also to take into account the implementation of other mechanisms.

% % what needs to be in place to enable efficient updates
Combined, the following systems parts are required to enable efficient model updates. For data management, there needs to be enough storage for both the incoming data before it is used for training and the data used to train the models currently in production. In addition, metadata on which data with which preprocessing was used on the current models are needed to enable reproducibility in cases of data errors. Only saving the preprocessed data for models in production is not sufficient as that will not enable backtracking the cases where an unexpected change in data caused erroneous training. Secondly, for training, efficient optimization schemes, such as distribution and approximation, need to be implemented. Finally, for monitoring, inspection of both incoming data and model performance need to be in place. Data monitoring should consist of a system looking for concept drifts and a system validating the incoming data. This is because only if data is thoroughly validated, model performance deterioration can be stated to be due to concept drifts, not due to data errors.

% what needs to be benchmarked
The comparisons of different approaches could only be conducted on a level of identifying promising solutions due to the lack of experimental testing. As the effectiveness of the approaches was seen to be highly task-dependent, to determine the actual usefulness and some implementation details, the following parts need to be benchmarked on maritime data and the models from the problem domain.

In monitoring, the actual statistical test used on the data to determine if drifts occur needs to be identified. This statistic needs likely to monitor both locations and timestamps, as both the speeds and the routes taken can change. Also the trigger value, the threshold at which the monitoring statistic change is marked as a concept drift, needs to be chosen. This depends both on the environment and the speed at which an update cycle can be performed. Both of these aspects, the way in which monitoring is done and the trigger value, equally need to be benchmarked for model monitoring.

The second aspect of benchmarking regards retraining optimization: The degree of distribution without deteriorated training quality needs to be identified for the case. Also the degree of approximation of the network, if this is possible in the case, needs to be tested with the data and model used in the application before the actual usability can be determined.

% what can be automated (is this necessary or kinda outta scope...)
While automating machine learning systems is a problem on its own, it can be stated on a high level for which aspects can be automated to start which, and where humans are definitely needed. The mechanisms presented in this work can be theoretically implemented fully automated, that is, during an individual update cycle, no human intervention is necessarily required. However, on a higher level the human factor is still necessary: it cannot be known in advance if the updating scheme works over the long term. This means that the functioning of the full retraining scheme will also be needed to evaluate and adjusted if necessary. These adjustments can include, for instance, changing the data preparation and validation schemes, or developing better models, as there is always the possibility of unknown factors emerging.

% gappia tarkasteltu. mitä sä löysit täytteeksi sille ja mitä jäi täyttämättä
% piirteitä jota sulla on kulkenu mukana, niiden kautta valaiseminen

\chapter{Conclusion}
% what was aimed for, the results, impact of the results

% what was done
In this thesis, the ways of enabling efficient model updating workflows were searched for from the relevant literature. Firstly, the necessary machine learning background, the big data processing pipeline and requirements from the maritime domain were presented. Then, the mechanisms necessary for the updating cycle regarding data management, optimized retraining and concept drift detection were compared and analyzed.

% motivation: why this was important
As big data analytics grows more and more mainstream, the analysis on applicability of existing solutions in literature to the real world grows in importance. Therefore, works like the one conducted; summarizing the applicability and maturity of the existing approaches is countinuously needed in order to successfully implement machine learning systems of scale. This aspect of maintenance can be seen as the factor determining the usability of all the other factors: a perfectly estabilshed machine learning system delivers its true value only if it is possible to operate it over the long term.

% impact: applicability of the conclusions to other systems
The results concluded in this thesis, deeming data parallelism, model approximation, and the presented data and model monitoring system as the most promising, are applicable to other applications in case their requirements match the one considered here to a necessary extent. The important factors were the machine learning paradigm being deep learning, the expected drift types being mainly of abrupt partial and gradual global type, and learning being conducted in batch mode. Also the fact that being able to process the extreme-scale data was a challenge whereas the end-to-end latency requirement was easier to meet affected the approaches preferred. Finally, delays in model feedback set constraints on the usability of some monitoring mechanisms.



%significant traits that should be considered in choosing an maintenance approach
% - the learning task, many approaches are classification only, the continued assumption of the learning task being a classification one raises questions to the applicability of these techniques to other learning tasks
 %- is the learning conducted in single instance or batch mode
% - what type of drift can be predicted to occur?
% - main constraint of the system
 
 %here delayed feedback was important, and the fact that partial and gradual drift can be predicted to dominate. also system requirements are important, the challenge was data volume, not response times
 
 % further contribution: gaps in literature
 As another point of contribution, summarizing the insight existing on how to efficiently maintain model accuracy shed light on the gaps existing in literature. Especially it was found that in a large amount of works a rather tight assumption was made regarding the learning task: the works largely focus on classification and rarely address the question of applicability to other learning tasks. Another field that was found to be lacking is AutoML: whereas there are lots of works regarding other aspects of big data systems, only a few recent works question the assumption of no concept drifts occurring~(\cite{celikAdaptationStrategiesAutomated2021}, \cite{madridAutoMLPresenceDrift2019}).
 
 % this is based on forbes article why machine learning models crash and burn in production, unsure if citing that is appropriate
 %concluding thought: concept drift is very studied in academia but expertise in industry is lagging behind. The reason does not seem to be a lack of solutions, but a lack of communication. The papers usually have a strict set of assumptions regarding nature of the problem, data, and the requirements of the task to be solved. There is a lack of thought addressing: if person xxx with ml problem xxx wants to keep their models accurate, which approaches are applicable, and of those, which one would work best? This would be extremenly important as lack of maintenance is no1 reason of failure in industry.
 
 % end jargon: nonstopping work because data volumes and models grow this also needs to grow in effectiveness
 %end jargon something like the techniques now address the volume and speed now but continual work needs to be done to meet the growing demands of the future.
 
 In general, in order to establish working practices concerning large machine learning systems, both the fields of building successful systems and ensuring operation for longer spans of time need to advance equally. As both the amounts of data and the sizes of deep learning models are expected to keep growing, the aspects of updating will be needed constant re-visiting to ensure that the growing systems keep operating successfully.
