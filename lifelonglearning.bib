@inproceedings{10.1145/3340482.3342743,
author = {Foidl, Harald and Felderer, Michael},
title = {Risk-Based Data Validation in Machine Learning-Based Software Systems},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.libproxy.helsinki.fi/10.1145/3340482.3342743},
doi = {10.1145/3340482.3342743},
abstract = {Data validation is an essential requirement to ensure the reliability and quality
of Machine Learning-based Software Systems. However, an exhaustive validation of all
data fed to these systems (i.e. up to several thousand features) is practically unfeasible.
In addition, there has been little discussion about methods that support software
engineers of such systems in determining how thorough to validate each feature (i.e.
data validation rigor). Therefore, this paper presents a conceptual data validation
approach that prioritizes features based on their estimated risk of poor data quality.
The risk of poor data quality is determined by the probability that a feature is of
low data quality and the impact of this low (data) quality feature on the result of
the machine learning model. Three criteria are presented to estimate the probability
of low data quality (Data Source Quality, Data Smells, Data Pipeline Quality). To
determine the impact of low (data) quality features, the importance of features according
to the performance of the machine learning model (i.e. Feature Importance) is utilized.
The presented approach provides decision support (i.e. data validation prioritization
and rigor) for software engineers during the implementation of data validation techniques
in the course of deploying a trained machine learning model and its software stack.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {13–18},
numpages = {6},
keywords = {Data Validation, Risk-based Testing, Machine Learning},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}


@article{10.1145/3369837,
author = {Lee, Seulki and Islam, Bashima and Luo, Yubo and Nirjon, Shahriar},
title = {Intermittent Learning: On-Device Machine Learning on Intermittently Powered System},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi-org.libproxy.helsinki.fi/10.1145/3369837},
doi = {10.1145/3369837},
abstract = {This paper introduces intermittent learning --- the goal of which is to enable energy
harvested computing platforms capable of executing certain classes of machine learning
tasks effectively and efficiently. We identify unique challenges to intermittent learning
relating to the data and application semantics of machine learning tasks, and to address
these challenges, we devise 1) an algorithm that determines a sequence of actions
to achieve the desired learning objective under tight energy constraints, and 2) propose
three heuristics that help an intermittent learner decide whether to learn or discard
training examples at run-time which increases the energy efficiency of the system.
We implement and evaluate three intermittent learning applications that learn the
1) air quality, 2) human presence, and 3) vibration using solar, RF, and kinetic energy
harvesters, respectively. We demonstrate that the proposed framework improves the
energy efficiency of a learner by up to 100% and cuts down the number of learning
examples by up to 50% when compared to state-of-the-art intermittent computing systems
that do not implement the proposed intermittent learning framework.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {141},
numpages = {30},
keywords = {Batteryless, Intermittent computing, Semi-supervised learning, Energy harvesting, On-device online learning, Unsupervised learning}
}


@inproceedings{10.1145/3453688.3461738,
author = {Chen, Yao and Hawkins, Cole and Zhang, Kaiqi and Zhang, Zheng and Hao, Cong},
title = {3U-EdgeAI: Ultra-Low Memory Training, Ultra-Low Bitwidth Quantization, and Ultra-Low Latency Acceleration},
year = {2021},
isbn = {9781450383936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.libproxy.helsinki.fi/10.1145/3453688.3461738},
doi = {10.1145/3453688.3461738},
abstract = {The deep neural network (DNN) based AI applications on the edge require both low-cost
computing platforms and high-quality services. However, the limited memory, computing
resources, and power budget of the edge devices constrain the effectiveness of the
DNN algorithms. Developing edge-oriented AI algorithms and implementations (e.g.,
accelerators) is challenging. In this paper, we summarize our recent efforts for efficient
on-device AI development from three aspects, including both training and inference.
First, we present on-device training with ultra-low memory usage. We propose a novel
rank-adaptive tensor-based tensorized neural network model, which offers orders-of-magnitude
memory reduction during training. Second, we introduce an ultra-low bitwidth quantization
method for DNN model compression, achieving the state-of-the-art accuracy under the
same compression ratio. Third, we introduce an ultra-low latency DNN accelerator design,
practicing the software/hardware co-design methodology. This paper emphasizes the
importance and efficacy of training, quantization and accelerator design, and calls
for more research breakthroughs in the area for AI on the edge.},
booktitle = {Proceedings of the 2021 on Great Lakes Symposium on VLSI},
pages = {157–162},
numpages = {6},
keywords = {edge ai, dnn quantization, dnn acceleration, on-device training},
location = {Virtual Event, USA},
series = {GLSVLSI '21}
}

@ARTICLE{9094172,
  author={Bousdekis, Alexandros and Papageorgiou, Nikos and Magoutas, Babis and Apostolou, Dimitris and Mentzas, Gregoris},
  journal={IEEE Access}, 
  title={Sensor-Driven Learning of Time-Dependent Parameters for Prescriptive Analytics}, 
  year={2020},
  volume={8},
  number={},
  pages={92383-92392},
  doi={10.1109/ACCESS.2020.2994933}}
  