@inproceedings{edgelatency,
author = {Chen, Zhuo and Hu, Wenlu and Wang, Junjue and Zhao, Siyan and Amos, Brandon and Wu, Guanhang and Ha, Kiryong and Elgazzar, Khalid and Pillai, Padmanabhan and Klatzky, Roberta and Siewiorek, Daniel and Satyanarayanan, Mahadev},
title = {An Empirical Study of Latency in an Emerging Class of Edge Computing Applications for Wearable Cognitive Assistance},
year = {2017},
isbn = {9781450350877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3132211.3134458},
booktitle = {Proceedings of the Second ACM/IEEE Symposium on Edge Computing},
articleno = {14},
numpages = {14},
keywords = {hololens, edge computing, mobile computing, smart glass, augmented reality, cloud computing, cloudlet},
location = {San Jose, California},
series = {SEC '17}
}

@article{millwheel,
author = {Akidau, Tyler and Balikov, Alex and Bekiro\u{g}lu, Kaya and Chernyak, Slava and Haberman, Josh and Lax, Reuven and McVeety, Sam and Mills, Daniel and Nordstrom, Paul and Whittle, Sam},
title = {MillWheel: Fault-Tolerant Stream Processing at Internet Scale},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
doi = {10.14778/2536222.2536229},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1033–1044},
numpages = {12}
}

@article{dataflow,
author = {Akidau, Tyler and Bradshaw, Robert and Chambers, Craig and Chernyak, Slava and Fern\'{a}ndez-Moctezuma, Rafael J. and Lax, Reuven and McVeety, Sam and Mills, Daniel and Perry, Frances and Schmidt, Eric and Whittle, Sam},
title = {The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, out-of-Order Data Processing},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
doi = {10.14778/2824032.2824076},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1792–1803},
numpages = {12}
}

@inproceedings{storm@twitter,
author = {Toshniwal, Ankit and Taneja, Siddarth and Shukla, Amit and Ramasamy, Karthik and Patel, Jignesh M. and Kulkarni, Sanjeev and Jackson, Jason and Gade, Krishna and Fu, Maosong and Donham, Jake and Bhagat, Nikunj and Mittal, Sailesh and Ryaboy, Dmitriy},
title = {Storm@twitter},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2588555.2595641},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {147–156},
numpages = {10},
keywords = {real-time query processing, stream data management},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{facebook,
author = {Chen, Guoqiang Jerry and Wiener, Janet L. and Iyer, Shridhar and Jaiswal, Anshul and Lei, Ran and Simha, Nikhil and Wang, Wei and Wilfong, Kevin and Williamson, Tim and Yilmaz, Serhat},
title = {Realtime Data Processing at Facebook},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2882903.2904441},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1087–1098},
numpages = {12},
keywords = {realtime data processing, stream processing},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{uber,
author = {Fu, Yupeng and Soman, Chinmay},
title = {Real-Time Data Infrastructure at Uber},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3448016.3457552},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2503–2516},
numpages = {14},
keywords = {streaming processing, real-time infrastructure},
location = {Virtual Event, China},
series = {SIGMOD/PODS '21}
}

@book{maritimeinformatics,
author = {Artikis, Alexander and Zissis, Dimitris},
year = {2021},
month = {01},
pages = {333},
title = {Guide to Maritime Informatics},
isbn = {978-3-030-61851-3},
doi = {10.1007/978-3-030-61852-0}
}

@inproceedings{lambdakappa,
author = {Feick, Martin AND Kleer, Niko AND Kohn, Marek},
title = {Fundamentals of Real-Time Data Processing Architectures Lambda and Kappa},
booktitle = {SKILL 2018 - Studierendenkonferenz Informatik},
year = {2018},
editor = {Becker, Michael} ,
pages = { 55-66 },
publisher = {Gesellschaft für Informatik e.V.},
address = {Bonn}
}

@article{mapreduce,
author = {Dean, Jeffrey and Ghemawat, Sanjay},
title = {MapReduce: Simplified Data Processing on Large Clusters},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0001-0782},
doi = {10.1145/1327452.1327492},
journal = {Commun. ACM},
month = jan,
pages = {107–113},
numpages = {7}
}

@INPROCEEDINGS{geolytics,
  author={Cheng, Bin and Papageorgiou, Apostolos and Cirillo, Flavio and Kovacs, Ernoe},
  booktitle={2015 IEEE 2nd World Forum on Internet of Things (WF-IoT)}, 
  title={GeeLytics: Geo-distributed edge analytics for large scale IoT systems based on dynamic topology}, 
  year={2015},
  volume={},
  number={},
  pages={565-570},
  doi={10.1109/WF-IoT.2015.7389116}}

@INPROCEEDINGS{edgeiot,
  author={Kumar, Umesh and Verma, Parul and Qamar Abbas, Syed},
  booktitle={2021 International Conference on Computer Communication and Informatics (ICCCI)}, 
  title={Bringing Edge Computing into IoT Architecture to Improve IoT Network Performance}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICCCI50826.2021.9402499}}

@ARTICLE{iotsystems,
  author={Swamy, S. Narasimha and Kota, Solomon Raju},
  journal={IEEE Access}, 
  title={An Empirical Study on System Level Aspects of Internet of Things (IoT)}, 
  year={2020},
  volume={8},
  number={},
  pages={188082-188134},
  doi={10.1109/ACCESS.2020.3029847}}


@inbook{mliot,
author = {Boovaraghavan, Sudershan and Maravi, Anurag and Mallela, Prahaladha and Agarwal, Yuvraj},
title = {MLIoT: An End-to-End Machine Learning System for the Internet-of-Things},
year = {2021},
isbn = {9781450383547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450268.3453522},
booktitle = {Proceedings of the International Conference on Internet-of-Things Design and Implementation},
pages = {169–181},
numpages = {13}
}

@inproceedings{apachesurvey,
author = {Nasiri, Hamid and Nasehi, Saeed and Goudarzi, Maziar},
title = {A Survey of Distributed Stream Processing Systems for Smart City Data Analytics},
year = {2018},
isbn = {9781450365321},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3269961.3282845},
booktitle = {Proceedings of the International Conference on Smart Cities and Internet of Things},
articleno = {12},
numpages = {7},
keywords = {Big data, Stream processing, Internet of Things, Distributed computing, Smart City},
location = {Mashhad, Iran},
series = {SCIOT '18}
}

@inproceedings{gpuinsmartcity,
author = {Shang, Lei and Lin, Ching Yeh and Atif, Muhammad and Williams, Allan},
title = {Evaluation of High Density GPUs as Sustainable Smart City Infrastructure},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {482–487},
numpages = {6},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{e2eiotstack,
author = {Hamann, Arne and Saidi, Selma and Ginthoer, David and Wietfeld, Christian and Ziegenbein, Dirk},
title = {Building End-to-End IoT Applications with QoS Guarantees},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {112},
numpages = {6},
location = {Virtual Event, USA},
series = {DAC '20}
}

@INPROCEEDINGS{i4sea,
  author={Tampakis, Panagiotis and Chondrodima, Eva and Pikrakis, Aggelos and Theodoridis, Yannis and Pristouris, Kostis and Nakos, Harry and Petra, Eleni and Dalamagas, Theodore and Kandiros, Andreas and Markakis, Georgios and Maina, Irida and Kavadas, Stefanos},
  booktitle={2020 21st IEEE International Conference on Mobile Data Management (MDM)}, 
  title={Sea Area Monitoring and Analysis of Fishing Vessels Activity: The i4sea Big Data Platform}, 
  year={2020},
  volume={},
  number={},
  pages={275-280},
  doi={10.1109/MDM48529.2020.00063}}
  
  @inproceedings{uprctrajectorysystem,
author = {Petrou, Petros and Nikitopoulos, Panagiotis and Tampakis, Panagiotis and Glenis, Apostolos and Koutroumanis, Nikolaos and Santipantakis, Georgios M. and Patroumpas, Kostas and Vlachou, Akrivi and Georgiou, Harris and Chondrodima, Eva and Doulkeridis, Christos and Pelekis, Nikos and Andrienko, Gennady L. and Patterson, Fabian and Fuchs, Georg and Theodoridis, Yannis and Vouros, George A.},
title = {ARGO: A Big Data Framework for Online Trajectory Prediction},
year = {2019},
isbn = {9781450362801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3340964.3340988},
booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
pages = {194–197},
numpages = {4},
keywords = {geostreaming, mobility events, trajectories, location prediction},
location = {Vienna, Austria},
series = {SSTD '19}
}

 @misc{questioninglambda, title={Questioning the Lambda Architecture}, url={https://www.oreilly.com/radar/questioning-the-lambda-architecture/}, journal={O'Reilly Radar}, publisher={O'Reilly}, author={Kreps, Jay}, year={2014}, month={7}, urldate={2021-07-23}} 
 @misc{beatingcap, title={How to beat the CAP theorem}, url={http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html}, journal={thoughts from the red planet}, author={Marz, Nathan}, year={2011}, month={10}, urldate={2021-07-23}} 
 
 @misc{D4.1,
  author       = "VesselAI",
  title        = "D4.1. Specification of The AI On-Demand Platform Extensions and Research Activities",
  day          = 30,
  month        = "June",
  year         = "2021",
}
 
  @misc{D1.1,
  author       = "VesselAI",
  title        = "D1.1 -State-of-the-art Analysis and Data Sources",
  day          = 29,
  month        = 4,
  year         = "2021",
}
 
@Article{edgefogcloud,
AUTHOR = {Cao, Hung and Wachowicz, Monica},
TITLE = {An Edge-Fog-Cloud Architecture of Streaming Analytics for Internet of Things Applications},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3594},
URL = {https://www.mdpi.com/1424-8220/19/16/3594},
ISSN = {1424-8220},
ABSTRACT = {Exploring Internet of Things (IoT) data streams generated by smart cities means not only transforming data into better business decisions in a timely way but also generating long-term location intelligence for developing new forms of urban governance and organization policies. This paper proposes a new architecture based on the edge-fog-cloud continuum to analyze IoT data streams for delivering data-driven insights in a smart parking scenario.},
DOI = {10.3390/s19163594}
}

@INPROCEEDINGS{anomalysystem,
  author={Chatzikokolakis, Konstantinos and Zissis, Dimitris and Vodas, Marios and Spiliopoulos, Giannis and Kontopoulos, Ioannis},
  booktitle={OCEANS 2019 - Marseille}, 
  title={A distributed lightning fast maritime anomaly detection service}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/OCEANSE.2019.8867269}}

 @article{thelambdarant, title={The Lambda and the Kappa}, journal={Big Data Bites}, author={Lin, Jimmy}, year={2017}, pages={60–66}, urldate={2021-07-25}} 
 
 @article{fogsurvey,
author = {Puliafito, Carlo and Mingozzi, Enzo and Longo, Francesco and Puliafito, Antonio and Rana, Omer},
title = {Fog Computing for the Internet of Things: A Survey},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1533-5399},
doi = {10.1145/3301443},
abstract = {Research in the Internet of Things (IoT) conceives a world where everyday objects
are connected to the Internet and exchange, store, process, and collect data from
the surrounding environment. IoT devices are becoming essential for supporting the
delivery of data to enable electronic services, but they are not sufficient in most
cases to host application services directly due to their intrinsic resource constraints.
Fog Computing (FC) can be a suitable paradigm to overcome these limitations, as it
can coexist and cooperate with centralized Cloud systems and extends the latter toward
the network edge. In this way, it is possible to distribute resources and services
of computing, storage, and networking along the Cloud-to-Things continuum. As such,
FC brings all the benefits of Cloud Computing (CC) closer to end (user) devices. This
article presents a survey on the employment of FC to support IoT devices and services.
The principles and literature characterizing FC are described, highlighting six IoT
application domains that may benefit from the use of this paradigm. The extension
of Cloud systems towards the network edge also creates new challenges and can have
an impact on existing approaches employed in Cloud-based deployments. Research directions
being adopted by the community are highlighted, with an indication of which of these
are likely to have the greatest impact. An overview of existing FC software and hardware
platforms for the IoT is also provided, along with the standardisation efforts in
this area initiated by the OpenFog Consortium (OFC).},
journal = {ACM Trans. Internet Technol.},
month = apr,
articleno = {18},
numpages = {41},
keywords = {cloud computing, Fog computing, internet of things, topological proximity}
}

@INPROCEEDINGS{apachebenchmarkI,
  author={Qian, Shilei and Wu, Gang and Huang, Jie and Das, Tathagata},
  booktitle={2016 IEEE International Conference on Industrial Technology (ICIT)}, 
  title={Benchmarking modern distributed streaming platforms}, 
  year={2016},
  volume={},
  number={},
  pages={592-598},
  doi={10.1109/ICIT.2016.7474816}}

@INPROCEEDINGS{apachebenchmarkII,
  author={Chintapalli, Sanket and Dagit, Derek and Evans, Bobby and Farivar, Reza and Graves, Thomas and Holderbaugh, Mark and Liu, Zhuo and Nusbaum, Kyle and Patil, Kishorkumar and Peng, Boyang Jerry and Poulosky, Paul},
  booktitle={2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming}, 
  year={2016},
  volume={},
  number={},
  pages={1789-1792},
  doi={10.1109/IPDPSW.2016.138}}
  
  @inproceedings{maritimetradvsbigdata,
  title={Anomaly detection in the maritime domain: Comparison of traditional and big data approach},
  author={Filipiak, Dominik and Str{\'o}zyna, Milena and Wecel, Krzysztof and Abramowicz, Witold},
  booktitle={NATO IST-160-RSM Specialists' Meeting on Big Data and Artificial Intelligence for Military Decision Makin},
  year={2018}
}

 @INPROCEEDINGS{timecriticalmarineaero,
author = {George A. Vouros,
Christos Doulkeridis,
Georgios Santipantakis,
Akrivi Vlachou,
Nikos Pelekis,
Harilaos Georgiou,
Yiannis Theodoridis,
Kostas Patroumpas,
Georg Fuchs, Michael Mock,
Gennady Andrienko,
Natalia Andrienko,
Cyril Ray,
Christophe Claramunt
Ecole Navale / ENSAM, France
Elena Camossi,
Anne-Laure Jousselme,
David Scarlatti,
Jose Manuel Cordero},
title = {Big Data Analytics for Time Critical Maritime and Aerial
Mobility Forecasting},
booktitle={Proceedings of the 21st
International Conference on Extending Database Technology (EDBT)},
year = {2018}
}

@article{lmlinneuralnets,
title = {Continual lifelong learning with neural networks: A review},
journal = {Neural Networks},
volume = {113},
pages = {54-71},
year = {2019},
issn = {0893-6080},
doi = {10.1016/j.neunet.2019.01.012},
author = {German I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and Stefan Wermter},
keywords = {Continual learning, Lifelong learning, Catastrophic forgetting, Developmental systems, Memory consolidation},
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.}
}

@INPROCEEDINGS{lmlsystemframework,
  author={Martinez Plumed, Fernando and Ferri, Cesar and Hernandez Orallo, Jose and Ramirez Quintana, Maria Jose},
  booktitle={2014 13th International Conference on Machine Learning and Applications}, 
  title={A Knowledge Growth and Consolidation Framework for Lifelong Machine Learning Systems}, 
  year={2014},
  volume={},
  number={},
  pages={111-116},
  doi={10.1109/ICMLA.2014.23}}
  
  @INPROCEEDINGS{multilambdaforlml,
  author={Pal, Gautam and Li, Gangmin and Atkinson, Katie},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
  title={Big Data Ingestion and Lifelong Learning Architecture}, 
  year={2018},
  volume={},
  number={},
  pages={5420-5423},
  doi={10.1109/BigData.2018.8621859}}

@inproceedings{lmlsystems,
  title={Lifelong machine learning systems: Beyond learning algorithms},
  author={Silver, Daniel L and Yang, Qiang and Li, Lianghao},
  booktitle={2013 AAAI spring symposium series},
  year={2013}
}

@article{conceptdriftsurvey,
author = {Gama, Jo\~{a}o and Žliobaitė, Indrė and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
title = {A Survey on Concept Drift Adaptation},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0360-0300},
doi = {10.1145/2523813},
abstract = {Concept drift primarily refers to an online supervised learning scenario when the
relation between the input data and the target variable changes over time. Assuming
a general knowledge of supervised learning in this article, we characterize adaptive
learning processes; categorize existing strategies for handling concept drift; overview
the most representative, distinct, and popular techniques and algorithms; discuss
evaluation methodology of adaptive algorithms; and present a set of illustrative applications.
The survey covers the different facets of concept drift in an integrated way to reflect
on the existing scattered state of the art. Thus, it aims at providing a comprehensive
introduction to the concept drift adaptation for researchers, industry analysts, and
practitioners.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {44},
numpages = {37},
keywords = {Concept drift, adaptive learning, change detection, data streams},
}

@article{mlforstreamingsurvey,
author = {Gomes, Heitor Murilo and Read, Jesse and Bifet, Albert and Barddal, Jean Paul and Gama, Jo\~{a}o},
title = {Machine Learning for Streaming Data: State of the Art, Challenges, and Opportunities},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1931-0145},
doi = {10.1145/3373464.3373470},
abstract = {Incremental learning, online learning, and data stream learning are terms commonly
associated with learning algorithms that update their models given a continuous influx
of data without performing multiple passes over data. Several works have been devoted
to this area, either directly or indirectly as characteristics of big data processing,
i.e., Velocity and Volume. Given the current industry needs, there are many challenges
to be addressed before existing methods can be efficiently applied to real-world problems.
In this work, we focus on elucidating the connections among the current stateof- the-art
on related fields; and clarifying open challenges in both academia and industry. We
treat with special care topics that were not thoroughly investigated in past position
and survey papers. This work aims to evoke discussion and elucidate the current research
opportunities, highlighting the relationship of different subareas and suggesting
courses of action when possible.},
journal = {SIGKDD Explor. Newsl.},
month = nov,
pages = {6–22},
numpages = {17}
}

@article{streamminingchallenges,
author = {Krempl, Georg and Žliobaitė, Indrė and Brzezi\'{n}ski, Dariusz and H\"{u}llermeier, Eyke and Last, Mark and Lemaire, Vincent and Noack, Tino and Shaker, Ammar and Sievi, Sonja and Spiliopoulou, Myra and Stefanowski, Jerzy},
title = {Open Challenges for Data Stream Mining Research},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1931-0145},
doi = {10.1145/2674026.2674028},
abstract = {Every day, huge volumes of sensory, transactional, and web data are continuously generated
as streams, which need to be analyzed online as they arrive. Streaming data can be
considered as one of the main sources of what is called big data. While predictive
modeling for data streams and big data have received a lot of attention over the last
decade, many research approaches are typically designed for well-behaved controlled
problem settings, overlooking important challenges imposed by real-world applications.
This article presents a discussion on eight open challenges for data stream mining.
Our goal is to identify gaps between current research and meaningful applications,
highlight open problems, and define new application-relevant research directions for
data stream mining. The identified challenges cover the full cycle of knowledge discovery
and involve such problems as: protecting data privacy, dealing with legacy systems,
handling incomplete and delayed information, analysis of complex data, and evaluation
of stream mining algorithms. The resulting analysis is illustrated by practical applications
and provides general suggestions concerning lines of future research in data stream
mining.},
journal = {SIGKDD Explor. Newsl.},
month = sep,
pages = {1–10},
numpages = {10}
}

@article{adaptivelearningsystems,
author = {Žliobaitė, Indrė and Bifet, Albert and Gaber, Mohamed and Gabrys, Bogdan and Gama, Joao and Minku, Leandro and Musial, Katarzyna},
title = {Next Challenges for Adaptive Learning Systems},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1931-0145},
doi = {10.1145/2408736.2408746},
abstract = {Learning from evolving streaming data has become a 'hot' research topic in the last
decade and many adaptive learning algorithms have been developed. This research was
stimulated by rapidly growing amounts of industrial, transactional, sensor and other
business data that arrives in real time and needs to be mined in real time. Under
such circumstances, constant manual adjustment of models is in-efficient and with
increasing amounts of data is becoming infeasible. Nevertheless, adaptive learning
models are still rarely employed in business applications in practice. In the light
of rapidly growing structurally rich 'big data', new generation of parallel computing
solutions and cloud computing services as well as recent advances in portable computing
devices, this article aims to identify the current key research directions to be taken
to bring the adaptive learning closer to application needs. We identify six forthcoming
challenges in designing and building adaptive learning (pre-diction) systems: making
adaptive systems scalable, dealing with realistic data, improving usability and trust,
integrat-ing expert knowledge, taking into account various application needs, and
moving from adaptive algorithms towards adaptive tools. Those challenges are critical
for the evolving stream settings, as the process of model building needs to be fully
automated and continuous.},
journal = {SIGKDD Explor. Newsl.},
month = dec,
pages = {48–55},
numpages = {8},
keywords = {adaptive learning systems}
}

@inproceedings{designprinciples,
author = {Raeder, Troy and Stitelman, Ori and Dalessandro, Brian and Perlich, Claudia and Provost, Foster},
title = {Design Principles of Massive, Robust Prediction Systems},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2339530.2339740},
abstract = {Most data mining research is concerned with building high-quality classification models
in isolation. In massive production systems, however, the ability to monitor and maintain
performance over time while growing in size and scope is equally important. Many external
factors may degrade classification performance including changes in data distribution,
noise or bias in the source data, and the evolution of the system itself. A well-functioning
system must gracefully handle all of these. This paper lays out a set of design principles
for large-scale autonomous data mining systems and then demonstrates our application
of these principles within the m6d automated ad targeting system. We demonstrate a
comprehensive set of quality control processes that allow us monitor and maintain
thousands of distinct classification models automatically, and to add new models,
take on new data, and correct poorly-performing models without manual intervention
or system disruption.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1357–1365},
numpages = {9},
keywords = {data mining systems, quality control, data monitoring},
location = {Beijing, China},
series = {KDD '12}
}

@misc{googlemlops, title={MLOps: Continuous delivery and automation pipelines in machine learning}, url={https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning}, journal={Google Cloud Developer Guide}, author={Google},
urldate={2021-08-10}}

@article{iotsurvey,
author = {Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv},
title = {Orchestrating the Development Lifecycle of Machine Learning-Based IoT Applications: A Taxonomy and Survey},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
doi = {10.1145/3398020},
abstract = {Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML
techniques unlock the potential of IoT with intelligence, and IoT applications increasingly
feed data collected by sensors into ML models, thereby employing results to improve
their business processes and services. Hence, orchestrating ML pipelines that encompass
model training and implication involved in the holistic development lifecycle of an
IoT application often leads to complex system integration. This article provides a
comprehensive and systematic survey of the development lifecycle of ML-based IoT applications.
We outline the core roadmap and taxonomy and subsequently assess and compare existing
standard techniques used at individual stages.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {82},
numpages = {47},
keywords = {deep learning, orchestration, machine learning, IoT}
}

@misc{dapbook, title={Python Data Science Handbook}, url={https://jakevdp.github.io/PythonDataScienceHandbook/}, journal={Python Data Science Handbook | Python Data Science Handbook}, author={VanderPlas, Jake}, urldate={2021-08-20}}

@INPROCEEDINGS{delayedlabelstreams,
  author={Plasse, Joshua and Adams, Niall},
  booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
  title={Handling delayed labels in temporally evolving data streams}, 
  year={2016},
  volume={},
  number={},
  pages={2416-2424},
  doi={10.1109/BigData.2016.7840877}}
  
  @article{giraud-carrier_note_2000,
	title = {Note on the utility of incremental learning},
	volume = {13},
	issn = {09217126},
	number = {4},
	author = {Giraud-Carrier, C},
	year = {2000},
	pages = {215--223},
	annote = {cites 232. Defines incremental learning tasks (task where not all data is available at once and waiting for all is impossible or impractical) and incremental algorithms (algorithms that update their hypotheses while examples arrive). Argues that incrementality should be used only for incremental tasks.},
}


@inproceedings{bachPairedLearnersConcept2008,
  title = {Paired {{Learners}} for {{Concept Drift}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Bach, Stephen H. and Maloof, Marcus A.},
  date = {2008-12},
  pages = {23--32},
  publisher = {{IEEE}},
  location = {{Pisa, Italy}},
  doi = {10.1109/ICDM.2008.119},
  isbn = {978-0-7695-3502-9}
}

@inproceedings{baena-garciaEarlyDriftDetection2006,
  title = {Early {{Drift Detection Method}}},
  author = {Baena-García, Manuel and del Campo-Ávila, José and Fidalgo, Raúl and Bifet, Albert and Gavaldà, Ricard and Morales-Bueno, Rafael},
  options = {useprefix=true},
  date = {2006},
  eventtitle = {Fourth {{International Workshop}} on {{Knowledge Discovery}} from {{Data Streams}}}
}

@online{bakirovAutomatedAdaptationStrategies2021,
  title = {Automated {{Adaptation Strategies}} for {{Stream Learning}}},
  author = {Bakirov, Rashid and Gabrys, Bogdan and Fay, Damien},
  date = {2021-04-30},
  eprint = {1812.10793},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.10793},
  urldate = {2021-10-12},
  abstract = {Automation of machine learning model development is increasingly becoming an established research area. While automated model selection and automated data pre-processing have been studied in depth, there is, however, a gap concerning automated model adaptation strategies when multiple strategies are available. Manually developing an adaptation strategy can be time consuming and costly. In this paper we address this issue by proposing the use of flexible adaptive mechanism deployment for automated development of adaptation strategies. Experimental results after using the proposed strategies with five adaptive algorithms on 36 datasets confirm their viability. These strategies achieve better or comparable performance to the custom adaptation strategies and the repeated deployment of any single adaptive mechanism.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/87XLKU2W/Bakirov et al. - 2021 - Automated Adaptation Strategies for Stream Learnin.pdf;/home/riikoro/Zotero/storage/SBPVZEQ8/1812.html}
}

@inproceedings{bakirovSequencesDifferentAdaptive2015,
  title = {On Sequences of Different Adaptive Mechanisms in Non-Stationary Regression Problems},
  booktitle = {2015 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Bakirov, Rashid and Gabrys, Bogdan and Fay, Damien},
  date = {2015-07},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Killarney, Ireland}},
  doi = {10.1109/IJCNN.2015.7280779},
  eventtitle = {2015 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  isbn = {978-1-4799-1960-4}
}

@article{ben-nunDemystifyingParallelDistributed2019,
  title = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}: An {{In}}-Depth {{Concurrency Analysis}}},
  shorttitle = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}},
  author = {Ben-Nun, Tal and Hoefler, Torsten},
  date = {2019-09-18},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {52},
  number = {4},
  pages = {1--43},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3320060},
  abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
  langid = {english}
}

@book{bifetMachineLearningData2017,
  title = {Machine Learning for Data Streams: With Practical Examples in {{MOA}}},
  shorttitle = {Machine Learning for Data Streams},
  author = {Bifet, Albert},
  date = {2017},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{MIT Press}},
  location = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-03779-2},
  pagetotal = {262},
  keywords = {Data mining,Streaming technology (Telecommunications)}
}

@article{celikAdaptationStrategiesAutomated2021,
  title = {Adaptation {{Strategies}} for {{Automated Machine Learning}} on {{Evolving Data}}},
  author = {Celik, Bilge and Vanschoren, Joaquin},
  date = {2021-09-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {43},
  number = {9},
  eprint = {2006.06480},
  eprinttype = {arxiv},
  pages = {3067--3078},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3062900},
  abstract = {Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of data stream challenges such as concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on different AutoML approaches. We do this for a variety of AutoML approaches for building machine learning pipelines, including those that leverage Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/R7QCGRW6/Celik and Vanschoren - 2021 - Adaptation Strategies for Automated Machine Learni.pdf;/home/riikoro/Zotero/storage/PPK8GY6S/2006.html}
}

@online{chetlurCuDNNEfficientPrimitives2014,
  title = {{{cuDNN}}: Efficient {{Primitives}} for {{Deep Learning}}},
  shorttitle = {{{cuDNN}}},
  author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  date = {2014-12-17},
  eprint = {1410.0759},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1410.0759},
  urldate = {2021-10-21},
  abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36\% on a standard model while also reducing memory consumption.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Computer Science - Neural and Evolutionary Computing},
  file = {/home/riikoro/Zotero/storage/MCC49HWZ/Chetlur et al. - 2014 - cuDNN Efficient Primitives for Deep Learning.pdf;/home/riikoro/Zotero/storage/LBPVS7IF/1410.html}
}

@incollection{chuFastLightBoosting2004,
  title = {Fast and {{Light Boosting}} for {{Adaptive Mining}} of {{Data Streams}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chu, Fang and Zaniolo, Carlo},
  editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Dai, Honghua and Srikant, Ramakrishnan and Zhang, Chengqi},
  date = {2004},
  volume = {3056},
  pages = {282--292},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24775-3_36},
  isbn = {978-3-540-22064-0 978-3-540-24775-3}
}

@thesis{faithfullUnsupervisedChangeDetection2018,
  title = {Unsupervised {{Change Detection}} in {{Multivariate Streaming Data}}},
  author = {Faithfull, Will},
  date = {2018},
  institution = {{Unpublished}},
  url = {http://rgdoi.net/10.13140/RG.2.2.25121.66409},
  urldate = {2021-10-12},
  langid = {english}
}

@inproceedings{gamaLearningDriftDetection2004,
  title = {Learning with {{Drift Detection}}},
  booktitle = {Advances in {{Artificial Intelligence}} – {{SBIA}} 2004},
  author = {Gama, João and Medas, Pedro and Castillo, Gladys and Rodrigues, Pedro},
  editor = {Bazzan, Ana L. C. and Labidi, Sofiane},
  date = {2004},
  pages = {286--295},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  abstract = {Most of the work in machine learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the distribution that generate the examples changes over time. We present a method for detection of changes in the probability distribution of examples. The idea behind the drift detection method is to control the online error-rate of the algorithm. The training examples are presented in sequence. When a new training example is available, it is classified using the actual model. Statistical theory guarantees that while the distribution is stationary, the error will decrease. When the distribution changes, the error will increase. The method controls the trace of the online error of the algorithm. For the actual context we define a warning level, and a drift level. A new context is declared, if in a sequence of examples, the error increases reaching the warning level at example kw, and the drift level at example kd. This is an indication of a change in the distribution of the examples. The algorithm learns a new model using only the examples since kw. The method was tested with a set of eight artificial datasets and a real world dataset. We used three learning algorithms: a perceptron, a neural network and a decision tree. The experimental results show a good performance detecting drift and with learning the new concept. We also observe that the method is independent of the learning algorithm.},
  isbn = {978-3-540-28645-5}
}

@article{giraud-carrierNoteUtilityIncremental2000,
  title = {Note on the Utility of Incremental Learning},
  author = {Giraud-Carrier, C},
  date = {2000},
  volume = {13},
  number = {4},
  pages = {215--223},
  issn = {09217126}
}

@inproceedings{gomesLearningRecurringConcepts2011,
  title = {Learning Recurring Concepts from Data Streams with a Context-Aware Ensemble},
  booktitle = {Proceedings of the 2011 {{ACM Symposium}} on {{Applied Computing}} - {{SAC}} '11},
  author = {Gomes, João Bártolo and Menasalvas, Ernestina and Sousa, Pedro A. C.},
  date = {2011},
  pages = {994},
  publisher = {{ACM Press}},
  location = {{TaiChung, Taiwan}},
  doi = {10.1145/1982185.1982403},
  isbn = {978-1-4503-0113-8},
  langid = {english}
}

@inproceedings{hazelwoodAppliedMachineLearning2018,
  title = {Applied {{Machine Learning}} at {{Facebook}}: A {{Datacenter Infrastructure Perspective}}},
  shorttitle = {Applied {{Machine Learning}} at {{Facebook}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong},
  date = {2018-02},
  pages = {620--629},
  publisher = {{IEEE}},
  location = {{Vienna}},
  doi = {10.1109/HPCA.2018.00059},
  eventtitle = {2018 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  isbn = {978-1-5386-3659-6}
}

@article{heAutoMLSurveyStateoftheart2021,
  title = {{{AutoML}}: A Survey of the State-of-the-Art},
  shorttitle = {{{AutoML}}},
  author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  date = {2021-01},
  journaltitle = {Knowledge-Based Systems},
  volume = {212},
  pages = {106622},
  issn = {09507051},
  doi = {10.1016/j.knosys.2020.106622},
  langid = {english}
}

@online{hoiOnlineLearningComprehensive2018,
  title = {Online {{Learning}}: A {{Comprehensive Survey}}},
  shorttitle = {Online {{Learning}}},
  author = {Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
  date = {2018-10-22},
  eprint = {1802.02871},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.02871},
  urldate = {2021-10-05},
  abstract = {Online learning represents an important family of machine learning algorithms, in which a learner attempts to resolve an online prediction (or any type of decision-making) task by learning a model/hypothesis from a sequence of data instances one at a time. The goal of online learning is to ensure that the online learner would make a sequence of accurate predictions (or correct decisions) given the knowledge of correct answers to previous prediction or learning tasks and possibly additional information. This is in contrast to many traditional batch learning or offline machine learning algorithms that are often designed to train a model in batch from a given collection of training data instances. This survey aims to provide a comprehensive survey of the online machine learning literatures through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the learning type and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) supervised online learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) unsupervised online learning where there is no feedback available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/riikoro/Zotero/storage/URNXRIXE/Hoi et al. - 2018 - Online Learning A Comprehensive Survey.pdf;/home/riikoro/Zotero/storage/YX8WGMY3/Hoi et al. - 2018 - Online Learning A Comprehensive Survey.pdf;/home/riikoro/Zotero/storage/54QFL6W8/1802.html;/home/riikoro/Zotero/storage/V9TX4FRU/1802.html}
}

@inproceedings{hultenMiningTimechangingData2001,
  title = {Mining Time-Changing Data Streams},
  booktitle = {Proceedings of the Seventh {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '01},
  author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
  date = {2001},
  pages = {97--106},
  publisher = {{ACM Press}},
  location = {{San Francisco, California}},
  doi = {10.1145/502512.502529},
  isbn = {978-1-58113-391-2},
  langid = {english}
}

@article{klinkenbergLearningDriftingConcepts2004,
  title = {Learning Drifting Concepts: Example Selection vs. Example Weighting},
  shorttitle = {Learning Drifting Concepts},
  author = {Klinkenberg, Ralf},
  date = {2004-08},
  journaltitle = {Intelligent Data Analysis},
  volume = {8},
  number = {3},
  pages = {281--300},
  issn = {15714128, 1088467X},
  doi = {10.3233/IDA-2004-8305}
}

@inproceedings{kolterDynamicWeightedMajority2007,
  title = {Dynamic {{Weighted Majority}}: An {{Ensemble Method}} for {{Drifting Concepts}}},
  booktitle = {Journal of {{Machine Learning Research}}},
  author = {Kolter, J and Maloof, Marcus A},
  date = {2007},
  url = {https://www.jmlr.org/papers/volume8/kolter07a/kolter07a.pdf},
  urldate = {2021-09-16}
}

@inproceedings{kolterUsingAdditiveExpert2005,
  title = {Using Additive Expert Ensembles to Cope with Concept Drift},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning - {{ICML}} '05},
  author = {Kolter, Jeremy Z. and Maloof, Marcus A.},
  date = {2005},
  pages = {449--456},
  publisher = {{ACM Press}},
  location = {{Bonn, Germany}},
  doi = {10.1145/1102351.1102408},
  isbn = {978-1-59593-180-1},
  langid = {english}
}

@inproceedings{koychevGradualForgettingAdaptation2000,
  title = {Gradual {{Forgetting}} for {{Adaptation}} to {{Concept Drift}}},
  booktitle = {Proceedings of {{ECAI}} 2000 {{Workshop}} on {{Current Issues}} in {{Spatio}}-{{Temporal Reasoning}}},
  author = {Koychev, Ivan},
  date = {2000},
  url = {http://hdl.handle.net/10506/57},
  urldate = {2021-09-16}
}

@online{madridAutoMLPresenceDrift2019,
  title = {Towards {{AutoML}} in the Presence of {{Drift}}: First Results},
  shorttitle = {Towards {{AutoML}} in the Presence of {{Drift}}},
  author = {Madrid, Jorge G. and Escalante, Hugo Jair and Morales, Eduardo F. and Tu, Wei-Wei and Yu, Yang and Sun-Hosoya, Lisheng and Guyon, Isabelle and Sebag, Michele},
  date = {2019-07-24},
  eprint = {1907.10772},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.10772},
  urldate = {2021-10-12},
  abstract = {Research progress in AutoML has lead to state of the art solutions that can cope quite wellwith supervised learning task, e.g., classification with AutoSklearn. However, so far thesesystems do not take into account the changing nature of evolving data over time (i.e., theystill assume i.i.d. data); even when this sort of domains are increasingly available in realapplications (e.g., spam filtering, user preferences, etc.). We describe a first attempt to de-velop an AutoML solution for scenarios in which data distribution changes relatively slowlyover time and in which the problem is approached in a lifelong learning setting. We extendAuto-Sklearn with sound and intuitive mechanisms that allow it to cope with this sort ofproblems. The extended Auto-Sklearn is combined with concept drift detection techniquesthat allow it to automatically determine when the initial models have to be adapted. Wereport experimental results in benchmark data from AutoML competitions that adhere tothis scenario. Results demonstrate the effectiveness of the proposed methodology.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/R5LEKEBY/Madrid et al. - 2019 - Towards AutoML in the presence of Drift first res.pdf;/home/riikoro/Zotero/storage/U8MNMRPB/Madrid et al. - 2019 - Towards AutoML in the presence of Drift first res.pdf;/home/riikoro/Zotero/storage/6W9Y6V73/1907.html;/home/riikoro/Zotero/storage/JG5BB3XS/1907.html}
}

@inproceedings{maiKungFuMakingTraining2020,
  title = {{{KungFu}}: Making {{Training}} in {{Distributed Machine Learning Adaptive}}},
  booktitle = {14th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}} ({{OSDI}} 20)},
  author = {Mai, Luo and Li, Guo and Wagenländer, Marcel and Fertakis, Konstantinos and Brabete, Andrei-Octavian and Pietzuch, Peter},
  date = {2020-11},
  pages = {937--954},
  publisher = {{USENIX Association}},
  url = {https://www.usenix.org/conference/osdi20/presentation/mai},
  isbn = {978-1-939133-19-9}
}

@article{maloofIncrementalLearningPartial2004,
  title = {Incremental Learning with Partial Instance Memory},
  author = {Maloof, Marcus A. and Michalski, Ryszard S.},
  date = {2004-04},
  journaltitle = {Artificial Intelligence},
  volume = {154},
  number = {1-2},
  pages = {95--126},
  issn = {00043702},
  doi = {10.1016/j.artint.2003.04.001},
  langid = {english}
}

@inproceedings{maloofMethodPartialmemoryIncremental1995,
  title = {A Method for Partial-Memory Incremental Learning and Its Application to Computer Intrusion Detection},
  booktitle = {Proceedings of 7th {{IEEE International Conference}} on {{Tools}} with {{Artificial Intelligence}}},
  author = {Maloof, M.A. and Michalski, R.S.},
  date = {1995},
  pages = {392--397},
  publisher = {{IEEE Comput. Soc. Press}},
  location = {{Herndon, VA, USA}},
  doi = {10.1109/TAI.1995.479784},
  isbn = {978-0-8186-7312-2}
}

@article{maloofSelectingExamplesPartial2000,
  title = {Selecting {{Examples}} for {{Partial Memory Learning}}},
  author = {Maloof, Marcus A. and Michalski, Ryszard S.},
  date = {2000},
  journaltitle = {Machine Learning},
  volume = {41},
  number = {1},
  pages = {27--52},
  issn = {08856125},
  doi = {10.1023/A:1007661119649}
}

@article{minkuDDDNewEnsemble2012,
  title = {{{DDD}}: A {{New Ensemble Approach}} for {{Dealing}} with {{Concept Drift}}},
  shorttitle = {{{DDD}}},
  author = {Minku, Leandro L. and Yao, Xin},
  date = {2012-04},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {24},
  number = {4},
  pages = {619--633},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2011.58}
}

@article{minkuImpactDiversityOnline2010,
  title = {The {{Impact}} of {{Diversity}} on {{Online Ensemble Learning}} in the {{Presence}} of {{Concept Drift}}},
  author = {Minku, L.L. and White, A.P. and {Xin Yao}},
  date = {2010-05},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {22},
  number = {5},
  pages = {730--742},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2009.156}
}

@inproceedings{ozaExperimentalComparisonsOnline2001,
  title = {Experimental Comparisons of Online and Batch Versions of Bagging and Boosting},
  booktitle = {Proceedings of the Seventh {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '01},
  author = {Oza, Nikunj C. and Russell, Stuart},
  date = {2001},
  pages = {359--364},
  publisher = {{ACM Press}},
  location = {{San Francisco, California}},
  doi = {10.1145/502512.502565},
  isbn = {978-1-58113-391-2},
  langid = {english}
}

@incollection{pesaranghaderFastHoeffdingDrift2016,
  title = {Fast {{Hoeffding Drift Detection Method}} for {{Evolving Data Streams}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Pesaranghader, Ali and Viktor, Herna L.},
  editor = {Frasconi, Paolo and Landwehr, Niels and Manco, Giuseppe and Vreeken, Jilles},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {9852},
  pages = {96--111},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-46227-1_7},
  isbn = {978-3-319-46226-4 978-3-319-46227-1},
  langid = {english}
}

@article{schlimmerIncrementalLearningNoisy1986,
  title = {Incremental Learning from Noisy Data},
  author = {Schlimmer, Jeffrey C. and Granger, Richard H.},
  date = {1986-09},
  journaltitle = {Machine Learning},
  volume = {1},
  number = {3},
  pages = {317--354},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00116895},
  langid = {english}
}

@article{szeEfficientProcessingDeep2017,
  title = {Efficient {{Processing}} of {{Deep Neural Networks}}: A {{Tutorial}} and {{Survey}}},
  shorttitle = {Efficient {{Processing}} of {{Deep Neural Networks}}},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  date = {2017-12},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {105},
  number = {12},
  pages = {2295--2329},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2017.2761740},
  file = {/home/riikoro/Zotero/storage/6TNGNIDR/Sze et al. - 2017 - Efficient Processing of Deep Neural Networks A Tu.pdf}
}

@article{velosoHyperparameterSelftuningData2021,
  title = {Hyperparameter Self-Tuning for Data Streams},
  author = {Veloso, Bruno and Gama, João and Malheiro, Benedita and Vinagre, João},
  date = {2021-12},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {76},
  pages = {75--86},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.04.011},
  langid = {english}
}

@inproceedings{wangMiningConceptdriftingData2003,
  title = {Mining Concept-Drifting Data Streams Using Ensemble Classifiers},
  booktitle = {Proceedings of the Ninth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '03},
  author = {Wang, Haixun and Fan, Wei and Yu, Philip S. and Han, Jiawei},
  date = {2003},
  pages = {226},
  publisher = {{ACM Press}},
  location = {{Washington, D.C.}},
  doi = {10.1145/956750.956778},
  isbn = {978-1-58113-737-8},
  langid = {english}
}

@article{webbCharacterizingConceptDrift2016,
  title = {Characterizing Concept Drift},
  author = {Webb, Geoffrey I. and Hyde, Roy and Cao, Hong and Nguyen, Hai Long and Petitjean, Francois},
  date = {2016-07},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Min Knowl Disc},
  volume = {30},
  number = {4},
  pages = {964--994},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-015-0448-4},
  langid = {english},
  file = {/home/riikoro/Zotero/storage/XMWI3BYM/Webb et al. - 2016 - Characterizing concept drift.pdf}
}

@incollection{widmerEffectiveLearningDynamic1993,
  title = {Effective Learning in Dynamic Environments by Explicit Context Tracking},
  booktitle = {Machine {{Learning}}: {{ECML}}-93},
  author = {Widmer, Gerhard and Kubat, Miroslav},
  editor = {Siekmann, J. and Goos, G. and Hartmanis, J. and Brazdil, Pavel B.},
  date = {1993},
  volume = {667},
  pages = {227--243},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-56602-3_139},
  isbn = {978-3-540-56602-1 978-3-540-47597-2}
}

@incollection{williamsonThoughtExperiments,
  title = {Thought Experiments},
  booktitle = {Doing {{Philosophy}}},
  author = {Williamson, Timothy}
}

@online{youLargeBatchTraining2017,
  title = {Large {{Batch Training}} of {{Convolutional Networks}}},
  author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
  date = {2017-09-13},
  eprint = {1708.03888},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1708.03888},
  urldate = {2021-10-05},
  abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/riikoro/Zotero/storage/6XRZSNFS/You et al. - 2017 - Large Batch Training of Convolutional Networks.pdf;/home/riikoro/Zotero/storage/32JRGHMM/1708.html}
}

@inproceedings{youLargebatchTrainingLSTM2019,
  title = {Large-Batch Training for {{LSTM}} and Beyond},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {You, Yang and Hseu, Jonathan and Ying, Chris and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  date = {2019-11-17},
  pages = {1--16},
  publisher = {{ACM}},
  location = {{Denver Colorado}},
  doi = {10.1145/3295500.3356137},
  eventtitle = {{{SC}} '19: The {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}}, and {{Analysis}}},
  isbn = {978-1-4503-6229-0},
  langid = {english},
  file = {/home/riikoro/Zotero/storage/HGL2ATJ8/You et al. - 2019 - Large-batch training for LSTM and beyond.pdf}
}

@online{youLimitBatchSize2020,
  title = {The {{Limit}} of the {{Batch Size}}},
  author = {You, Yang and Wang, Yuhui and Zhang, Huan and Zhang, Zhao and Demmel, James and Hsieh, Cho-Jui},
  date = {2020-06-15},
  eprint = {2006.08517},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.08517},
  urldate = {2021-10-05},
  abstract = {Large-batch training is an efficient approach for current distributed deep learning systems. It has enabled researchers to reduce the ImageNet/ResNet-50 training from 29 hours to around 1 minute. In this paper, we focus on studying the limit of the batch size. We think it may provide a guidance to AI supercomputer and algorithm designers. We provide detailed numerical optimization instructions for step-by-step comparison. Moreover, it is important to understand the generalization and optimization performance of huge batch training. Hoffer et al. introduced "ultra-slow diffusion" theory to large-batch training. However, our experiments show contradictory results with the conclusion of Hoffer et al. We provide comprehensive experimental results and detailed analysis to study the limitations of batch size scaling and "ultra-slow diffusion" theory. For the first time we scale the batch size on ImageNet to at least a magnitude larger than all previous work, and provide detailed studies on the performance of many state-of-the-art optimization schemes under this setting. We propose an optimization recipe that is able to improve the top-1 test accuracy by 18\% compared to the baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/SDMNT6CR/You et al. - 2020 - The Limit of the Batch Size.pdf;/home/riikoro/Zotero/storage/XDFC6SAN/2006.html}
}

@thesis{zliobaiteAdaptiveTrainingSet2010,
  title = {Adaptive Training Set Formation},
  author = {Žliobaitė, Indrė},
  date = {2010},
  institution = {{Vilnius University}},
  location = {{Vilnus}},
  url = {https://epublications.vu.lt/object/elaba:1932399/},
  urldate = {2021-10-12}
}

@online{zliobaiteLearningConceptDrift2010,
  title = {Learning under {{Concept Drift}}: An {{Overview}}},
  shorttitle = {Learning under {{Concept Drift}}},
  author = {Žliobaitė, Indrė},
  date = {2010-10},
  eprint = {1010.4784},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1010.4784},
  urldate = {2021-09-13},
  abstract = {Concept drift refers to a non stationary learning problem over time. The training and the application data often mismatch in real life problems. In this report we present a context of concept drift problem 1. We focus on the issues relevant to adaptive training set formation. We present the framework and terminology, and formulate a global picture of concept drift learners design. We start with formalizing the framework for the concept drifting data in Section 1. In Section 2 we discuss the adaptivity mechanisms of the concept drift learners. In Section 3 we overview the principle mechanisms of concept drift learners. In this chapter we give a general picture of the available algorithms and categorize them based on their properties. Section 5 discusses the related research fields and Section 5 groups and presents major concept drift applications. This report is intended to give a bird's view of concept drift research field, provide a context of the research and position it within broad spectrum of research fields and applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}
