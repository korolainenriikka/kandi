@inproceedings{edgelatency,
author = {Chen, Zhuo and Hu, Wenlu and Wang, Junjue and Zhao, Siyan and Amos, Brandon and Wu, Guanhang and Ha, Kiryong and Elgazzar, Khalid and Pillai, Padmanabhan and Klatzky, Roberta and Siewiorek, Daniel and Satyanarayanan, Mahadev},
title = {An Empirical Study of Latency in an Emerging Class of Edge Computing Applications for Wearable Cognitive Assistance},
year = {2017},
isbn = {9781450350877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3132211.3134458},
booktitle = {Proceedings of the Second ACM/IEEE Symposium on Edge Computing},
articleno = {14},
numpages = {14},
keywords = {hololens, edge computing, mobile computing, smart glass, augmented reality, cloud computing, cloudlet},
location = {San Jose, California},
series = {SEC '17}
}

@article{millwheel,
author = {Akidau, Tyler and Balikov, Alex and Bekiro\u{g}lu, Kaya and Chernyak, Slava and Haberman, Josh and Lax, Reuven and McVeety, Sam and Mills, Daniel and Nordstrom, Paul and Whittle, Sam},
title = {MillWheel: Fault-Tolerant Stream Processing at Internet Scale},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
doi = {10.14778/2536222.2536229},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1033–1044},
numpages = {12}
}

@article{dataflow,
author = {Akidau, Tyler and Bradshaw, Robert and Chambers, Craig and Chernyak, Slava and Fern\'{a}ndez-Moctezuma, Rafael J. and Lax, Reuven and McVeety, Sam and Mills, Daniel and Perry, Frances and Schmidt, Eric and Whittle, Sam},
title = {The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, out-of-Order Data Processing},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
doi = {10.14778/2824032.2824076},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1792–1803},
numpages = {12}
}

@inproceedings{storm@twitter,
author = {Toshniwal, Ankit and Taneja, Siddarth and Shukla, Amit and Ramasamy, Karthik and Patel, Jignesh M. and Kulkarni, Sanjeev and Jackson, Jason and Gade, Krishna and Fu, Maosong and Donham, Jake and Bhagat, Nikunj and Mittal, Sailesh and Ryaboy, Dmitriy},
title = {Storm@twitter},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2588555.2595641},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {147–156},
numpages = {10},
keywords = {real-time query processing, stream data management},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{facebook,
author = {Chen, Guoqiang Jerry and Wiener, Janet L. and Iyer, Shridhar and Jaiswal, Anshul and Lei, Ran and Simha, Nikhil and Wang, Wei and Wilfong, Kevin and Williamson, Tim and Yilmaz, Serhat},
title = {Realtime Data Processing at Facebook},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2882903.2904441},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1087–1098},
numpages = {12},
keywords = {realtime data processing, stream processing},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{uber,
author = {Fu, Yupeng and Soman, Chinmay},
title = {Real-Time Data Infrastructure at Uber},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3448016.3457552},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2503–2516},
numpages = {14},
keywords = {streaming processing, real-time infrastructure},
location = {Virtual Event, China},
series = {SIGMOD/PODS '21}
}

@book{maritimeinformatics,
author = {Artikis, Alexander and Zissis, Dimitris},
year = {2021},
month = {01},
pages = {333},
title = {Guide to Maritime Informatics},
isbn = {978-3-030-61851-3},
doi = {10.1007/978-3-030-61852-0}
}

@inproceedings{lambdakappa,
author = {Feick, Martin AND Kleer, Niko AND Kohn, Marek},
title = {Fundamentals of Real-Time Data Processing Architectures Lambda and Kappa},
booktitle = {SKILL 2018 - Studierendenkonferenz Informatik},
year = {2018},
editor = {Becker, Michael} ,
pages = { 55-66 },
publisher = {Gesellschaft für Informatik e.V.},
address = {Bonn}
}

@article{mapreduce,
author = {Dean, Jeffrey and Ghemawat, Sanjay},
title = {MapReduce: Simplified Data Processing on Large Clusters},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0001-0782},
doi = {10.1145/1327452.1327492},
journal = {Commun. ACM},
month = jan,
pages = {107–113},
numpages = {7}
}

@INPROCEEDINGS{geolytics,
  author={Cheng, Bin and Papageorgiou, Apostolos and Cirillo, Flavio and Kovacs, Ernoe},
  booktitle={2015 IEEE 2nd World Forum on Internet of Things (WF-IoT)}, 
  title={GeeLytics: Geo-distributed edge analytics for large scale IoT systems based on dynamic topology}, 
  year={2015},
  volume={},
  number={},
  pages={565-570},
  doi={10.1109/WF-IoT.2015.7389116}}

@INPROCEEDINGS{edgeiot,
  author={Kumar, Umesh and Verma, Parul and Qamar Abbas, Syed},
  booktitle={2021 International Conference on Computer Communication and Informatics (ICCCI)}, 
  title={Bringing Edge Computing into IoT Architecture to Improve IoT Network Performance}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICCCI50826.2021.9402499}}

@ARTICLE{iotsystems,
  author={Swamy, S. Narasimha and Kota, Solomon Raju},
  journal={IEEE Access}, 
  title={An Empirical Study on System Level Aspects of Internet of Things (IoT)}, 
  year={2020},
  volume={8},
  number={},
  pages={188082-188134},
  doi={10.1109/ACCESS.2020.3029847}}


@inbook{mliot,
author = {Boovaraghavan, Sudershan and Maravi, Anurag and Mallela, Prahaladha and Agarwal, Yuvraj},
title = {MLIoT: An End-to-End Machine Learning System for the Internet-of-Things},
year = {2021},
isbn = {9781450383547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450268.3453522},
booktitle = {Proceedings of the International Conference on Internet-of-Things Design and Implementation},
pages = {169–181},
numpages = {13}
}

@inproceedings{apachesurvey,
author = {Nasiri, Hamid and Nasehi, Saeed and Goudarzi, Maziar},
title = {A Survey of Distributed Stream Processing Systems for Smart City Data Analytics},
year = {2018},
isbn = {9781450365321},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3269961.3282845},
booktitle = {Proceedings of the International Conference on Smart Cities and Internet of Things},
articleno = {12},
numpages = {7},
keywords = {Big data, Stream processing, Internet of Things, Distributed computing, Smart City},
location = {Mashhad, Iran},
series = {SCIOT '18}
}

@inproceedings{gpuinsmartcity,
author = {Shang, Lei and Lin, Ching Yeh and Atif, Muhammad and Williams, Allan},
title = {Evaluation of High Density GPUs as Sustainable Smart City Infrastructure},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {482–487},
numpages = {6},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{e2eiotstack,
author = {Hamann, Arne and Saidi, Selma and Ginthoer, David and Wietfeld, Christian and Ziegenbein, Dirk},
title = {Building End-to-End IoT Applications with QoS Guarantees},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {112},
numpages = {6},
location = {Virtual Event, USA},
series = {DAC '20}
}

@INPROCEEDINGS{i4sea,
  author={Tampakis, Panagiotis and Chondrodima, Eva and Pikrakis, Aggelos and Theodoridis, Yannis and Pristouris, Kostis and Nakos, Harry and Petra, Eleni and Dalamagas, Theodore and Kandiros, Andreas and Markakis, Georgios and Maina, Irida and Kavadas, Stefanos},
  booktitle={2020 21st IEEE International Conference on Mobile Data Management (MDM)}, 
  title={Sea Area Monitoring and Analysis of Fishing Vessels Activity: The i4sea Big Data Platform}, 
  year={2020},
  volume={},
  number={},
  pages={275-280},
  doi={10.1109/MDM48529.2020.00063}}
  
  @inproceedings{uprctrajectorysystem,
author = {Petrou, Petros and Nikitopoulos, Panagiotis and Tampakis, Panagiotis and Glenis, Apostolos and Koutroumanis, Nikolaos and Santipantakis, Georgios M. and Patroumpas, Kostas and Vlachou, Akrivi and Georgiou, Harris and Chondrodima, Eva and Doulkeridis, Christos and Pelekis, Nikos and Andrienko, Gennady L. and Patterson, Fabian and Fuchs, Georg and Theodoridis, Yannis and Vouros, George A.},
title = {ARGO: A Big Data Framework for Online Trajectory Prediction},
year = {2019},
isbn = {9781450362801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3340964.3340988},
booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
pages = {194–197},
numpages = {4},
keywords = {geostreaming, mobility events, trajectories, location prediction},
location = {Vienna, Austria},
series = {SSTD '19}
}

 @misc{questioninglambda, title={Questioning the Lambda Architecture}, url={https://www.oreilly.com/radar/questioning-the-lambda-architecture/}, journal={O'Reilly Radar}, publisher={O'Reilly}, author={Kreps, Jay}, year={2014}, month={7}, urldate={2021-07-23}} 

 @misc{beatingcap, title={How to beat the CAP theorem}, url={http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html}, journal={thoughts from the red planet}, author={Marz, Nathan}, year={2011}, month={10}, urldate={2021-07-23}} 
 
 @misc{D4.1,
  author       = "VesselAI",
  title        = "D4.1. Specification of The AI On-Demand Platform Extensions and Research Activities",
  day          = 30,
  month        = "June",
  year         = "2021",
}
 
  @misc{D1.1,
  author       = "VesselAI",
  title        = "D1.1 -State-of-the-art Analysis and Data Sources",
  day          = 29,
  month        = 4,
  year         = "2021",
}
 

@Article{edgefogcloud,
AUTHOR = {Cao, Hung and Wachowicz, Monica},
TITLE = {An Edge-Fog-Cloud Architecture of Streaming Analytics for Internet of Things Applications},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3594},
URL = {https://www.mdpi.com/1424-8220/19/16/3594},
ISSN = {1424-8220},
ABSTRACT = {Exploring Internet of Things (IoT) data streams generated by smart cities means not only transforming data into better business decisions in a timely way but also generating long-term location intelligence for developing new forms of urban governance and organization policies. This paper proposes a new architecture based on the edge-fog-cloud continuum to analyze IoT data streams for delivering data-driven insights in a smart parking scenario.},
DOI = {10.3390/s19163594}
}

@INPROCEEDINGS{anomalysystem,
  author={Chatzikokolakis, Konstantinos and Zissis, Dimitris and Vodas, Marios and Spiliopoulos, Giannis and Kontopoulos, Ioannis},
  booktitle={OCEANS 2019 - Marseille}, 
  title={A distributed lightning fast maritime anomaly detection service}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/OCEANSE.2019.8867269}}

 @article{thelambdarant, title={The Lambda and the Kappa}, journal={Big Data Bites}, author={Lin, Jimmy}, year={2017}, pages={60–66}, urldate={2021-07-25}} 
 
 @article{fogsurvey,
author = {Puliafito, Carlo and Mingozzi, Enzo and Longo, Francesco and Puliafito, Antonio and Rana, Omer},
title = {Fog Computing for the Internet of Things: A Survey},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1533-5399},
doi = {10.1145/3301443},
abstract = {Research in the Internet of Things (IoT) conceives a world where everyday objects
are connected to the Internet and exchange, store, process, and collect data from
the surrounding environment. IoT devices are becoming essential for supporting the
delivery of data to enable electronic services, but they are not sufficient in most
cases to host application services directly due to their intrinsic resource constraints.
Fog Computing (FC) can be a suitable paradigm to overcome these limitations, as it
can coexist and cooperate with centralized Cloud systems and extends the latter toward
the network edge. In this way, it is possible to distribute resources and services
of computing, storage, and networking along the Cloud-to-Things continuum. As such,
FC brings all the benefits of Cloud Computing (CC) closer to end (user) devices. This
article presents a survey on the employment of FC to support IoT devices and services.
The principles and literature characterizing FC are described, highlighting six IoT
application domains that may benefit from the use of this paradigm. The extension
of Cloud systems towards the network edge also creates new challenges and can have
an impact on existing approaches employed in Cloud-based deployments. Research directions
being adopted by the community are highlighted, with an indication of which of these
are likely to have the greatest impact. An overview of existing FC software and hardware
platforms for the IoT is also provided, along with the standardisation efforts in
this area initiated by the OpenFog Consortium (OFC).},
journal = {ACM Trans. Internet Technol.},
month = apr,
articleno = {18},
numpages = {41},
keywords = {cloud computing, Fog computing, internet of things, topological proximity}
}

@INPROCEEDINGS{apachebenchmarkI,
  author={Qian, Shilei and Wu, Gang and Huang, Jie and Das, Tathagata},
  booktitle={2016 IEEE International Conference on Industrial Technology (ICIT)}, 
  title={Benchmarking modern distributed streaming platforms}, 
  year={2016},
  volume={},
  number={},
  pages={592-598},
  doi={10.1109/ICIT.2016.7474816}}

@INPROCEEDINGS{apachebenchmarkII,
  author={Chintapalli, Sanket and Dagit, Derek and Evans, Bobby and Farivar, Reza and Graves, Thomas and Holderbaugh, Mark and Liu, Zhuo and Nusbaum, Kyle and Patil, Kishorkumar and Peng, Boyang Jerry and Poulosky, Paul},
  booktitle={2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming}, 
  year={2016},
  volume={},
  number={},
  pages={1789-1792},
  doi={10.1109/IPDPSW.2016.138}}
  
  @inproceedings{maritimetradvsbigdata,
  title={Anomaly detection in the maritime domain: Comparison of traditional and big data approach},
  author={Filipiak, Dominik and Str{\'o}zyna, Milena and Wecel, Krzysztof and Abramowicz, Witold},
  booktitle={NATO IST-160-RSM Specialists' Meeting on Big Data and Artificial Intelligence for Military Decision Makin},
  year={2018}
}

 @INPROCEEDINGS{timecriticalmarineaero,
author = {George A. Vouros,
Christos Doulkeridis,
Georgios Santipantakis,
Akrivi Vlachou,
Nikos Pelekis,
Harilaos Georgiou,
Yiannis Theodoridis,
Kostas Patroumpas,
Georg Fuchs, Michael Mock,
Gennady Andrienko,
Natalia Andrienko,
Cyril Ray,
Christophe Claramunt
Ecole Navale / ENSAM, France
Elena Camossi,
Anne-Laure Jousselme,
David Scarlatti,
Jose Manuel Cordero},
title = {Big Data Analytics for Time Critical Maritime and Aerial
Mobility Forecasting},
booktitle={Proceedings of the 21st
International Conference on Extending Database Technology (EDBT)},
year = {2018}
}

@article{lmlinneuralnets,
title = {Continual lifelong learning with neural networks: A review},
journal = {Neural Networks},
volume = {113},
pages = {54-71},
year = {2019},
issn = {0893-6080},
doi = {10.1016/j.neunet.2019.01.012},
author = {German I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and Stefan Wermter},
keywords = {Continual learning, Lifelong learning, Catastrophic forgetting, Developmental systems, Memory consolidation},
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.}
}

@INPROCEEDINGS{lmlsystemframework,
  author={Martinez Plumed, Fernando and Ferri, Cesar and Hernandez Orallo, Jose and Ramirez Quintana, Maria Jose},
  booktitle={2014 13th International Conference on Machine Learning and Applications}, 
  title={A Knowledge Growth and Consolidation Framework for Lifelong Machine Learning Systems}, 
  year={2014},
  volume={},
  number={},
  pages={111-116},
  doi={10.1109/ICMLA.2014.23}}
  
  @INPROCEEDINGS{multilambdaforlml,
  author={Pal, Gautam and Li, Gangmin and Atkinson, Katie},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
  title={Big Data Ingestion and Lifelong Learning Architecture}, 
  year={2018},
  volume={},
  number={},
  pages={5420-5423},
  doi={10.1109/BigData.2018.8621859}}

@inproceedings{lmlsystems,
  title={Lifelong machine learning systems: Beyond learning algorithms},
  author={Silver, Daniel L and Yang, Qiang and Li, Lianghao},
  booktitle={2013 AAAI spring symposium series},
  year={2013}
}

@article{conceptdriftsurvey,
author = {Gama, Jo\~{a}o and Žliobaitė, Indrė and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
title = {A Survey on Concept Drift Adaptation},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0360-0300},
doi = {10.1145/2523813},
abstract = {Concept drift primarily refers to an online supervised learning scenario when the
relation between the input data and the target variable changes over time. Assuming
a general knowledge of supervised learning in this article, we characterize adaptive
learning processes; categorize existing strategies for handling concept drift; overview
the most representative, distinct, and popular techniques and algorithms; discuss
evaluation methodology of adaptive algorithms; and present a set of illustrative applications.
The survey covers the different facets of concept drift in an integrated way to reflect
on the existing scattered state of the art. Thus, it aims at providing a comprehensive
introduction to the concept drift adaptation for researchers, industry analysts, and
practitioners.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {44},
numpages = {37},
keywords = {Concept drift, adaptive learning, change detection, data streams},
}

@article{mlforstreamingsurvey,
author = {Gomes, Heitor Murilo and Read, Jesse and Bifet, Albert and Barddal, Jean Paul and Gama, Jo\~{a}o},
title = {Machine Learning for Streaming Data: State of the Art, Challenges, and Opportunities},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1931-0145},
doi = {10.1145/3373464.3373470},
abstract = {Incremental learning, online learning, and data stream learning are terms commonly
associated with learning algorithms that update their models given a continuous influx
of data without performing multiple passes over data. Several works have been devoted
to this area, either directly or indirectly as characteristics of big data processing,
i.e., Velocity and Volume. Given the current industry needs, there are many challenges
to be addressed before existing methods can be efficiently applied to real-world problems.
In this work, we focus on elucidating the connections among the current stateof- the-art
on related fields; and clarifying open challenges in both academia and industry. We
treat with special care topics that were not thoroughly investigated in past position
and survey papers. This work aims to evoke discussion and elucidate the current research
opportunities, highlighting the relationship of different subareas and suggesting
courses of action when possible.},
journal = {SIGKDD Explor. Newsl.},
month = nov,
pages = {6–22},
numpages = {17}
}

@article{streamminingchallenges,
author = {Krempl, Georg and Žliobaitė, Indrė and Brzezi\'{n}ski, Dariusz and H\"{u}llermeier, Eyke and Last, Mark and Lemaire, Vincent and Noack, Tino and Shaker, Ammar and Sievi, Sonja and Spiliopoulou, Myra and Stefanowski, Jerzy},
title = {Open Challenges for Data Stream Mining Research},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1931-0145},
doi = {10.1145/2674026.2674028},
abstract = {Every day, huge volumes of sensory, transactional, and web data are continuously generated
as streams, which need to be analyzed online as they arrive. Streaming data can be
considered as one of the main sources of what is called big data. While predictive
modeling for data streams and big data have received a lot of attention over the last
decade, many research approaches are typically designed for well-behaved controlled
problem settings, overlooking important challenges imposed by real-world applications.
This article presents a discussion on eight open challenges for data stream mining.
Our goal is to identify gaps between current research and meaningful applications,
highlight open problems, and define new application-relevant research directions for
data stream mining. The identified challenges cover the full cycle of knowledge discovery
and involve such problems as: protecting data privacy, dealing with legacy systems,
handling incomplete and delayed information, analysis of complex data, and evaluation
of stream mining algorithms. The resulting analysis is illustrated by practical applications
and provides general suggestions concerning lines of future research in data stream
mining.},
journal = {SIGKDD Explor. Newsl.},
month = sep,
pages = {1–10},
numpages = {10}
}

@article{adaptivelearningsystems,
author = {Žliobaitė, Indrė and Bifet, Albert and Gaber, Mohamed and Gabrys, Bogdan and Gama, Joao and Minku, Leandro and Musial, Katarzyna},
title = {Next Challenges for Adaptive Learning Systems},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1931-0145},
doi = {10.1145/2408736.2408746},
abstract = {Learning from evolving streaming data has become a 'hot' research topic in the last
decade and many adaptive learning algorithms have been developed. This research was
stimulated by rapidly growing amounts of industrial, transactional, sensor and other
business data that arrives in real time and needs to be mined in real time. Under
such circumstances, constant manual adjustment of models is in-efficient and with
increasing amounts of data is becoming infeasible. Nevertheless, adaptive learning
models are still rarely employed in business applications in practice. In the light
of rapidly growing structurally rich 'big data', new generation of parallel computing
solutions and cloud computing services as well as recent advances in portable computing
devices, this article aims to identify the current key research directions to be taken
to bring the adaptive learning closer to application needs. We identify six forthcoming
challenges in designing and building adaptive learning (pre-diction) systems: making
adaptive systems scalable, dealing with realistic data, improving usability and trust,
integrat-ing expert knowledge, taking into account various application needs, and
moving from adaptive algorithms towards adaptive tools. Those challenges are critical
for the evolving stream settings, as the process of model building needs to be fully
automated and continuous.},
journal = {SIGKDD Explor. Newsl.},
month = dec,
pages = {48–55},
numpages = {8},
keywords = {adaptive learning systems}
}

@inproceedings{designprinciples,
author = {Raeder, Troy and Stitelman, Ori and Dalessandro, Brian and Perlich, Claudia and Provost, Foster},
title = {Design Principles of Massive, Robust Prediction Systems},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2339530.2339740},
abstract = {Most data mining research is concerned with building high-quality classification models
in isolation. In massive production systems, however, the ability to monitor and maintain
performance over time while growing in size and scope is equally important. Many external
factors may degrade classification performance including changes in data distribution,
noise or bias in the source data, and the evolution of the system itself. A well-functioning
system must gracefully handle all of these. This paper lays out a set of design principles
for large-scale autonomous data mining systems and then demonstrates our application
of these principles within the m6d automated ad targeting system. We demonstrate a
comprehensive set of quality control processes that allow us monitor and maintain
thousands of distinct classification models automatically, and to add new models,
take on new data, and correct poorly-performing models without manual intervention
or system disruption.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1357–1365},
numpages = {9},
keywords = {data mining systems, quality control, data monitoring},
location = {Beijing, China},
series = {KDD '12}
}

@misc{googlemlops, title={MLOps: Continuous delivery and automation pipelines in machine learning}, url={https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning}, journal={Google Cloud Developer Guide}, author={Google},
urldate={2021-08-10}}

@article{iotsurvey,
author = {Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv},
title = {Orchestrating the Development Lifecycle of Machine Learning-Based IoT Applications: A Taxonomy and Survey},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
doi = {10.1145/3398020},
abstract = {Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML
techniques unlock the potential of IoT with intelligence, and IoT applications increasingly
feed data collected by sensors into ML models, thereby employing results to improve
their business processes and services. Hence, orchestrating ML pipelines that encompass
model training and implication involved in the holistic development lifecycle of an
IoT application often leads to complex system integration. This article provides a
comprehensive and systematic survey of the development lifecycle of ML-based IoT applications.
We outline the core roadmap and taxonomy and subsequently assess and compare existing
standard techniques used at individual stages.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {82},
numpages = {47},
keywords = {deep learning, orchestration, machine learning, IoT}
}

@misc{dapbook, title={Python Data Science Handbook}, url={https://jakevdp.github.io/PythonDataScienceHandbook/}, journal={Python Data Science Handbook | Python Data Science Handbook}, author={VanderPlas, Jake}, urldate={2021-08-20}}

@article{zliobaite_driftsurvey,
	title = {Learning under {Concept} {Drift}: an {Overview}},
	shorttitle = {Learning under {Concept} {Drift}},
	url = {http://arxiv.org/abs/1010.4784},
	abstract = {Concept drift refers to a non stationary learning problem over time. The training and the application data often mismatch in real life problems. In this report we present a context of concept drift problem 1. We focus on the issues relevant to adaptive training set formation. We present the framework and terminology, and formulate a global picture of concept drift learners design. We start with formalizing the framework for the concept drifting data in Section 1. In Section 2 we discuss the adaptivity mechanisms of the concept drift learners. In Section 3 we overview the principle mechanisms of concept drift learners. In this chapter we give a general picture of the available algorithms and categorize them based on their properties. Section 5 discusses the related research fields and Section 5 groups and presents major concept drift applications. This report is intended to give a bird's view of concept drift research field, provide a context of the research and position it within broad spectrum of research fields and applications.},
	urldate = {2021-09-13},
	journal = {arXiv:1010.4784 [cs]},
	author = {Žliobaitė, Indrė},
	month = oct,
	year = {2010},
	note = {arXiv: 1010.4784},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {A formal definition of concept drift, accompanied by a taxonomy of mitigation techniques and discussion of these with a broad bibliography. Gives general statements on which strategies to go for under which types of drifting.},
	annote = {Comment: Technical report, Vilnius University, 2009 techniques, related areas, applications},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/YTYX9JIE/Žliobaitė - 2010 - Learning under Concept Drift an Overview.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/SPKESR8W/1010.html:text/html},
}

@INPROCEEDINGS{delayedlabelstreams,
  author={Plasse, Joshua and Adams, Niall},
  booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
  title={Handling delayed labels in temporally evolving data streams}, 
  year={2016},
  volume={},
  number={},
  pages={2416-2424},
  doi={10.1109/BigData.2016.7840877}}
  
  @article{giraud-carrier_note_2000,
	title = {Note on the utility of incremental learning},
	volume = {13},
	issn = {09217126},
	number = {4},
	author = {Giraud-Carrier, C},
	year = {2000},
	pages = {215--223},
	annote = {cites 232. Defines incremental learning tasks (task where not all data is available at once and waiting for all is impossible or impractical) and incremental algorithms (algorithms that update their hypotheses while examples arrive). Argues that incrementality should be used only for incremental tasks.},
}

























@inproceedings{wang_mining_2003,
	location = {Washington, D.C.},
	title = {Mining concept-drifting data streams using ensemble classifiers},
	isbn = {978-1-58113-737-8},
	url = {http://portal.acm.org/citation.cfm?doid=956750.956778},
	doi = {10.1145/956750.956778},
	pages = {226},
	booktitle = {Proceedings of the ninth {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} '03},
	publisher = {{ACM} Press},
	author = {Wang, Haixun and Fan, Wei and Yu, Philip S. and Han, Jiawei},
	urldate = {2021-09-17},
	date = {2003},
	langid = {english},
}

@inproceedings{kolter_dynamic_2007,
	title = {Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts},
	url = {https://www.jmlr.org/papers/volume8/kolter07a/kolter07a.pdf},
	booktitle = {Journal of Machine Learning Research},
	author = {Kolter, J and Maloof, Marcus A},
	urldate = {2021-09-16},
	date = {2007},
}

@article{minku_impact_2010,
	title = {The Impact of Diversity on Online Ensemble Learning in the Presence of Concept Drift},
	volume = {22},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5156502/},
	doi = {10.1109/TKDE.2009.156},
	pages = {730--742},
	number = {5},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Minku, L.L. and White, A.P. and {Xin Yao}},
	urldate = {2021-09-17},
	date = {2010-05},
}

@article{minku_ddd_2012,
	title = {{DDD}: A New Ensemble Approach for Dealing with Concept Drift},
	volume = {24},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5719616/},
	doi = {10.1109/TKDE.2011.58},
	shorttitle = {{DDD}},
	pages = {619--633},
	number = {4},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Minku, Leandro L. and Yao, Xin},
	urldate = {2021-09-17},
	date = {2012-04},
}

@inproceedings{oza_experimental_2001,
	location = {San Francisco, California},
	title = {Experimental comparisons of online and batch versions of bagging and boosting},
	isbn = {978-1-58113-391-2},
	url = {http://portal.acm.org/citation.cfm?doid=502512.502565},
	doi = {10.1145/502512.502565},
	pages = {359--364},
	booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} '01},
	publisher = {{ACM} Press},
	author = {Oza, Nikunj C. and Russell, Stuart},
	urldate = {2021-09-17},
	date = {2001},
	langid = {english},
}

@inproceedings{kolter_using_2005,
	location = {Bonn, Germany},
	title = {Using additive expert ensembles to cope with concept drift},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102408},
	doi = {10.1145/1102351.1102408},
	pages = {449--456},
	booktitle = {Proceedings of the 22nd international conference on Machine learning - {ICML} '05},
	publisher = {{ACM} Press},
	author = {Kolter, Jeremy Z. and Maloof, Marcus A.},
	urldate = {2021-09-17},
	date = {2005},
	langid = {english},
}

@incollection{chu_fast_2004,
	location = {Berlin, Heidelberg},
	title = {Fast and Light Boosting for Adaptive Mining of Data Streams},
	volume = {3056},
	isbn = {978-3-540-22064-0 978-3-540-24775-3},
	url = {http://link.springer.com/10.1007/978-3-540-24775-3_36},
	pages = {282--292},
	booktitle = {Advances in Knowledge Discovery and Data Mining},
	publisher = {Springer Berlin Heidelberg},
	author = {Chu, Fang and Zaniolo, Carlo},
	editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Dai, Honghua and Srikant, Ramakrishnan and Zhang, Chengqi},
	urldate = {2021-09-17},
	date = {2004},
	doi = {10.1007/978-3-540-24775-3_36},
}

@inproceedings{bach_paired_2008,
	location = {Pisa, Italy},
	title = {Paired Learners for Concept Drift},
	isbn = {978-0-7695-3502-9},
	url = {http://ieeexplore.ieee.org/document/4781097/},
	doi = {10.1109/ICDM.2008.119},
	pages = {23--32},
	booktitle = {2008 Eighth {IEEE} International Conference on Data Mining},
	publisher = {{IEEE}},
	author = {Bach, Stephen H. and Maloof, Marcus A.},
	urldate = {2021-09-17},
	date = {2008-12},
}

@inproceedings{gomes_learning_2011,
	location = {{TaiChung}, Taiwan},
	title = {Learning recurring concepts from data streams with a context-aware ensemble},
	isbn = {978-1-4503-0113-8},
	url = {http://portal.acm.org/citation.cfm?doid=1982185.1982403},
	doi = {10.1145/1982185.1982403},
	pages = {994},
	booktitle = {Proceedings of the 2011 {ACM} Symposium on Applied Computing - {SAC} '11},
	publisher = {{ACM} Press},
	author = {Gomes, João Bártolo and Menasalvas, Ernestina and Sousa, Pedro A. C.},
	urldate = {2021-09-17},
	date = {2011},
	langid = {english},
}

@article{schlimmer_incremental_1986,
	title = {Incremental learning from noisy data},
	volume = {1},
	issn = {0885-6125, 1573-0565},
	doi = {10.1007/BF00116895},
	pages = {317--354},
	number = {3},
	journaltitle = {Machine Learning},
	author = {Schlimmer, Jeffrey C. and Granger, Richard H.},
	date = {1986-09},
	langid = {english},
	annotation = {707 cites. Schlimmer \& Granger introducing {STAGGER}, the 1st existing solution (according to {DWM} paper) to address concept drift. {STAGGER} deals with pre-labeled binary classification only, is probably not useful as is, but as a comparison point that other, more modern technologies use. This is a single-model incremental solution, there are 2 processes updating a single model},
}

@inproceedings{hulten_mining_2001,
	location = {San Francisco, California},
	title = {Mining time-changing data streams},
	isbn = {978-1-58113-391-2},
	url = {http://portal.acm.org/citation.cfm?doid=502512.502529},
	doi = {10.1145/502512.502529},
	pages = {97--106},
	booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} '01},
	publisher = {{ACM} Press},
	author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
	urldate = {2021-09-17},
	date = {2001},
	langid = {english},
	annotation = {2088 cites. {CVFDT} algo for windowing-outperforming fast classification. A decision tree model, updated from the previous algorithm, {VFDT}. From 2001, outdated approach in terms of data volume.},
}

@article{klinkenberg_learning_2004,
	title = {Learning drifting concepts: Example selection vs. example weighting},
	volume = {8},
	issn = {15714128, 1088467X},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IDA-2004-8305},
	doi = {10.3233/IDA-2004-8305},
	shorttitle = {Learning drifting concepts},
	pages = {281--300},
	number = {3},
	journaltitle = {Intelligent Data Analysis},
	author = {Klinkenberg, Ralf},
	urldate = {2021-09-17},
	date = {2004-08},
	annotation = {584 cites. Presents a support vector machine approach for adapting to drift by adjusting training window size, and selecting and weighting examples. {SVM} is for classification only so it is unsure if this has any applicability for our case.},
}

@article{maloof_selecting_2000,
	title = {Selecting Examples for Partial Memory Learning},
	volume = {41},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1007661119649},
	doi = {10.1023/A:1007661119649},
	pages = {27--52},
	number = {1},
	journaltitle = {Machine Learning},
	author = {Maloof, Marcus A. and Michalski, Ryszard S.},
	urldate = {2021-09-17},
	date = {2000},
	annotation = {237 cites, maloof\&michalski 2000 (old). Presents how to choose examples for a partial memory learner (uses a combination of old and new data examples to adjust to concepts) by choosing examples close to concept boundaries and forgetting old examples. Compares to {IB}2 and {FLORAs} using {STAGGER} concepts.},
}

@incollection{widmer_effective_1993,
	location = {Berlin, Heidelberg},
	title = {Effective learning in dynamic environments by explicit context tracking},
	volume = {667},
	isbn = {978-3-540-56602-1 978-3-540-47597-2},
	url = {http://link.springer.com/10.1007/3-540-56602-3_139},
	pages = {227--243},
	booktitle = {Machine Learning: {ECML}-93},
	publisher = {Springer Berlin Heidelberg},
	author = {Widmer, Gerhard and Kubat, Miroslav},
	editor = {Siekmann, J. and Goos, G. and Hartmanis, J. and Brazdil, Pavel B.},
	urldate = {2021-09-17},
	date = {1993},
	doi = {10.1007/3-540-56602-3_139},
	annotation = {234 cites, '05. Presents {FLORA}3, an improvement to {FLORA}2 that has as additions the ability to adjust window size and store old concepts ({FLORAs} deal in general mainly with windowing for classificaiton). Also context information is utilized. This gains better performance in dealing with abrupt drifts.},
}

@inproceedings{koychev_gradual_2000,
	title = {Gradual Forgetting for Adaptation to Concept Drift},
	url = {http://hdl.handle.net/10506/57},
	booktitle = {Proceedings of {ECAI} 2000 Workshop on Current Issues in Spatio-Temporal Reasoning},
	author = {Koychev, Ivan},
	urldate = {2021-09-16},
	date = {2000},
}

@article{maloof_incremental_2004,
	title = {Incremental learning with partial instance memory},
	volume = {154},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370203001498},
	doi = {10.1016/j.artint.2003.04.001},
	pages = {95--126},
	number = {1},
	journaltitle = {Artificial Intelligence},
	author = {Maloof, Marcus A. and Michalski, Ryszard S.},
	urldate = {2021-09-17},
	date = {2004-04},
	langid = {english},
	annotation = {168 cites. "Sequel" to Selecting Examples for Partial Memory Learning, also by Maloof \& Mihalski. In the previous one they used batch learning algorithms, now they extend to incremental algorithms. Conducts empirical comparison with stagger concepts to {AQ}11 and {GEM}, as previously partial memory reduces accuracy a bit but saves a lot of memory. Additionally, this paper has a good summary and bibliography of memory schemes of adaptive learners.},
}

@inproceedings{maloof_method_1995,
	location = {Herndon, {VA}, {USA}},
	title = {A method for partial-memory incremental learning and its application to computer intrusion detection},
	isbn = {978-0-8186-7312-2},
	url = {http://ieeexplore.ieee.org/document/479784/},
	doi = {10.1109/TAI.1995.479784},
	pages = {392--397},
	booktitle = {Proceedings of 7th {IEEE} International Conference on Tools with Artificial Intelligence},
	publisher = {{IEEE} Comput. Soc. Press},
	author = {Maloof, M.A. and Michalski, R.S.},
	urldate = {2021-09-17},
	date = {1995},
	annotation = {26 cites. Maloof \& michalski partial memory applied to intrusion detection. Again, as result, compared to batch learners there is better learning speed and memory efficiency at the expense of prediction accuracy.},
}

@article{he_automl_2021,
	title = {{AutoML}: A survey of the state-of-the-art},
	volume = {212},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705120307516},
	doi = {10.1016/j.knosys.2020.106622},
	shorttitle = {{AutoML}},
	pages = {106622},
	journaltitle = {Knowledge-Based Systems},
	author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
	urldate = {2021-09-26},
	date = {2021-01},
	langid = {english},
}

@inproceedings{you_large-batch_2019,
	location = {Denver Colorado},
	title = {Large-batch training for {LSTM} and beyond},
	isbn = {978-1-4503-6229-0},
	url = {https://dl.acm.org/doi/10.1145/3295500.3356137},
	doi = {10.1145/3295500.3356137},
	eventtitle = {{SC} '19: The International Conference for High Performance Computing, Networking, Storage, and Analysis},
	pages = {1--16},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	publisher = {{ACM}},
	author = {You, Yang and Hseu, Jonathan and Ying, Chris and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
	urldate = {2021-10-05},
	date = {2019-11-17},
	langid = {english},
	annotation = {60 cites. Presents the {DATE} scaling approach that enables sqrt scaling (better approach for acieving large batches, which significantly speeds up training) for especially {LSTM}, but also {RNNs} and {CNNs} (recurrent neural network, convolutional neural network). Mitigates the need for hyperparameter tuning.},
	file = {Submitted Version:/home/riikoro/Zotero/storage/HGL2ATJ8/You et al. - 2019 - Large-batch training for LSTM and beyond.pdf:application/pdf},
}

@inproceedings{mai_kungfu_2020,
	title = {{KungFu}: Making Training in Distributed Machine Learning Adaptive},
	isbn = {978-1-939133-19-9},
	url = {https://www.usenix.org/conference/osdi20/presentation/mai},
	pages = {937--954},
	booktitle = {14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)},
	publisher = {{USENIX} Association},
	author = {Mai, Luo and Li, Guo and Wagenländer, Marcel and Fertakis, Konstantinos and Brabete, Andrei-Octavian and Pietzuch, Peter},
	date = {2020-11},
	annotation = {11 cites. Proposes a library that allows setting adaptation policies that are then automatically used to adapt hyperparameters and systems parameters during training to optimize e.g. batch size, communication and resource usage. Also an automated training monitoring system is included. so: federated learning, partly {autoML}},
}

@article{hoi_online_2018,
	title = {Online Learning: A Comprehensive Survey},
	url = {http://arxiv.org/abs/1802.02871},
	shorttitle = {Online Learning},
	abstract = {Online learning represents an important family of machine learning algorithms, in which a learner attempts to resolve an online prediction (or any type of decision-making) task by learning a model/hypothesis from a sequence of data instances one at a time. The goal of online learning is to ensure that the online learner would make a sequence of accurate predictions (or correct decisions) given the knowledge of correct answers to previous prediction or learning tasks and possibly additional information. This is in contrast to many traditional batch learning or offline machine learning algorithms that are often designed to train a model in batch from a given collection of training data instances. This survey aims to provide a comprehensive survey of the online machine learning literatures through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the learning type and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) supervised online learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) unsupervised online learning where there is no feedback available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.},
	journaltitle = {{arXiv}:1802.02871 [cs]},
	author = {Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
	urldate = {2021-10-05},
	date = {2018-10-22},
	eprinttype = {arxiv},
	eprint = {1802.02871},
	keywords = {Computer Science - Machine Learning},
	annotation = {Comment: 100 pages, {\textasciitilde}400 references
149 cites. Survey on online learning (really incremental learning in my definitions). Could be read for better understanding the field, less important than training speedup survey but better than lifelong learning survey. Chapters 7-10 especially: term definitions \& semi/unsupervised online learning for insight on feedback, drift detection, and delayed labels},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/URNXRIXE/Hoi et al. - 2018 - Online Learning A Comprehensive Survey.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/54QFL6W8/1802.html:text/html},
}

@article{you_large_2017,
	title = {Large Batch Training of Convolutional Networks},
	url = {http://arxiv.org/abs/1708.03888},
	abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent ({SGD}) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling ({LARS}). Using {LARS}, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
	journaltitle = {{arXiv}:1708.03888 [cs]},
	author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	urldate = {2021-10-05},
	date = {2017-09-13},
	eprinttype = {arxiv},
	eprint = {1708.03888},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annotation = {238 cites. Another proposal on how to achieve large batches without accuracy loss ({LARS}). As background, other solutions to large batch are given: the badly working baseline of linear scaling, square root scaling and linear scaling with warm up. Proves that accuracy gap was decreased from 14\% to 2.2\%, batch size 32K was achieved.},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/6XRZSNFS/You et al. - 2017 - Large Batch Training of Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/32JRGHMM/1708.html:text/html},
}

@inproceedings{hazelwood_applied_2018,
	location = {Vienna},
	title = {Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective},
	isbn = {978-1-5386-3659-6},
	url = {http://ieeexplore.ieee.org/document/8327042/},
	doi = {10.1109/HPCA.2018.00059},
	shorttitle = {Applied Machine Learning at Facebook},
	eventtitle = {2018 {IEEE} International Symposium on High Performance Computer Architecture ({HPCA})},
	pages = {620--629},
	booktitle = {2018 {IEEE} International Symposium on High Performance Computer Architecture ({HPCA})},
	publisher = {{IEEE}},
	author = {Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong},
	urldate = {2021-10-05},
	date = {2018-02},
	annotation = {371 cites. Describes facebooks' ml services and data center infrastructure. May be usable as further input to 2.1, otherwise a background read without much on-topic content.},
}

@article{you_limit_2020,
	title = {The Limit of the Batch Size},
	url = {http://arxiv.org/abs/2006.08517},
	abstract = {Large-batch training is an efficient approach for current distributed deep learning systems. It has enabled researchers to reduce the {ImageNet}/{ResNet}-50 training from 29 hours to around 1 minute. In this paper, we focus on studying the limit of the batch size. We think it may provide a guidance to {AI} supercomputer and algorithm designers. We provide detailed numerical optimization instructions for step-by-step comparison. Moreover, it is important to understand the generalization and optimization performance of huge batch training. Hoffer et al. introduced "ultra-slow diffusion" theory to large-batch training. However, our experiments show contradictory results with the conclusion of Hoffer et al. We provide comprehensive experimental results and detailed analysis to study the limitations of batch size scaling and "ultra-slow diffusion" theory. For the first time we scale the batch size on {ImageNet} to at least a magnitude larger than all previous work, and provide detailed studies on the performance of many state-of-the-art optimization schemes under this setting. We propose an optimization recipe that is able to improve the top-1 test accuracy by 18\% compared to the baseline.},
	journaltitle = {{arXiv}:2006.08517 [cs, stat]},
	author = {You, Yang and Wang, Yuhui and Zhang, Huan and Zhang, Zhao and Demmel, James and Hsieh, Cho-Jui},
	urldate = {2021-10-05},
	date = {2020-06-15},
	eprinttype = {arxiv},
	eprint = {2006.08517},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning},
	annotation = {2 cites, from 2020. Explores the limits of large-batch approach in batch size. Using 4 levels of optimization (including {LARS} as the last one), they find a spot where previous ultra slow diffusion theory (from ref 13, not explained here) no longer holds and the generalization problems (unknown meaning) becomes so big that they are no longer fixable by training for a longer time. Largest batch tested was 1.28M},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/SDMNT6CR/You et al. - 2020 - The Limit of the Batch Size.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/XDFC6SAN/2006.html:text/html},
}

@book{bifet_machine_2017,
	location = {Cambridge, Massachusetts},
	title = {Machine learning for data streams: with practical examples in {MOA}},
	isbn = {978-0-262-03779-2},
	series = {Adaptive computation and machine learning series},
	shorttitle = {Machine learning for data streams},
	pagetotal = {262},
	publisher = {{MIT} Press},
	author = {Bifet, Albert},
	date = {2017},
	keywords = {Data mining, Streaming technology (Telecommunications)},
}

@article{ben-nun_demystifying_2019,
	title = {Demystifying Parallel and Distributed Deep Learning: An In-depth Concurrency Analysis},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3320060},
	doi = {10.1145/3320060},
	shorttitle = {Demystifying Parallel and Distributed Deep Learning},
	abstract = {Deep Neural Networks ({DNNs}) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in {DNN} architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in {DNNs}: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
	pages = {1--43},
	number = {4},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	urldate = {2021-10-06},
	date = {2019-09-18},
	langid = {english},
	annotation = {388 cites. Surveys techniques of distributing deep learning. Chapters 5, 6 and 7 cover these, 5 being too math-y to understand. Covers one answer to the question "How to efficiently retrain" which is through distributed training.},
}

@article{celik_adaptation_2021,
	title = {Adaptation Strategies for Automated Machine Learning on Evolving Data},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/2006.06480},
	doi = {10.1109/TPAMI.2021.3062900},
	abstract = {Automated Machine Learning ({AutoML}) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of data stream challenges such as concept drift on the performance of {AutoML} methods, and which adaptation strategies can be employed to make them more robust. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on different {AutoML} approaches. We do this for a variety of {AutoML} approaches for building machine learning pipelines, including those that leverage Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust {AutoML} techniques.},
	pages = {3067--3078},
	number = {9},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Celik, Bilge and Vanschoren, Joaquin},
	urldate = {2021-10-06},
	date = {2021-09-01},
	eprinttype = {arxiv},
	eprint = {2006.06480},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annotation = {9 cites (jan '21). Presents and empirically benchmarks 6 {AutoML} approaches for adapting to concept drift. Seemingly the adaptation includes choosing an algorithm and its hyperparameters for classification. Contains also good brief charting of related work.},
	annotation = {Comment: 12 pages, 7 figures (14 counting subfigures), submitted to {TPAMI} - {AutoML} Special Issue},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/R7QCGRW6/Celik and Vanschoren - 2021 - Adaptation Strategies for Automated Machine Learni.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/PPK8GY6S/2006.html:text/html},
}

@article{bakirov_automated_2021,
	title = {Automated Adaptation Strategies for Stream Learning},
	url = {http://arxiv.org/abs/1812.10793},
	abstract = {Automation of machine learning model development is increasingly becoming an established research area. While automated model selection and automated data pre-processing have been studied in depth, there is, however, a gap concerning automated model adaptation strategies when multiple strategies are available. Manually developing an adaptation strategy can be time consuming and costly. In this paper we address this issue by proposing the use of flexible adaptive mechanism deployment for automated development of adaptation strategies. Experimental results after using the proposed strategies with five adaptive algorithms on 36 datasets confirm their viability. These strategies achieve better or comparable performance to the custom adaptation strategies and the repeated deployment of any single adaptive mechanism.},
	journaltitle = {{arXiv}:1812.10793 [cs, stat]},
	author = {Bakirov, Rashid and Gabrys, Bogdan and Fay, Damien},
	urldate = {2021-10-12},
	date = {2021-04-30},
	eprinttype = {arxiv},
	eprint = {1812.10793},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/87XLKU2W/Bakirov et al. - 2021 - Automated Adaptation Strategies for Stream Learnin.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/SBPVZEQ8/1812.html:text/html},
}

@article{madrid_towards_2019,
	title = {Towards {AutoML} in the presence of Drift: first results},
	url = {http://arxiv.org/abs/1907.10772},
	shorttitle = {Towards {AutoML} in the presence of Drift},
	abstract = {Research progress in {AutoML} has lead to state of the art solutions that can cope quite wellwith supervised learning task, e.g., classification with {AutoSklearn}. However, so far thesesystems do not take into account the changing nature of evolving data over time (i.e., theystill assume i.i.d. data); even when this sort of domains are increasingly available in realapplications (e.g., spam filtering, user preferences, etc.). We describe a first attempt to de-velop an {AutoML} solution for scenarios in which data distribution changes relatively slowlyover time and in which the problem is approached in a lifelong learning setting. We {extendAuto}-Sklearn with sound and intuitive mechanisms that allow it to cope with this sort ofproblems. The extended Auto-Sklearn is combined with concept drift detection techniquesthat allow it to automatically determine when the initial models have to be adapted. Wereport experimental results in benchmark data from {AutoML} competitions that adhere tothis scenario. Results demonstrate the effectiveness of the proposed methodology.},
	journaltitle = {{arXiv}:1907.10772 [cs, stat]},
	author = {Madrid, Jorge G. and Escalante, Hugo Jair and Morales, Eduardo F. and Tu, Wei-Wei and Yu, Yang and Sun-Hosoya, Lisheng and Guyon, Isabelle and Sebag, Michele},
	urldate = {2021-10-12},
	date = {2019-07-24},
	eprinttype = {arxiv},
	eprint = {1907.10772},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annotation = {Comment: {AutoML} 2018 @ {ICML}/{IJCAI}-{ECAI}},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/R5LEKEBY/Madrid et al. - 2019 - Towards AutoML in the presence of Drift first res.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/6W9Y6V73/1907.html:text/html},
}


@inproceedings{gama_learning_2004,
	location = {Berlin, Heidelberg},
	title = {Learning with Drift Detection},
	isbn = {978-3-540-28645-5},
	abstract = {Most of the work in machine learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the distribution that generate the examples changes over time. We present a method for detection of changes in the probability distribution of examples. The idea behind the drift detection method is to control the online error-rate of the algorithm. The training examples are presented in sequence. When a new training example is available, it is classified using the actual model. Statistical theory guarantees that while the distribution is stationary, the error will decrease. When the distribution changes, the error will increase. The method controls the trace of the online error of the algorithm. For the actual context we define a warning level, and a drift level. A new context is declared, if in a sequence of examples, the error increases reaching the warning level at example kw, and the drift level at example kd. This is an indication of a change in the distribution of the examples. The algorithm learns a new model using only the examples since kw. The method was tested with a set of eight artificial datasets and a real world dataset. We used three learning algorithms: a perceptron, a neural network and a decision tree. The experimental results show a good performance detecting drift and with learning the new concept. We also observe that the method is independent of the learning algorithm.},
	pages = {286--295},
	booktitle = {Advances in Artificial Intelligence – {SBIA} 2004},
	publisher = {Springer Berlin Heidelberg},
	author = {Gama, João and Medas, Pedro and Castillo, Gladys and Rodrigues, Pedro},
	editor = {Bazzan, Ana L. C. and Labidi, Sofiane},
	date = {2004},
}

@article{veloso_hyperparameter_2021,
	title = {Hyperparameter self-tuning for data streams},
	volume = {76},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253521000841},
	doi = {10.1016/j.inffus.2021.04.011},
	pages = {75--86},
	journaltitle = {Information Fusion},
	shortjournal = {Information Fusion},
	author = {Veloso, Bruno and Gama, João and Malheiro, Benedita and Vinagre, João},
	urldate = {2021-10-12},
	date = {2021-12},
	langid = {english},
}

@article{webb_characterizing_2016,
	title = {Characterizing concept drift},
	volume = {30},
	issn = {1384-5810, 1573-756X},
	url = {http://link.springer.com/10.1007/s10618-015-0448-4},
	doi = {10.1007/s10618-015-0448-4},
	pages = {964--994},
	number = {4},
	journaltitle = {Data Mining and Knowledge Discovery},
	shortjournal = {Data Min Knowl Disc},
	author = {Webb, Geoffrey I. and Hyde, Roy and Cao, Hong and Nguyen, Hai Long and Petitjean, Francois},
	urldate = {2021-10-12},
	date = {2016-07},
	langid = {english},
	file = {Submitted Version:/home/riikoro/Zotero/storage/XMWI3BYM/Webb et al. - 2016 - Characterizing concept drift.pdf:application/pdf},
}

@thesis{zliobaite_adaptive_2010,
	location = {Vilnus},
	title = {Adaptive training set formation},
	url = {https://epublications.vu.lt/object/elaba:1932399/},
	institution = {Vilnius University},
	type = {phdthesis},
	author = {Žliobaitė, Indrė},
	urldate = {2021-10-12},
	date = {2010},
}

@thesis{faithfull_unsupervised_2018,
	title = {Unsupervised Change Detection in Multivariate Streaming Data},
	url = {http://rgdoi.net/10.13140/RG.2.2.25121.66409},
	type = {phdthesis},
	author = {Faithfull, Will},
	urldate = {2021-10-12},
	date = {2018},
	langid = {english},
	note = {Publisher: Unpublished},
}