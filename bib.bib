
@incollection{williamson_thought_nodate,
	title = {Thought experiments},
	booktitle = {Doing Philosophy},
	author = {Williamson, Timothy},
	annotation = {Description of the philosophical method of thought experiments. They are powerful for intuitively finding cases where a theory should hold but doesn't. They are not perfect in correctness, however.},
}

@article{zliobaite_learning_2010,
	title = {Learning under Concept Drift: an Overview},
	url = {http://arxiv.org/abs/1010.4784},
	shorttitle = {Learning under Concept Drift},
	abstract = {Concept drift refers to a non stationary learning problem over time. The training and the application data often mismatch in real life problems. In this report we present a context of concept drift problem 1. We focus on the issues relevant to adaptive training set formation. We present the framework and terminology, and formulate a global picture of concept drift learners design. We start with formalizing the framework for the concept drifting data in Section 1. In Section 2 we discuss the adaptivity mechanisms of the concept drift learners. In Section 3 we overview the principle mechanisms of concept drift learners. In this chapter we give a general picture of the available algorithms and categorize them based on their properties. Section 5 discusses the related research fields and Section 5 groups and presents major concept drift applications. This report is intended to give a bird's view of concept drift research field, provide a context of the research and position it within broad spectrum of research fields and applications.},
	journaltitle = {{arXiv}:1010.4784 [cs]},
	author = {Žliobaitė, Indrė},
	urldate = {2021-09-13},
	date = {2010-10},
	keywords = {Computer Science - Artificial Intelligence},
	annotation = {A formal definition of concept drift, accompanied by a taxonomy of mitigation techniques and discussion of these with a broad bibliography. Gives general statements on which strategies to go for under which types of drifting.},
	annotation = {{arXiv}: 1010.4784},
}

@inproceedings{wang_mining_2003,
	location = {Washington, D.C.},
	title = {Mining concept-drifting data streams using ensemble classifiers},
	isbn = {978-1-58113-737-8},
	url = {http://portal.acm.org/citation.cfm?doid=956750.956778},
	doi = {10.1145/956750.956778},
	pages = {226},
	booktitle = {Proceedings of the ninth {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} '03},
	publisher = {{ACM} Press},
	author = {Wang, Haixun and Fan, Wei and Yu, Philip S. and Han, Jiawei},
	urldate = {2021-09-17},
	date = {2003},
	langid = {english},
}

@inproceedings{kolter_dynamic_2007,
	title = {Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts},
	url = {https://www.jmlr.org/papers/volume8/kolter07a/kolter07a.pdf},
	booktitle = {Journal of Machine Learning Research},
	author = {Kolter, J and Maloof, Marcus A},
	urldate = {2021-09-16},
	date = {2007},
}

@article{minku_impact_2010,
	title = {The Impact of Diversity on Online Ensemble Learning in the Presence of Concept Drift},
	volume = {22},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5156502/},
	doi = {10.1109/TKDE.2009.156},
	pages = {730--742},
	number = {5},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Minku, L.L. and White, A.P. and {Xin Yao}},
	urldate = {2021-09-17},
	date = {2010-05},
}

@article{minku_ddd_2012,
	title = {{DDD}: A New Ensemble Approach for Dealing with Concept Drift},
	volume = {24},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5719616/},
	doi = {10.1109/TKDE.2011.58},
	shorttitle = {{DDD}},
	pages = {619--633},
	number = {4},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Minku, Leandro L. and Yao, Xin},
	urldate = {2021-09-17},
	date = {2012-04},
}

@inproceedings{oza_experimental_2001,
	location = {San Francisco, California},
	title = {Experimental comparisons of online and batch versions of bagging and boosting},
	isbn = {978-1-58113-391-2},
	url = {http://portal.acm.org/citation.cfm?doid=502512.502565},
	doi = {10.1145/502512.502565},
	pages = {359--364},
	booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} '01},
	publisher = {{ACM} Press},
	author = {Oza, Nikunj C. and Russell, Stuart},
	urldate = {2021-09-17},
	date = {2001},
	langid = {english},
}

@inproceedings{kolter_using_2005,
	location = {Bonn, Germany},
	title = {Using additive expert ensembles to cope with concept drift},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102408},
	doi = {10.1145/1102351.1102408},
	pages = {449--456},
	booktitle = {Proceedings of the 22nd international conference on Machine learning - {ICML} '05},
	publisher = {{ACM} Press},
	author = {Kolter, Jeremy Z. and Maloof, Marcus A.},
	urldate = {2021-09-17},
	date = {2005},
	langid = {english},
}

@incollection{chu_fast_2004,
	location = {Berlin, Heidelberg},
	title = {Fast and Light Boosting for Adaptive Mining of Data Streams},
	volume = {3056},
	isbn = {978-3-540-22064-0 978-3-540-24775-3},
	url = {http://link.springer.com/10.1007/978-3-540-24775-3_36},
	pages = {282--292},
	booktitle = {Advances in Knowledge Discovery and Data Mining},
	publisher = {Springer Berlin Heidelberg},
	author = {Chu, Fang and Zaniolo, Carlo},
	editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Dai, Honghua and Srikant, Ramakrishnan and Zhang, Chengqi},
	urldate = {2021-09-17},
	date = {2004},
	doi = {10.1007/978-3-540-24775-3_36},
}

@inproceedings{bach_paired_2008,
	location = {Pisa, Italy},
	title = {Paired Learners for Concept Drift},
	isbn = {978-0-7695-3502-9},
	url = {http://ieeexplore.ieee.org/document/4781097/},
	doi = {10.1109/ICDM.2008.119},
	pages = {23--32},
	booktitle = {2008 Eighth {IEEE} International Conference on Data Mining},
	publisher = {{IEEE}},
	author = {Bach, Stephen H. and Maloof, Marcus A.},
	urldate = {2021-09-17},
	date = {2008-12},
}

@inproceedings{gomes_learning_2011,
	location = {{TaiChung}, Taiwan},
	title = {Learning recurring concepts from data streams with a context-aware ensemble},
	isbn = {978-1-4503-0113-8},
	url = {http://portal.acm.org/citation.cfm?doid=1982185.1982403},
	doi = {10.1145/1982185.1982403},
	pages = {994},
	booktitle = {Proceedings of the 2011 {ACM} Symposium on Applied Computing - {SAC} '11},
	publisher = {{ACM} Press},
	author = {Gomes, João Bártolo and Menasalvas, Ernestina and Sousa, Pedro A. C.},
	urldate = {2021-09-17},
	date = {2011},
	langid = {english},
}

@article{schlimmer_incremental_1986,
	title = {Incremental learning from noisy data},
	volume = {1},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00116895},
	doi = {10.1007/BF00116895},
	pages = {317--354},
	number = {3},
	journaltitle = {Machine Learning},
	author = {Schlimmer, Jeffrey C. and Granger, Richard H.},
	urldate = {2021-09-17},
	date = {1986-09},
	langid = {english},
	annotation = {707 cites. Schlimmer \& Granger introducing {STAGGER}, the 1st existing solution (according to {DWM} paper) to address concept drift. {STAGGER} deals with pre-labeled binary classification only, is probably not useful as is, but as a comparison point that other, more modern technologies use. This is a single-model incremental solution, there are 2 processes updating a single model},
}

@inproceedings{hulten_mining_2001,
	location = {San Francisco, California},
	title = {Mining time-changing data streams},
	isbn = {978-1-58113-391-2},
	url = {http://portal.acm.org/citation.cfm?doid=502512.502529},
	doi = {10.1145/502512.502529},
	pages = {97--106},
	booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} '01},
	publisher = {{ACM} Press},
	author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
	urldate = {2021-09-17},
	date = {2001},
	langid = {english},
	annotation = {2088 cites. {CVFDT} algo for windowing-outperforming fast classification. A decision tree model, updated from the previous algorithm, {VFDT}. From 2001, outdated approach in terms of data volume.},
}

@article{klinkenberg_learning_2004,
	title = {Learning drifting concepts: Example selection vs. example weighting},
	volume = {8},
	issn = {15714128, 1088467X},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IDA-2004-8305},
	doi = {10.3233/IDA-2004-8305},
	shorttitle = {Learning drifting concepts},
	pages = {281--300},
	number = {3},
	journaltitle = {Intelligent Data Analysis},
	author = {Klinkenberg, Ralf},
	urldate = {2021-09-17},
	date = {2004-08},
	annotation = {584 cites. Presents a support vector machine approach for adapting to drift by adjusting training window size, and selecting and weighting examples. {SVM} is for classification only so it is unsure if this has any applicability for our case.},
}

@article{maloof_selecting_2000,
	title = {Selecting Examples for Partial Memory Learning},
	volume = {41},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1007661119649},
	doi = {10.1023/A:1007661119649},
	pages = {27--52},
	number = {1},
	journaltitle = {Machine Learning},
	author = {Maloof, Marcus A. and Michalski, Ryszard S.},
	urldate = {2021-09-17},
	date = {2000},
	annotation = {237 cites, maloof\&michalski 2000 (old). Presents how to choose examples for a partial memory learner (uses a combination of old and new data examples to adjust to concepts) by choosing examples close to concept boundaries and forgetting old examples. Compares to {IB}2 and {FLORAs} using {STAGGER} concepts.},
}

@incollection{widmer_effective_1993,
	location = {Berlin, Heidelberg},
	title = {Effective learning in dynamic environments by explicit context tracking},
	volume = {667},
	isbn = {978-3-540-56602-1 978-3-540-47597-2},
	url = {http://link.springer.com/10.1007/3-540-56602-3_139},
	pages = {227--243},
	booktitle = {Machine Learning: {ECML}-93},
	publisher = {Springer Berlin Heidelberg},
	author = {Widmer, Gerhard and Kubat, Miroslav},
	editor = {Siekmann, J. and Goos, G. and Hartmanis, J. and Brazdil, Pavel B.},
	urldate = {2021-09-17},
	date = {1993},
	doi = {10.1007/3-540-56602-3_139},
	annotation = {234 cites, '05. Presents {FLORA}3, an improvement to {FLORA}2 that has as additions the ability to adjust window size and store old concepts ({FLORAs} deal in general mainly with windowing for classificaiton). Also context information is utilized. This gains better performance in dealing with abrupt drifts.},
}

@article{giraud-carrier_note_2000,
	title = {Note on the utility of incremental learning},
	volume = {13},
	issn = {09217126},
	pages = {215--223},
	number = {4},
	author = {Giraud-Carrier, C},
	date = {2000},
	annotation = {cites 232. Defines incremental learning tasks (task where not all data is available at once and waiting for all is impossible or impractical) and incremental algorithms (algorithms that update their hypotheses while examples arrive). Argues that incrementality should be used only for incremental tasks.},
}

@inproceedings{koychev_gradual_2000,
	title = {Gradual Forgetting for Adaptation to Concept Drift},
	url = {http://hdl.handle.net/10506/57},
	booktitle = {Proceedings of {ECAI} 2000 Workshop on Current Issues in Spatio-Temporal Reasoning},
	author = {Koychev, Ivan},
	urldate = {2021-09-16},
	date = {2000},
}

@article{maloof_incremental_2004,
	title = {Incremental learning with partial instance memory},
	volume = {154},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370203001498},
	doi = {10.1016/j.artint.2003.04.001},
	pages = {95--126},
	number = {1},
	journaltitle = {Artificial Intelligence},
	author = {Maloof, Marcus A. and Michalski, Ryszard S.},
	urldate = {2021-09-17},
	date = {2004-04},
	langid = {english},
	annotation = {168 cites. "Sequel" to Selecting Examples for Partial Memory Learning, also by Maloof \& Mihalski. In the previous one they used batch learning algorithms, now they extend to incremental algorithms. Conducts empirical comparison with stagger concepts to {AQ}11 and {GEM}, as previously partial memory reduces accuracy a bit but saves a lot of memory. Additionally, this paper has a good summary and bibliography of memory schemes of adaptive learners.},
}

@inproceedings{maloof_method_1995,
	location = {Herndon, {VA}, {USA}},
	title = {A method for partial-memory incremental learning and its application to computer intrusion detection},
	isbn = {978-0-8186-7312-2},
	url = {http://ieeexplore.ieee.org/document/479784/},
	doi = {10.1109/TAI.1995.479784},
	pages = {392--397},
	booktitle = {Proceedings of 7th {IEEE} International Conference on Tools with Artificial Intelligence},
	publisher = {{IEEE} Comput. Soc. Press},
	author = {Maloof, M.A. and Michalski, R.S.},
	urldate = {2021-09-17},
	date = {1995},
	annotation = {26 cites. Maloof \& michalski partial memory applied to intrusion detection. Again, as result, compared to batch learners there is better learning speed and memory efficiency at the expense of prediction accuracy.},
}

@article{he_automl_2021,
	title = {{AutoML}: A survey of the state-of-the-art},
	volume = {212},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705120307516},
	doi = {10.1016/j.knosys.2020.106622},
	shorttitle = {{AutoML}},
	pages = {106622},
	journaltitle = {Knowledge-Based Systems},
	author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
	urldate = {2021-09-26},
	date = {2021-01},
	langid = {english},
}

@inproceedings{you_large-batch_2019,
	location = {Denver Colorado},
	title = {Large-batch training for {LSTM} and beyond},
	isbn = {978-1-4503-6229-0},
	url = {https://dl.acm.org/doi/10.1145/3295500.3356137},
	doi = {10.1145/3295500.3356137},
	eventtitle = {{SC} '19: The International Conference for High Performance Computing, Networking, Storage, and Analysis},
	pages = {1--16},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	publisher = {{ACM}},
	author = {You, Yang and Hseu, Jonathan and Ying, Chris and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
	urldate = {2021-10-05},
	date = {2019-11-17},
	langid = {english},
	annotation = {60 cites. Presents the {DATE} scaling approach that enables sqrt scaling (better approach for acieving large batches, which significantly speeds up training) for especially {LSTM}, but also {RNNs} and {CNNs} (recurrent neural network, convolutional neural network). Mitigates the need for hyperparameter tuning.},
	file = {Submitted Version:/home/riikoro/Zotero/storage/HGL2ATJ8/You et al. - 2019 - Large-batch training for LSTM and beyond.pdf:application/pdf},
}

@inproceedings{mai_kungfu_2020,
	title = {{KungFu}: Making Training in Distributed Machine Learning Adaptive},
	isbn = {978-1-939133-19-9},
	url = {https://www.usenix.org/conference/osdi20/presentation/mai},
	pages = {937--954},
	booktitle = {14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)},
	publisher = {{USENIX} Association},
	author = {Mai, Luo and Li, Guo and Wagenländer, Marcel and Fertakis, Konstantinos and Brabete, Andrei-Octavian and Pietzuch, Peter},
	date = {2020-11},
	annotation = {11 cites. Proposes a library that allows setting adaptation policies that are then automatically used to adapt hyperparameters and systems parameters during training to optimize e.g. batch size, communication and resource usage. Also an automated training monitoring system is included. so: federated learning, partly {autoML}},
}

@article{hoi_online_2018,
	title = {Online Learning: A Comprehensive Survey},
	url = {http://arxiv.org/abs/1802.02871},
	shorttitle = {Online Learning},
	abstract = {Online learning represents an important family of machine learning algorithms, in which a learner attempts to resolve an online prediction (or any type of decision-making) task by learning a model/hypothesis from a sequence of data instances one at a time. The goal of online learning is to ensure that the online learner would make a sequence of accurate predictions (or correct decisions) given the knowledge of correct answers to previous prediction or learning tasks and possibly additional information. This is in contrast to many traditional batch learning or offline machine learning algorithms that are often designed to train a model in batch from a given collection of training data instances. This survey aims to provide a comprehensive survey of the online machine learning literatures through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the learning type and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) supervised online learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) unsupervised online learning where there is no feedback available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.},
	journaltitle = {{arXiv}:1802.02871 [cs]},
	author = {Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
	urldate = {2021-10-05},
	date = {2018-10-22},
	eprinttype = {arxiv},
	eprint = {1802.02871},
	keywords = {Computer Science - Machine Learning},
	annotation = {Comment: 100 pages, {\textasciitilde}400 references
149 cites. Survey on online learning (really incremental learning in my definitions). Could be read for better understanding the field, less important than training speedup survey but better than lifelong learning survey. Chapters 7-10 especially: term definitions \& semi/unsupervised online learning for insight on feedback, drift detection, and delayed labels},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/URNXRIXE/Hoi et al. - 2018 - Online Learning A Comprehensive Survey.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/54QFL6W8/1802.html:text/html},
}

@article{you_large_2017,
	title = {Large Batch Training of Convolutional Networks},
	url = {http://arxiv.org/abs/1708.03888},
	abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent ({SGD}) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling ({LARS}). Using {LARS}, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
	journaltitle = {{arXiv}:1708.03888 [cs]},
	author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	urldate = {2021-10-05},
	date = {2017-09-13},
	eprinttype = {arxiv},
	eprint = {1708.03888},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annotation = {238 cites. Another proposal on how to achieve large batches without accuracy loss ({LARS}). As background, other solutions to large batch are given: the badly working baseline of linear scaling, square root scaling and linear scaling with warm up. Proves that accuracy gap was decreased from 14\% to 2.2\%, batch size 32K was achieved.},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/6XRZSNFS/You et al. - 2017 - Large Batch Training of Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/32JRGHMM/1708.html:text/html},
}

@inproceedings{hazelwood_applied_2018,
	location = {Vienna},
	title = {Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective},
	isbn = {978-1-5386-3659-6},
	url = {http://ieeexplore.ieee.org/document/8327042/},
	doi = {10.1109/HPCA.2018.00059},
	shorttitle = {Applied Machine Learning at Facebook},
	eventtitle = {2018 {IEEE} International Symposium on High Performance Computer Architecture ({HPCA})},
	pages = {620--629},
	booktitle = {2018 {IEEE} International Symposium on High Performance Computer Architecture ({HPCA})},
	publisher = {{IEEE}},
	author = {Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong},
	urldate = {2021-10-05},
	date = {2018-02},
	annotation = {371 cites. Describes facebooks' ml services and data center infrastructure. May be usable as further input to 2.1, otherwise a background read without much on-topic content.},
}

@article{you_limit_2020,
	title = {The Limit of the Batch Size},
	url = {http://arxiv.org/abs/2006.08517},
	abstract = {Large-batch training is an efficient approach for current distributed deep learning systems. It has enabled researchers to reduce the {ImageNet}/{ResNet}-50 training from 29 hours to around 1 minute. In this paper, we focus on studying the limit of the batch size. We think it may provide a guidance to {AI} supercomputer and algorithm designers. We provide detailed numerical optimization instructions for step-by-step comparison. Moreover, it is important to understand the generalization and optimization performance of huge batch training. Hoffer et al. introduced "ultra-slow diffusion" theory to large-batch training. However, our experiments show contradictory results with the conclusion of Hoffer et al. We provide comprehensive experimental results and detailed analysis to study the limitations of batch size scaling and "ultra-slow diffusion" theory. For the first time we scale the batch size on {ImageNet} to at least a magnitude larger than all previous work, and provide detailed studies on the performance of many state-of-the-art optimization schemes under this setting. We propose an optimization recipe that is able to improve the top-1 test accuracy by 18\% compared to the baseline.},
	journaltitle = {{arXiv}:2006.08517 [cs, stat]},
	author = {You, Yang and Wang, Yuhui and Zhang, Huan and Zhang, Zhao and Demmel, James and Hsieh, Cho-Jui},
	urldate = {2021-10-05},
	date = {2020-06-15},
	eprinttype = {arxiv},
	eprint = {2006.08517},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning},
	annotation = {2 cites, from 2020. Explores the limits of large-batch approach in batch size. Using 4 levels of optimization (including {LARS} as the last one), they find a spot where previous ultra slow diffusion theory (from ref 13, not explained here) no longer holds and the generalization problems (unknown meaning) becomes so big that they are no longer fixable by training for a longer time. Largest batch tested was 1.28M},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/SDMNT6CR/You et al. - 2020 - The Limit of the Batch Size.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/XDFC6SAN/2006.html:text/html},
}

@book{bifet_machine_2017,
	location = {Cambridge, Massachusetts},
	title = {Machine learning for data streams: with practical examples in {MOA}},
	isbn = {978-0-262-03779-2},
	series = {Adaptive computation and machine learning series},
	shorttitle = {Machine learning for data streams},
	pagetotal = {262},
	publisher = {{MIT} Press},
	author = {Bifet, Albert},
	date = {2017},
	keywords = {Data mining, Streaming technology (Telecommunications)},
}

@article{ben-nun_demystifying_2019,
	title = {Demystifying Parallel and Distributed Deep Learning: An In-depth Concurrency Analysis},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3320060},
	doi = {10.1145/3320060},
	shorttitle = {Demystifying Parallel and Distributed Deep Learning},
	abstract = {Deep Neural Networks ({DNNs}) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in {DNN} architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in {DNNs}: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
	pages = {1--43},
	number = {4},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	urldate = {2021-10-06},
	date = {2019-09-18},
	langid = {english},
	annotation = {388 cites. Surveys techniques of distributing deep learning. Chapters 5, 6 and 7 cover these, 5 being too math-y to understand. Covers one answer to the question "How to efficiently retrain" which is through distributed training.},
}

@article{celik_adaptation_2021,
	title = {Adaptation Strategies for Automated Machine Learning on Evolving Data},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/2006.06480},
	doi = {10.1109/TPAMI.2021.3062900},
	abstract = {Automated Machine Learning ({AutoML}) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of data stream challenges such as concept drift on the performance of {AutoML} methods, and which adaptation strategies can be employed to make them more robust. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on different {AutoML} approaches. We do this for a variety of {AutoML} approaches for building machine learning pipelines, including those that leverage Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust {AutoML} techniques.},
	pages = {3067--3078},
	number = {9},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Celik, Bilge and Vanschoren, Joaquin},
	urldate = {2021-10-06},
	date = {2021-09-01},
	eprinttype = {arxiv},
	eprint = {2006.06480},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annotation = {9 cites (jan '21). Presents and empirically benchmarks 6 {AutoML} approaches for adapting to concept drift, including drift detection. Seemingly the adaptation includes choosing an algorithm and its hyperparameters for classification. Contains also good brief charting of related work.},
	annotation = {Comment: 12 pages, 7 figures (14 counting subfigures), submitted to {TPAMI} - {AutoML} Special Issue},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/R7QCGRW6/Celik and Vanschoren - 2021 - Adaptation Strategies for Automated Machine Learni.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/PPK8GY6S/2006.html:text/html},
}

@article{bakirov_automated_2021,
	title = {Automated Adaptation Strategies for Stream Learning},
	url = {http://arxiv.org/abs/1812.10793},
	abstract = {Automation of machine learning model development is increasingly becoming an established research area. While automated model selection and automated data pre-processing have been studied in depth, there is, however, a gap concerning automated model adaptation strategies when multiple strategies are available. Manually developing an adaptation strategy can be time consuming and costly. In this paper we address this issue by proposing the use of flexible adaptive mechanism deployment for automated development of adaptation strategies. Experimental results after using the proposed strategies with five adaptive algorithms on 36 datasets confirm their viability. These strategies achieve better or comparable performance to the custom adaptation strategies and the repeated deployment of any single adaptive mechanism.},
	journaltitle = {{arXiv}:1812.10793 [cs, stat]},
	author = {Bakirov, Rashid and Gabrys, Bogdan and Fay, Damien},
	urldate = {2021-10-12},
	date = {2021-04-30},
	eprinttype = {arxiv},
	eprint = {1812.10793},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/87XLKU2W/Bakirov et al. - 2021 - Automated Adaptation Strategies for Stream Learnin.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/SBPVZEQ8/1812.html:text/html},
}

@article{madrid_towards_2019,
	title = {Towards {AutoML} in the presence of Drift: first results},
	url = {http://arxiv.org/abs/1907.10772},
	shorttitle = {Towards {AutoML} in the presence of Drift},
	abstract = {Research progress in {AutoML} has lead to state of the art solutions that can cope quite wellwith supervised learning task, e.g., classification with {AutoSklearn}. However, so far thesesystems do not take into account the changing nature of evolving data over time (i.e., theystill assume i.i.d. data); even when this sort of domains are increasingly available in realapplications (e.g., spam filtering, user preferences, etc.). We describe a first attempt to de-velop an {AutoML} solution for scenarios in which data distribution changes relatively slowlyover time and in which the problem is approached in a lifelong learning setting. We {extendAuto}-Sklearn with sound and intuitive mechanisms that allow it to cope with this sort ofproblems. The extended Auto-Sklearn is combined with concept drift detection techniquesthat allow it to automatically determine when the initial models have to be adapted. Wereport experimental results in benchmark data from {AutoML} competitions that adhere tothis scenario. Results demonstrate the effectiveness of the proposed methodology.},
	journaltitle = {{arXiv}:1907.10772 [cs, stat]},
	author = {Madrid, Jorge G. and Escalante, Hugo Jair and Morales, Eduardo F. and Tu, Wei-Wei and Yu, Yang and Sun-Hosoya, Lisheng and Guyon, Isabelle and Sebag, Michele},
	urldate = {2021-10-12},
	date = {2019-07-24},
	eprinttype = {arxiv},
	eprint = {1907.10772},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annotation = {Comment: {AutoML} 2018 @ {ICML}/{IJCAI}-{ECAI}},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/R5LEKEBY/Madrid et al. - 2019 - Towards AutoML in the presence of Drift first res.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/6W9Y6V73/1907.html:text/html},
}

@article{madrid_towards_2019-1,
	title = {Towards {AutoML} in the presence of Drift: first results},
	url = {http://arxiv.org/abs/1907.10772},
	shorttitle = {Towards {AutoML} in the presence of Drift},
	abstract = {Research progress in {AutoML} has lead to state of the art solutions that can cope quite wellwith supervised learning task, e.g., classification with {AutoSklearn}. However, so far thesesystems do not take into account the changing nature of evolving data over time (i.e., theystill assume i.i.d. data); even when this sort of domains are increasingly available in realapplications (e.g., spam filtering, user preferences, etc.). We describe a first attempt to de-velop an {AutoML} solution for scenarios in which data distribution changes relatively slowlyover time and in which the problem is approached in a lifelong learning setting. We {extendAuto}-Sklearn with sound and intuitive mechanisms that allow it to cope with this sort ofproblems. The extended Auto-Sklearn is combined with concept drift detection techniquesthat allow it to automatically determine when the initial models have to be adapted. Wereport experimental results in benchmark data from {AutoML} competitions that adhere tothis scenario. Results demonstrate the effectiveness of the proposed methodology.},
	journaltitle = {{arXiv}:1907.10772 [cs, stat]},
	author = {Madrid, Jorge G. and Escalante, Hugo Jair and Morales, Eduardo F. and Tu, Wei-Wei and Yu, Yang and Sun-Hosoya, Lisheng and Guyon, Isabelle and Sebag, Michele},
	urldate = {2021-10-12},
	date = {2019-07-24},
	eprinttype = {arxiv},
	eprint = {1907.10772},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annotation = {Comment: {AutoML} 2018 @ {ICML}/{IJCAI}-{ECAI}},
	file = {arXiv Fulltext PDF:/home/riikoro/Zotero/storage/U8MNMRPB/Madrid et al. - 2019 - Towards AutoML in the presence of Drift first res.pdf:application/pdf;arXiv.org Snapshot:/home/riikoro/Zotero/storage/JG5BB3XS/1907.html:text/html},
}

@inproceedings{gama_learning_2004,
	location = {Berlin, Heidelberg},
	title = {Learning with Drift Detection},
	isbn = {978-3-540-28645-5},
	abstract = {Most of the work in machine learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the distribution that generate the examples changes over time. We present a method for detection of changes in the probability distribution of examples. The idea behind the drift detection method is to control the online error-rate of the algorithm. The training examples are presented in sequence. When a new training example is available, it is classified using the actual model. Statistical theory guarantees that while the distribution is stationary, the error will decrease. When the distribution changes, the error will increase. The method controls the trace of the online error of the algorithm. For the actual context we define a warning level, and a drift level. A new context is declared, if in a sequence of examples, the error increases reaching the warning level at example kw, and the drift level at example kd. This is an indication of a change in the distribution of the examples. The algorithm learns a new model using only the examples since kw. The method was tested with a set of eight artificial datasets and a real world dataset. We used three learning algorithms: a perceptron, a neural network and a decision tree. The experimental results show a good performance detecting drift and with learning the new concept. We also observe that the method is independent of the learning algorithm.},
	pages = {286--295},
	booktitle = {Advances in Artificial Intelligence – {SBIA} 2004},
	publisher = {Springer Berlin Heidelberg},
	author = {Gama, João and Medas, Pedro and Castillo, Gladys and Rodrigues, Pedro},
	editor = {Bazzan, Ana L. C. and Labidi, Sofiane},
	date = {2004},
}

@article{veloso_hyperparameter_2021,
	title = {Hyperparameter self-tuning for data streams},
	volume = {76},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253521000841},
	doi = {10.1016/j.inffus.2021.04.011},
	pages = {75--86},
	journaltitle = {Information Fusion},
	shortjournal = {Information Fusion},
	author = {Veloso, Bruno and Gama, João and Malheiro, Benedita and Vinagre, João},
	urldate = {2021-10-12},
	date = {2021-12},
	langid = {english},
}

@article{webb_characterizing_2016,
	title = {Characterizing concept drift},
	volume = {30},
	issn = {1384-5810, 1573-756X},
	url = {http://link.springer.com/10.1007/s10618-015-0448-4},
	doi = {10.1007/s10618-015-0448-4},
	pages = {964--994},
	number = {4},
	journaltitle = {Data Mining and Knowledge Discovery},
	shortjournal = {Data Min Knowl Disc},
	author = {Webb, Geoffrey I. and Hyde, Roy and Cao, Hong and Nguyen, Hai Long and Petitjean, Francois},
	urldate = {2021-10-12},
	date = {2016-07},
	langid = {english},
	file = {Submitted Version:/home/riikoro/Zotero/storage/XMWI3BYM/Webb et al. - 2016 - Characterizing concept drift.pdf:application/pdf},
}

@thesis{zliobaite_adaptive_2010,
	location = {Vilnus},
	title = {Adaptive training set formation},
	url = {https://epublications.vu.lt/object/elaba:1932399/},
	institution = {Vilnius University},
	type = {phdthesis},
	author = {Žliobaitė, Indrė},
	urldate = {2021-10-12},
	date = {2010},
}

@thesis{faithfull_unsupervised_2018,
	title = {Unsupervised Change Detection in Multivariate Streaming Data},
	url = {http://rgdoi.net/10.13140/RG.2.2.25121.66409},
	type = {phdthesis},
	author = {Faithfull, Will},
	urldate = {2021-10-12},
	date = {2018},
	langid = {english},
	note = {Publisher: Unpublished},
}