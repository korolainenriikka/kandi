
@incollection{leibe_xnor-net_2016,
	address = {Cham},
	title = {{XNOR}-{Net}: {ImageNet} {Classification} {Using} {Binary} {Convolutional} {Neural} {Networks}},
	volume = {9908},
	isbn = {978-3-319-46492-3 978-3-319-46493-0},
	shorttitle = {{XNOR}-{Net}},
	url = {http://link.springer.com/10.1007/978-3-319-46493-0_32},
	language = {en},
	urldate = {2021-10-28},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46493-0_32},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {525--542},
}

@article{li_ternary_2016,
	title = {Ternary {Weight} {Networks}},
	url = {http://arxiv.org/abs/1605.04711},
	abstract = {We introduce ternary weight networks (TWNs) - neural networks with weights constrained to +1, 0 and -1. The Euclidian distance between full (float or double) precision weights and the ternary weights along with a scaling factor is minimized. Besides, a threshold-based ternary function is optimized to get an approximated solution which can be fast and easily computed. TWNs have stronger expressive abilities than the recently proposed binary precision counterparts and are thus more effective than the latter. Meanwhile, TWNs achieve up to 16\${\textbackslash}times\$ or 32\${\textbackslash}times\$ model compression rate and need fewer multiplications compared with the full precision counterparts. Benchmarks on MNIST, CIFAR-10, and large scale ImageNet datasets show that the performance of TWNs is only slightly worse than the full precision counterparts but outperforms the analogous binary precision counterparts a lot.},
	urldate = {2021-10-28},
	journal = {arXiv:1605.04711 [cs]},
	author = {Li, Fengfu and Zhang, Bo and Liu, Bin},
	month = nov,
	year = {2016},
	note = {arXiv: 1605.04711},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{courbariaux_binaryconnect_2016,
	title = {{BinaryConnect}: {Training} {Deep} {Neural} {Networks} with binary weights during propagations},
	shorttitle = {{BinaryConnect}},
	url = {http://arxiv.org/abs/1511.00363},
	abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
	urldate = {2021-10-28},
	journal = {arXiv:1511.00363 [cs]},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	month = apr,
	year = {2016},
	note = {arXiv: 1511.00363},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{courbariaux_binarized_2016,
	title = {Binarized {Neural} {Networks}: {Training} {Deep} {Neural} {Networks} with {Weights} and {Activations} {Constrained} to +1 or -1},
	shorttitle = {Binarized {Neural} {Networks}},
	url = {http://arxiv.org/abs/1602.02830},
	abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
	urldate = {2021-10-27},
	journal = {arXiv:1602.02830 [cs]},
	author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	month = mar,
	year = {2016},
	note = {arXiv: 1602.02830},
	keywords = {Computer Science - Machine Learning},
}

@article{micikevicius_mixed_2018,
	title = {Mixed {Precision} {Training}},
	url = {http://arxiv.org/abs/1710.03740},
	abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
	urldate = {2021-10-27},
	journal = {arXiv:1710.03740 [cs, stat]},
	author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
	month = feb,
	year = {2018},
	note = {arXiv: 1710.03740},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{de_sa_high-accuracy_2018,
	title = {High-{Accuracy} {Low}-{Precision} {Training}},
	url = {http://arxiv.org/abs/1803.03383},
	abstract = {Low-precision computation is often used to lower the time and energy cost of machine learning, and recently hardware accelerators have been developed to support it. Still, it has been used primarily for inference - not training. Previous low-precision training algorithms suffered from a fundamental tradeoff: as the number of bits of precision is lowered, quantization noise is added to the model, which limits statistical accuracy. To address this issue, we describe a simple low-precision stochastic gradient descent variant called HALP. HALP converges at the same theoretical rate as full-precision algorithms despite the noise introduced by using low precision throughout execution. The key idea is to use SVRG to reduce gradient variance, and to combine this with a novel technique called bit centering to reduce quantization error. We show that on the CPU, HALP can run up to \$4 {\textbackslash}times\$ faster than full-precision SVRG and can match its convergence trajectory. We implemented HALP in TensorQuant, and show that it exceeds the validation performance of plain low-precision SGD on two deep learning tasks.},
	urldate = {2021-10-27},
	journal = {arXiv:1803.03383 [cs, stat]},
	author = {De Sa, Christopher and Leszczynski, Megan and Zhang, Jian and Marzoev, Alana and Aberger, Christopher R. and Olukotun, Kunle and Ré, Christopher},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.03383},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{courbariaux_training_2015,
	title = {Training deep neural networks with low precision multiplications},
	url = {http://arxiv.org/abs/1412.7024},
	abstract = {Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.},
	urldate = {2021-10-27},
	journal = {arXiv:1412.7024 [cs]},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	month = sep,
	year = {2015},
	note = {arXiv: 1412.7024},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{jia_highly_2018,
	title = {Highly {Scalable} {Deep} {Learning} {Training} {System} with {Mixed}-{Precision}: {Training} {ImageNet} in {Four} {Minutes}},
	shorttitle = {Highly {Scalable} {Deep} {Learning} {Training} {System} with {Mixed}-{Precision}},
	url = {http://arxiv.org/abs/1807.11205},
	abstract = {Synchronized stochastic gradient descent (SGD) optimizers with data parallelism are widely used in training large-scale deep neural networks. Although using larger mini-batch sizes can improve the system scalability by reducing the communication-to-computation ratio, it may hurt the generalization ability of the models. To this end, we build a highly scalable deep learning training system for dense GPU clusters with three main contributions: (1) We propose a mixed-precision training method that significantly improves the training throughput of a single GPU without losing accuracy. (2) We propose an optimization approach for extremely large mini-batch size (up to 64k) that can train CNN models on the ImageNet dataset without losing accuracy. (3) We propose highly optimized all-reduce algorithms that achieve up to 3x and 11x speedup on AlexNet and ResNet-50 respectively than NCCL-based training on a cluster with 1024 Tesla P40 GPUs. On training ResNet-50 with 90 epochs, the state-of-the-art GPU-based system with 1024 Tesla P100 GPUs spent 15 minutes and achieved 74.9{\textbackslash}\% top-1 test accuracy, and another KNL-based system with 2048 Intel KNLs spent 20 minutes and achieved 75.4{\textbackslash}\% accuracy. Our training system can achieve 75.8{\textbackslash}\% top-1 test accuracy in only 6.6 minutes using 2048 Tesla P40 GPUs. When training AlexNet with 95 epochs, our system can achieve 58.7{\textbackslash}\% top-1 test accuracy within 4 minutes, which also outperforms all other existing systems.},
	urldate = {2021-10-27},
	journal = {arXiv:1807.11205 [cs, stat]},
	author = {Jia, Xianyan and Song, Shutao and He, Wei and Wang, Yangzihao and Rong, Haidong and Zhou, Feihu and Xie, Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei and Chen, Tiegang and Hu, Guangxiao and Shi, Shaohuai and Chu, Xiaowen},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.11205},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hubara_quantized_2016,
	title = {Quantized {Neural} {Networks}: {Training} {Neural} {Networks} with {Low} {Precision} {Weights} and {Activations}},
	shorttitle = {Quantized {Neural} {Networks}},
	url = {http://arxiv.org/abs/1609.07061},
	abstract = {We introduce a method to train Quantized Neural Networks (QNNs) --- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At train-time the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves \$51{\textbackslash}\%\$ top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online.},
	urldate = {2021-10-27},
	journal = {arXiv:1609.07061 [cs]},
	author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.07061},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{pmlr-v37-gupta15,
	address = {Lille, France},
	series = {Proceedings of machine learning research},
	title = {Deep learning with limited numerical precision},
	volume = {37},
	url = {https://proceedings.mlr.press/v37/gupta15.html},
	abstract = {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network’s behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding},
	booktitle = {Proceedings of the 32nd international conference on machine learning},
	publisher = {PMLR},
	author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
	editor = {Bach, Francis and Blei, David},
	month = jul,
	year = {2015},
	note = {tex.pdf: http://proceedings.mlr.press/v37/gupta15.pdf},
	pages = {1737--1746},
}

@article{an_effects_1996,
	title = {The {Effects} of {Adding} {Noise} {During} {Backpropagation} {Training} on a {Generalization} {Performance}},
	volume = {8},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/8/3/643-674/5975},
	doi = {10.1162/neco.1996.8.3.643},
	abstract = {We study the effects of adding noise to the inputs, outputs, weight connections, and weight changes of multilayer feedforward neural networks during backpropagation training. We rigorously derive and analyze the objective functions that are minimized by the noise-affected training processes. We show that input noise and weight noise encourage the neural-network output to be a smooth function of the input or its weights, respectively. In the weak-noise limit, noise added to the output of the neural networks only changes the objective function by a constant. Hence, it cannot improve generalization. Input noise introduces penalty terms in the objective function that are related to, but distinct from, those found in the regularization approaches. Simulations have been performed on a regression and a classification problem to further substantiate our analysis. Input noise is found to be effective in improving the generalization performance for both problems. However, weight noise is found to be effective in improving the generalization performance only for the classification problem. Other forms of noise have practically no effect on generalization.},
	language = {en},
	number = {3},
	urldate = {2021-10-27},
	journal = {Neural Computation},
	author = {An, Guozhong},
	month = apr,
	year = {1996},
	pages = {643--674},
}

@article{murray_enhanced_1994,
	title = {Enhanced {MLP} performance and fault tolerance resulting from synaptic weight noise during training},
	volume = {5},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/317730/},
	doi = {10.1109/72.317730},
	number = {5},
	urldate = {2021-10-27},
	journal = {IEEE Transactions on Neural Networks},
	author = {Murray, A.F. and Edwards, P.J.},
	month = sep,
	year = {1994},
	pages = {792--802},
}

@inproceedings{DBLP:conf/icml/LeNCLPN11,
	title = {On optimization methods for deep learning},
	url = {https://icml.cc/2011/papers/210_icmlpaper.pdf},
	booktitle = {{ICML}},
	author = {Le, Quoc V. and Ngiam, Jiquan and Coates, Adam and Lahiri, Ahbik and Prochnow, Bobby and Ng, Andrew Y.},
	year = {2011},
	note = {tex.cdate: 1293840000000
tex.crossref: conf/icml/2011},
	pages = {265--272},
}

@article{smith_dont_2018,
	title = {Don't {Decay} the {Learning} {Rate}, {Increase} the {Batch} {Size}},
	url = {http://arxiv.org/abs/1711.00489},
	abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate \${\textbackslash}epsilon\$ and scaling the batch size \$B {\textbackslash}propto {\textbackslash}epsilon\$. Finally, one can increase the momentum coefficient \$m\$ and scale \$B {\textbackslash}propto 1/(1-m)\$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to \$76.1{\textbackslash}\%\$ validation accuracy in under 30 minutes.},
	urldate = {2021-10-27},
	journal = {arXiv:1711.00489 [cs, stat]},
	author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
	month = feb,
	year = {2018},
	note = {arXiv: 1711.00489},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{osawa_large-scale_2019,
	title = {Large-{Scale} {Distributed} {Second}-{Order} {Optimization} {Using} {Kronecker}-{Factored} {Approximate} {Curvature} for {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1811.12019},
	abstract = {Large-scale distributed training of deep neural networks suffer from the generalization gap caused by the increase in the effective mini-batch size. Previous approaches try to solve this problem by varying the learning rate and batch size over epochs and layers, or some ad hoc modification of the batch normalization. We propose an alternative approach using a second-order optimization method that shows similar generalization capability to first-order methods, but converges faster and can handle larger mini-batches. To test our method on a benchmark where highly optimized first-order methods are available as references, we train ResNet-50 on ImageNet. We converged to 75\% Top-1 validation accuracy in 35 epochs for mini-batch sizes under 16,384, and achieved 75\% even with a mini-batch size of 131,072, which took only 978 iterations.},
	urldate = {2021-10-27},
	journal = {arXiv:1811.12019 [cs, stat]},
	author = {Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
	month = mar,
	year = {2019},
	note = {arXiv: 1811.12019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mikami_massively_2019,
	title = {Massively {Distributed} {SGD}: {ImageNet}/{ResNet}-50 {Training} in a {Flash}},
	shorttitle = {Massively {Distributed} {SGD}},
	url = {http://arxiv.org/abs/1811.05233},
	abstract = {Scaling the distributed deep learning to a massive GPU cluster level is challenging due to the instability of the large mini-batch training and the overhead of the gradient synchronization. We address the instability of the large mini-batch training with batch-size control and label smoothing. We address the overhead of the gradient synchronization with 2D-Torus all-reduce. Specifically, 2D-Torus all-reduce arranges GPUs in a logical 2D grid and performs a series of collective operation in different orientations. These two techniques are implemented with Neural Network Libraries (NNL). We have successfully trained ImageNet/ResNet-50 in 122 seconds without significant accuracy loss on ABCI cluster.},
	urldate = {2021-10-27},
	journal = {arXiv:1811.05233 [cs]},
	author = {Mikami, Hiroaki and Suganuma, Hisahiro and U-chupala, Pongsakorn and Tanaka, Yoshiki and Kageyama, Yuichi},
	month = mar,
	year = {2019},
	note = {arXiv: 1811.05233},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{jia_highly_2018-1,
	title = {Highly {Scalable} {Deep} {Learning} {Training} {System} with {Mixed}-{Precision}: {Training} {ImageNet} in {Four} {Minutes}},
	shorttitle = {Highly {Scalable} {Deep} {Learning} {Training} {System} with {Mixed}-{Precision}},
	url = {http://arxiv.org/abs/1807.11205},
	abstract = {Synchronized stochastic gradient descent (SGD) optimizers with data parallelism are widely used in training large-scale deep neural networks. Although using larger mini-batch sizes can improve the system scalability by reducing the communication-to-computation ratio, it may hurt the generalization ability of the models. To this end, we build a highly scalable deep learning training system for dense GPU clusters with three main contributions: (1) We propose a mixed-precision training method that significantly improves the training throughput of a single GPU without losing accuracy. (2) We propose an optimization approach for extremely large mini-batch size (up to 64k) that can train CNN models on the ImageNet dataset without losing accuracy. (3) We propose highly optimized all-reduce algorithms that achieve up to 3x and 11x speedup on AlexNet and ResNet-50 respectively than NCCL-based training on a cluster with 1024 Tesla P40 GPUs. On training ResNet-50 with 90 epochs, the state-of-the-art GPU-based system with 1024 Tesla P100 GPUs spent 15 minutes and achieved 74.9{\textbackslash}\% top-1 test accuracy, and another KNL-based system with 2048 Intel KNLs spent 20 minutes and achieved 75.4{\textbackslash}\% accuracy. Our training system can achieve 75.8{\textbackslash}\% top-1 test accuracy in only 6.6 minutes using 2048 Tesla P40 GPUs. When training AlexNet with 95 epochs, our system can achieve 58.7{\textbackslash}\% top-1 test accuracy within 4 minutes, which also outperforms all other existing systems.},
	urldate = {2021-10-27},
	journal = {arXiv:1807.11205 [cs, stat]},
	author = {Jia, Xianyan and Song, Shutao and He, Wei and Wang, Yangzihao and Rong, Haidong and Zhou, Feihu and Xie, Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei and Chen, Tiegang and Hu, Guangxiao and Shi, Shaohuai and Chu, Xiaowen},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.11205},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{friedlander_hybrid_2012,
	title = {Hybrid {Deterministic}-{Stochastic} {Methods} for {Data} {Fitting}},
	volume = {34},
	issn = {1064-8275, 1095-7197},
	url = {http://arxiv.org/abs/1104.2373},
	doi = {10.1137/110830629},
	abstract = {Many structured data-fitting applications require the solution of an optimization problem involving a sum over a potentially large number of measurements. Incremental gradient algorithms offer inexpensive iterations by sampling a subset of the terms in the sum. These methods can make great progress initially, but often slow as they approach a solution. In contrast, full-gradient methods achieve steady convergence at the expense of evaluating the full objective and gradient on each iteration. We explore hybrid methods that exhibit the benefits of both approaches. Rate-of-convergence analysis shows that by controlling the sample size in an incremental gradient algorithm, it is possible to maintain the steady convergence rates of full-gradient methods. We detail a practical quasi-Newton implementation based on this approach. Numerical experiments illustrate its potential benefits.},
	number = {3},
	urldate = {2021-10-27},
	journal = {SIAM Journal on Scientific Computing},
	author = {Friedlander, Michael P. and Schmidt, Mark},
	month = jan,
	year = {2012},
	note = {arXiv: 1104.2373},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Statistics - Machine Learning},
	pages = {A1380--A1405},
}

@article{krizhevsky_one_2014,
	title = {One weird trick for parallelizing convolutional neural networks},
	url = {http://arxiv.org/abs/1404.5997},
	abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.},
	urldate = {2021-10-27},
	journal = {arXiv:1404.5997 [cs]},
	author = {Krizhevsky, Alex},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.5997},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{goyal_accurate_2018,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	shorttitle = {Accurate, {Large} {Minibatch} {SGD}},
	url = {http://arxiv.org/abs/1706.02677},
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\textasciitilde}90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
	urldate = {2021-10-27},
	journal = {arXiv:1706.02677 [cs]},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	month = apr,
	year = {2018},
	note = {arXiv: 1706.02677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@article{chetlur_cudnn_2014,
	title = {{cuDNN}: {Efficient} {Primitives} for {Deep} {Learning}},
	shorttitle = {{cuDNN}},
	url = {http://arxiv.org/abs/1410.0759},
	abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36\% on a standard model while also reducing memory consumption.},
	urldate = {2021-10-21},
	journal = {arXiv:1410.0759 [cs]},
	author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
	month = dec,
	year = {2014},
	note = {arXiv: 1410.0759},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Computer Science - Neural and Evolutionary Computing},
}

@article{hoi_online_2018,
	title = {Online {Learning}: {A} {Comprehensive} {Survey}},
	shorttitle = {Online {Learning}},
	url = {http://arxiv.org/abs/1802.02871},
	abstract = {Online learning represents an important family of machine learning algorithms, in which a learner attempts to resolve an online prediction (or any type of decision-making) task by learning a model/hypothesis from a sequence of data instances one at a time. The goal of online learning is to ensure that the online learner would make a sequence of accurate predictions (or correct decisions) given the knowledge of correct answers to previous prediction or learning tasks and possibly additional information. This is in contrast to many traditional batch learning or offline machine learning algorithms that are often designed to train a model in batch from a given collection of training data instances. This survey aims to provide a comprehensive survey of the online machine learning literatures through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the learning type and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) supervised online learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) unsupervised online learning where there is no feedback available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.},
	urldate = {2021-10-05},
	journal = {arXiv:1802.02871 [cs]},
	author = {Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.02871},
	keywords = {Computer Science - Machine Learning},
}

@article{madrid_towards_2019,
	title = {Towards {AutoML} in the presence of {Drift}: first results},
	shorttitle = {Towards {AutoML} in the presence of {Drift}},
	url = {http://arxiv.org/abs/1907.10772},
	abstract = {Research progress in AutoML has lead to state of the art solutions that can cope quite wellwith supervised learning task, e.g., classification with AutoSklearn. However, so far thesesystems do not take into account the changing nature of evolving data over time (i.e., theystill assume i.i.d. data); even when this sort of domains are increasingly available in realapplications (e.g., spam filtering, user preferences, etc.). We describe a first attempt to de-velop an AutoML solution for scenarios in which data distribution changes relatively slowlyover time and in which the problem is approached in a lifelong learning setting. We extendAuto-Sklearn with sound and intuitive mechanisms that allow it to cope with this sort ofproblems. The extended Auto-Sklearn is combined with concept drift detection techniquesthat allow it to automatically determine when the initial models have to be adapted. Wereport experimental results in benchmark data from AutoML competitions that adhere tothis scenario. Results demonstrate the effectiveness of the proposed methodology.},
	urldate = {2021-10-12},
	journal = {arXiv:1907.10772 [cs, stat]},
	author = {Madrid, Jorge G. and Escalante, Hugo Jair and Morales, Eduardo F. and Tu, Wei-Wei and Yu, Yang and Sun-Hosoya, Lisheng and Guyon, Isabelle and Sebag, Michele},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.10772},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sze_efficient_2017,
	title = {Efficient {Processing} of {Deep} {Neural} {Networks}: {A} {Tutorial} and {Survey}},
	volume = {105},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Efficient {Processing} of {Deep} {Neural} {Networks}},
	url = {http://ieeexplore.ieee.org/document/8114708/},
	doi = {10.1109/JPROC.2017.2761740},
	number = {12},
	urldate = {2021-10-20},
	journal = {Proceedings of the IEEE},
	author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
	month = dec,
	year = {2017},
	pages = {2295--2329},
}

@inproceedings{baena-garcia_early_2006,
	title = {Early {Drift} {Detection} {Method}},
	author = {Baena-García, Manuel and del Campo-Ávila, José and Fidalgo, Raúl and Bifet, Albert and Gavaldà, Ricard and Morales-Bueno, Rafael},
	year = {2006},
}

@inproceedings{bakirov_sequences_2015,
	address = {Killarney, Ireland},
	title = {On sequences of different adaptive mechanisms in non-stationary regression problems},
	isbn = {978-1-4799-1960-4},
	url = {http://ieeexplore.ieee.org/document/7280779/},
	doi = {10.1109/IJCNN.2015.7280779},
	urldate = {2021-10-12},
	booktitle = {2015 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Bakirov, Rashid and Gabrys, Bogdan and Fay, Damien},
	month = jul,
	year = {2015},
	pages = {1--8},
}

@incollection{frasconi_fast_2016,
	address = {Cham},
	title = {Fast {Hoeffding} {Drift} {Detection} {Method} for {Evolving} {Data} {Streams}},
	volume = {9852},
	isbn = {978-3-319-46226-4 978-3-319-46227-1},
	url = {https://link.springer.com/10.1007/978-3-319-46227-1_7},
	language = {en},
	urldate = {2021-10-12},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Pesaranghader, Ali and Viktor, Herna L.},
	editor = {Frasconi, Paolo and Landwehr, Niels and Manco, Giuseppe and Vreeken, Jilles},
	year = {2016},
	doi = {10.1007/978-3-319-46227-1_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {96--111},
}

@phdthesis{faithfull_unsupervised_2018,
	title = {Unsupervised {Change} {Detection} in {Multivariate} {Streaming} {Data}},
	url = {http://rgdoi.net/10.13140/RG.2.2.25121.66409},
	language = {en},
	urldate = {2021-10-12},
	author = {Faithfull, Will},
	year = {2018},
	note = {Publisher: Unpublished},
}

@phdthesis{zliobaite_adaptive_2010,
	address = {Vilnus},
	title = {Adaptive training set formation},
	url = {https://epublications.vu.lt/object/elaba:1932399/},
	urldate = {2021-10-12},
	school = {Vilnius University},
	author = {Žliobaitė, Indrė},
	year = {2010},
}

@article{webb_characterizing_2016,
	title = {Characterizing concept drift},
	volume = {30},
	issn = {1384-5810, 1573-756X},
	url = {http://link.springer.com/10.1007/s10618-015-0448-4},
	doi = {10.1007/s10618-015-0448-4},
	language = {en},
	number = {4},
	urldate = {2021-10-12},
	journal = {Data Mining and Knowledge Discovery},
	author = {Webb, Geoffrey I. and Hyde, Roy and Cao, Hong and Nguyen, Hai Long and Petitjean, Francois},
	month = jul,
	year = {2016},
	pages = {964--994},
}

@article{veloso_hyperparameter_2021,
	title = {Hyperparameter self-tuning for data streams},
	volume = {76},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253521000841},
	doi = {10.1016/j.inffus.2021.04.011},
	language = {en},
	urldate = {2021-10-12},
	journal = {Information Fusion},
	author = {Veloso, Bruno and Gama, João and Malheiro, Benedita and Vinagre, João},
	month = dec,
	year = {2021},
	pages = {75--86},
}

@inproceedings{gama_learning_2004,
	address = {Berlin, Heidelberg},
	title = {Learning with {Drift} {Detection}},
	isbn = {978-3-540-28645-5},
	abstract = {Most of the work in machine learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the distribution that generate the examples changes over time. We present a method for detection of changes in the probability distribution of examples. The idea behind the drift detection method is to control the online error-rate of the algorithm. The training examples are presented in sequence. When a new training example is available, it is classified using the actual model. Statistical theory guarantees that while the distribution is stationary, the error will decrease. When the distribution changes, the error will increase. The method controls the trace of the online error of the algorithm. For the actual context we define a warning level, and a drift level. A new context is declared, if in a sequence of examples, the error increases reaching the warning level at example kw, and the drift level at example kd. This is an indication of a change in the distribution of the examples. The algorithm learns a new model using only the examples since kw. The method was tested with a set of eight artificial datasets and a real world dataset. We used three learning algorithms: a perceptron, a neural network and a decision tree. The experimental results show a good performance detecting drift and with learning the new concept. We also observe that the method is independent of the learning algorithm.},
	booktitle = {Advances in {Artificial} {Intelligence} – {SBIA} 2004},
	publisher = {Springer Berlin Heidelberg},
	author = {Gama, João and Medas, Pedro and Castillo, Gladys and Rodrigues, Pedro},
	editor = {Bazzan, Ana L. C. and Labidi, Sofiane},
	year = {2004},
	pages = {286--295},
}

@article{bakirov_automated_2021,
	title = {Automated {Adaptation} {Strategies} for {Stream} {Learning}},
	url = {http://arxiv.org/abs/1812.10793},
	abstract = {Automation of machine learning model development is increasingly becoming an established research area. While automated model selection and automated data pre-processing have been studied in depth, there is, however, a gap concerning automated model adaptation strategies when multiple strategies are available. Manually developing an adaptation strategy can be time consuming and costly. In this paper we address this issue by proposing the use of flexible adaptive mechanism deployment for automated development of adaptation strategies. Experimental results after using the proposed strategies with five adaptive algorithms on 36 datasets confirm their viability. These strategies achieve better or comparable performance to the custom adaptation strategies and the repeated deployment of any single adaptive mechanism.},
	urldate = {2021-10-12},
	journal = {arXiv:1812.10793 [cs, stat]},
	author = {Bakirov, Rashid and Gabrys, Bogdan and Fay, Damien},
	month = apr,
	year = {2021},
	note = {arXiv: 1812.10793},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{celik_adaptation_2021,
	title = {Adaptation {Strategies} for {Automated} {Machine} {Learning} on {Evolving} {Data}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/2006.06480},
	doi = {10.1109/TPAMI.2021.3062900},
	abstract = {Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of data stream challenges such as concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on different AutoML approaches. We do this for a variety of AutoML approaches for building machine learning pipelines, including those that leverage Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.},
	number = {9},
	urldate = {2021-10-06},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Celik, Bilge and Vanschoren, Joaquin},
	month = sep,
	year = {2021},
	note = {arXiv: 2006.06480},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3067--3078},
}

@article{ben-nun_demystifying_2019,
	title = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}: {An} {In}-depth {Concurrency} {Analysis}},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3320060},
	doi = {10.1145/3320060},
	abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
	language = {en},
	number = {4},
	urldate = {2021-10-06},
	journal = {ACM Computing Surveys},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	month = sep,
	year = {2019},
	pages = {1--43},
}

@book{bifet_machine_2017,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning series},
	title = {Machine learning for data streams: with practical examples in {MOA}},
	isbn = {978-0-262-03779-2},
	shorttitle = {Machine learning for data streams},
	publisher = {MIT Press},
	author = {Bifet, Albert},
	year = {2017},
	keywords = {Data mining, Streaming technology (Telecommunications)},
}

@article{you_limit_2020,
	title = {The {Limit} of the {Batch} {Size}},
	url = {http://arxiv.org/abs/2006.08517},
	abstract = {Large-batch training is an efficient approach for current distributed deep learning systems. It has enabled researchers to reduce the ImageNet/ResNet-50 training from 29 hours to around 1 minute. In this paper, we focus on studying the limit of the batch size. We think it may provide a guidance to AI supercomputer and algorithm designers. We provide detailed numerical optimization instructions for step-by-step comparison. Moreover, it is important to understand the generalization and optimization performance of huge batch training. Hoffer et al. introduced "ultra-slow diffusion" theory to large-batch training. However, our experiments show contradictory results with the conclusion of Hoffer et al. We provide comprehensive experimental results and detailed analysis to study the limitations of batch size scaling and "ultra-slow diffusion" theory. For the first time we scale the batch size on ImageNet to at least a magnitude larger than all previous work, and provide detailed studies on the performance of many state-of-the-art optimization schemes under this setting. We propose an optimization recipe that is able to improve the top-1 test accuracy by 18\% compared to the baseline.},
	urldate = {2021-10-05},
	journal = {arXiv:2006.08517 [cs, stat]},
	author = {You, Yang and Wang, Yuhui and Zhang, Huan and Zhang, Zhao and Demmel, James and Hsieh, Cho-Jui},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.08517},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{hazelwood_applied_2018,
	address = {Vienna},
	title = {Applied {Machine} {Learning} at {Facebook}: {A} {Datacenter} {Infrastructure} {Perspective}},
	isbn = {978-1-5386-3659-6},
	shorttitle = {Applied {Machine} {Learning} at {Facebook}},
	url = {http://ieeexplore.ieee.org/document/8327042/},
	doi = {10.1109/HPCA.2018.00059},
	urldate = {2021-10-05},
	booktitle = {2018 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	publisher = {IEEE},
	author = {Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong},
	month = feb,
	year = {2018},
	pages = {620--629},
}

@article{you_large_2017,
	title = {Large {Batch} {Training} of {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1708.03888},
	abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
	urldate = {2021-10-05},
	journal = {arXiv:1708.03888 [cs]},
	author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	month = sep,
	year = {2017},
	note = {arXiv: 1708.03888},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{mai_kungfu_2020,
	title = {{KungFu}: {Making} {Training} in {Distributed} {Machine} {Learning} {Adaptive}},
	isbn = {978-1-939133-19-9},
	url = {https://www.usenix.org/conference/osdi20/presentation/mai},
	booktitle = {14th {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation} ({OSDI} 20)},
	publisher = {USENIX Association},
	author = {Mai, Luo and Li, Guo and Wagenländer, Marcel and Fertakis, Konstantinos and Brabete, Andrei-Octavian and Pietzuch, Peter},
	month = nov,
	year = {2020},
	pages = {937--954},
}

@inproceedings{you_large-batch_2019,
	address = {Denver Colorado},
	title = {Large-batch training for {LSTM} and beyond},
	isbn = {978-1-4503-6229-0},
	url = {https://dl.acm.org/doi/10.1145/3295500.3356137},
	doi = {10.1145/3295500.3356137},
	language = {en},
	urldate = {2021-10-05},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {ACM},
	author = {You, Yang and Hseu, Jonathan and Ying, Chris and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
	month = nov,
	year = {2019},
	pages = {1--16},
}

@article{he_automl_2021,
	title = {{AutoML}: {A} survey of the state-of-the-art},
	volume = {212},
	issn = {09507051},
	shorttitle = {{AutoML}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705120307516},
	doi = {10.1016/j.knosys.2020.106622},
	language = {en},
	urldate = {2021-09-26},
	journal = {Knowledge-Based Systems},
	author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
	month = jan,
	year = {2021},
	pages = {106622},
}

@inproceedings{maloof_method_1995,
	address = {Herndon, VA, USA},
	title = {A method for partial-memory incremental learning and its application to computer intrusion detection},
	isbn = {978-0-8186-7312-2},
	url = {http://ieeexplore.ieee.org/document/479784/},
	doi = {10.1109/TAI.1995.479784},
	urldate = {2021-09-17},
	booktitle = {Proceedings of 7th {IEEE} {International} {Conference} on {Tools} with {Artificial} {Intelligence}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Maloof, M.A. and Michalski, R.S.},
	year = {1995},
	pages = {392--397},
}

@article{maloof_incremental_2004,
	title = {Incremental learning with partial instance memory},
	volume = {154},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370203001498},
	doi = {10.1016/j.artint.2003.04.001},
	language = {en},
	number = {1-2},
	urldate = {2021-09-17},
	journal = {Artificial Intelligence},
	author = {Maloof, Marcus A. and Michalski, Ryszard S.},
	month = apr,
	year = {2004},
	pages = {95--126},
}

@inproceedings{koychev_gradual_2000,
	title = {Gradual {Forgetting} for {Adaptation} to {Concept} {Drift}},
	url = {http://hdl.handle.net/10506/57},
	urldate = {2021-09-16},
	booktitle = {Proceedings of {ECAI} 2000 {Workshop} on {Current} {Issues} in {Spatio}-{Temporal} {Reasoning}},
	author = {Koychev, Ivan},
	year = {2000},
}

@article{maloof_selecting_2000,
	title = {Selecting {Examples} for {Partial} {Memory} {Learning}},
	volume = {41},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1007661119649},
	doi = {10.1023/A:1007661119649},
	number = {1},
	urldate = {2021-09-17},
	journal = {Machine Learning},
	author = {Maloof, Marcus A. and Michalski, Ryszard S.},
	year = {2000},
	pages = {27--52},
}

@article{schlimmer_incremental_1986,
	title = {Incremental learning from noisy data},
	volume = {1},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00116895},
	doi = {10.1007/BF00116895},
	language = {en},
	number = {3},
	urldate = {2021-09-17},
	journal = {Machine Learning},
	author = {Schlimmer, Jeffrey C. and Granger, Richard H.},
	month = sep,
	year = {1986},
	pages = {317--354},
}

@inproceedings{gomes_learning_2011,
	address = {TaiChung, Taiwan},
	title = {Learning recurring concepts from data streams with a context-aware ensemble},
	isbn = {978-1-4503-0113-8},
	url = {http://portal.acm.org/citation.cfm?doid=1982185.1982403},
	doi = {10.1145/1982185.1982403},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the 2011 {ACM} {Symposium} on {Applied} {Computing} - {SAC} '11},
	publisher = {ACM Press},
	author = {Gomes, João Bártolo and Menasalvas, Ernestina and Sousa, Pedro A. C.},
	year = {2011},
	pages = {994},
}

@article{giraud-carrier_note_2000,
	title = {Note on the utility of incremental learning},
	volume = {13},
	issn = {09217126},
	number = {4},
	author = {Giraud-Carrier, C},
	year = {2000},
	pages = {215--223},
}

@incollection{widmer_effective_1993,
	address = {Berlin, Heidelberg},
	title = {Effective learning in dynamic environments by explicit context tracking},
	volume = {667},
	isbn = {978-3-540-56602-1 978-3-540-47597-2},
	url = {http://link.springer.com/10.1007/3-540-56602-3_139},
	urldate = {2021-09-17},
	booktitle = {Machine {Learning}: {ECML}-93},
	publisher = {Springer Berlin Heidelberg},
	author = {Widmer, Gerhard and Kubat, Miroslav},
	editor = {Siekmann, J. and Goos, G. and Hartmanis, J. and Brazdil, Pavel B.},
	year = {1993},
	doi = {10.1007/3-540-56602-3_139},
	pages = {227--243},
}

@article{klinkenberg_learning_2004,
	title = {Learning drifting concepts: {Example} selection vs. example weighting},
	volume = {8},
	issn = {15714128, 1088467X},
	shorttitle = {Learning drifting concepts},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IDA-2004-8305},
	doi = {10.3233/IDA-2004-8305},
	number = {3},
	urldate = {2021-09-17},
	journal = {Intelligent Data Analysis},
	author = {Klinkenberg, Ralf},
	month = aug,
	year = {2004},
	pages = {281--300},
}

@inproceedings{hulten_mining_2001,
	address = {San Francisco, California},
	title = {Mining time-changing data streams},
	isbn = {978-1-58113-391-2},
	url = {http://portal.acm.org/citation.cfm?doid=502512.502529},
	doi = {10.1145/502512.502529},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '01},
	publisher = {ACM Press},
	author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
	year = {2001},
	pages = {97--106},
}

@inproceedings{bach_paired_2008,
	address = {Pisa, Italy},
	title = {Paired {Learners} for {Concept} {Drift}},
	isbn = {978-0-7695-3502-9},
	url = {http://ieeexplore.ieee.org/document/4781097/},
	doi = {10.1109/ICDM.2008.119},
	urldate = {2021-09-17},
	booktitle = {2008 {Eighth} {IEEE} {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE},
	author = {Bach, Stephen H. and Maloof, Marcus A.},
	month = dec,
	year = {2008},
	pages = {23--32},
}

@incollection{chu_fast_2004,
	address = {Berlin, Heidelberg},
	title = {Fast and {Light} {Boosting} for {Adaptive} {Mining} of {Data} {Streams}},
	volume = {3056},
	isbn = {978-3-540-22064-0 978-3-540-24775-3},
	url = {http://link.springer.com/10.1007/978-3-540-24775-3_36},
	urldate = {2021-09-17},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chu, Fang and Zaniolo, Carlo},
	editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Dai, Honghua and Srikant, Ramakrishnan and Zhang, Chengqi},
	year = {2004},
	doi = {10.1007/978-3-540-24775-3_36},
	pages = {282--292},
}

@inproceedings{oza_experimental_2001,
	address = {San Francisco, California},
	title = {Experimental comparisons of online and batch versions of bagging and boosting},
	isbn = {978-1-58113-391-2},
	url = {http://portal.acm.org/citation.cfm?doid=502512.502565},
	doi = {10.1145/502512.502565},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '01},
	publisher = {ACM Press},
	author = {Oza, Nikunj C. and Russell, Stuart},
	year = {2001},
	pages = {359--364},
}

@article{minku_ddd_2012,
	title = {{DDD}: {A} {New} {Ensemble} {Approach} for {Dealing} with {Concept} {Drift}},
	volume = {24},
	issn = {1041-4347},
	shorttitle = {{DDD}},
	url = {http://ieeexplore.ieee.org/document/5719616/},
	doi = {10.1109/TKDE.2011.58},
	number = {4},
	urldate = {2021-09-17},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Minku, Leandro L. and Yao, Xin},
	month = apr,
	year = {2012},
	pages = {619--633},
}

@article{minku_impact_2010,
	title = {The {Impact} of {Diversity} on {Online} {Ensemble} {Learning} in the {Presence} of {Concept} {Drift}},
	volume = {22},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5156502/},
	doi = {10.1109/TKDE.2009.156},
	number = {5},
	urldate = {2021-09-17},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Minku, L.L. and White, A.P. and {Xin Yao}},
	month = may,
	year = {2010},
	pages = {730--742},
}

@inproceedings{kolter_dynamic_2007,
	title = {Dynamic {Weighted} {Majority}: {An} {Ensemble} {Method} for {Drifting} {Concepts}},
	url = {https://www.jmlr.org/papers/volume8/kolter07a/kolter07a.pdf},
	urldate = {2021-09-16},
	booktitle = {Journal of {Machine} {Learning} {Research}},
	author = {Kolter, J and Maloof, Marcus A},
	year = {2007},
}

@article{zliobaite_learning_2010,
	title = {Learning under {Concept} {Drift}: an {Overview}},
	shorttitle = {Learning under {Concept} {Drift}},
	url = {http://arxiv.org/abs/1010.4784},
	abstract = {Concept drift refers to a non stationary learning problem over time. The training and the application data often mismatch in real life problems. In this report we present a context of concept drift problem 1. We focus on the issues relevant to adaptive training set formation. We present the framework and terminology, and formulate a global picture of concept drift learners design. We start with formalizing the framework for the concept drifting data in Section 1. In Section 2 we discuss the adaptivity mechanisms of the concept drift learners. In Section 3 we overview the principle mechanisms of concept drift learners. In this chapter we give a general picture of the available algorithms and categorize them based on their properties. Section 5 discusses the related research fields and Section 5 groups and presents major concept drift applications. This report is intended to give a bird's view of concept drift research field, provide a context of the research and position it within broad spectrum of research fields and applications.},
	urldate = {2021-09-13},
	journal = {arXiv:1010.4784 [cs]},
	author = {Žliobaitė, Indrė},
	month = oct,
	year = {2010},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{kolter_using_2005,
	address = {Bonn, Germany},
	title = {Using additive expert ensembles to cope with concept drift},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102408},
	doi = {10.1145/1102351.1102408},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning - {ICML} '05},
	publisher = {ACM Press},
	author = {Kolter, Jeremy Z. and Maloof, Marcus A.},
	year = {2005},
	pages = {449--456},
}

@inproceedings{wang_mining_2003,
	address = {Washington, D.C.},
	title = {Mining concept-drifting data streams using ensemble classifiers},
	isbn = {978-1-58113-737-8},
	url = {http://portal.acm.org/citation.cfm?doid=956750.956778},
	doi = {10.1145/956750.956778},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the ninth {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '03},
	publisher = {ACM Press},
	author = {Wang, Haixun and Fan, Wei and Yu, Philip S. and Han, Jiawei},
	year = {2003},
	pages = {226},
}

@incollection{williamson_thought_nodate,
	title = {Thought experiments},
	booktitle = {Doing {Philosophy}},
	author = {Williamson, Timothy},
}
