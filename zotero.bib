
@article{anEffectsAddingNoise1996,
  title = {The {{Effects}} of {{Adding Noise During Backpropagation Training}} on a {{Generalization Performance}}},
  author = {An, Guozhong},
  date = {1996-04},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {8},
  number = {3},
  pages = {643--674},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1996.8.3.643},
  abstract = {We study the effects of adding noise to the inputs, outputs, weight connections, and weight changes of multilayer feedforward neural networks during backpropagation training. We rigorously derive and analyze the objective functions that are minimized by the noise-affected training processes. We show that input noise and weight noise encourage the neural-network output to be a smooth function of the input or its weights, respectively. In the weak-noise limit, noise added to the output of the neural networks only changes the objective function by a constant. Hence, it cannot improve generalization. Input noise introduces penalty terms in the objective function that are related to, but distinct from, those found in the regularization approaches. Simulations have been performed on a regression and a classification problem to further substantiate our analysis. Input noise is found to be effective in improving the generalization performance for both problems. However, weight noise is found to be effective in improving the generalization performance only for the classification problem. Other forms of noise have practically no effect on generalization.},
  langid = {english}
}

@inproceedings{bachPairedLearnersConcept2008,
  title = {Paired {{Learners}} for {{Concept Drift}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Bach, Stephen H. and Maloof, Marcus A.},
  date = {2008-12},
  pages = {23--32},
  publisher = {{IEEE}},
  location = {{Pisa, Italy}},
  doi = {10.1109/ICDM.2008.119},
  isbn = {978-0-7695-3502-9}
}

@inproceedings{baena-garciaEarlyDriftDetection2006,
  title = {Early {{Drift Detection Method}}},
  author = {Baena-García, Manuel and del Campo-Ávila, José and Fidalgo, Raúl and Bifet, Albert and Gavaldà, Ricard and Morales-Bueno, Rafael},
  options = {useprefix=true},
  date = {2006},
  eventtitle = {Fourth {{International Workshop}} on {{Knowledge Discovery}} from {{Data Streams}}}
}

@online{bakirovAutomatedAdaptationStrategies2021,
  title = {Automated {{Adaptation Strategies}} for {{Stream Learning}}},
  author = {Bakirov, Rashid and Gabrys, Bogdan and Fay, Damien},
  date = {2021-04-30},
  eprint = {1812.10793},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.10793},
  urldate = {2021-10-12},
  abstract = {Automation of machine learning model development is increasingly becoming an established research area. While automated model selection and automated data pre-processing have been studied in depth, there is, however, a gap concerning automated model adaptation strategies when multiple strategies are available. Manually developing an adaptation strategy can be time consuming and costly. In this paper we address this issue by proposing the use of flexible adaptive mechanism deployment for automated development of adaptation strategies. Experimental results after using the proposed strategies with five adaptive algorithms on 36 datasets confirm their viability. These strategies achieve better or comparable performance to the custom adaptation strategies and the repeated deployment of any single adaptive mechanism.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/87XLKU2W/Bakirov et al. - 2021 - Automated Adaptation Strategies for Stream Learnin.pdf;/home/riikoro/Zotero/storage/SBPVZEQ8/1812.html}
}

@inproceedings{bakirovSequencesDifferentAdaptive2015,
  title = {On Sequences of Different Adaptive Mechanisms in Non-Stationary Regression Problems},
  booktitle = {2015 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Bakirov, Rashid and Gabrys, Bogdan and Fay, Damien},
  date = {2015-07},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Killarney, Ireland}},
  doi = {10.1109/IJCNN.2015.7280779},
  eventtitle = {2015 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  isbn = {978-1-4799-1960-4}
}

@article{ben-nunDemystifyingParallelDistributed2019,
  title = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}: An {{In}}-Depth {{Concurrency Analysis}}},
  shorttitle = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}},
  author = {Ben-Nun, Tal and Hoefler, Torsten},
  date = {2019-09-18},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {52},
  number = {4},
  pages = {1--43},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3320060},
  abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
  langid = {english}
}

@incollection{bengioDeepLearningRepresentations2013,
  title = {Deep {{Learning}} of {{Representations}}: Looking {{Forward}}},
  shorttitle = {Deep {{Learning}} of {{Representations}}},
  booktitle = {Statistical {{Language}} and {{Speech Processing}}},
  author = {Bengio, Yoshua},
  editor = {Dediu, Adrian-Horia and Martín-Vide, Carlos and Mitkov, Ruslan and Truthe, Bianca},
  date = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {7978},
  pages = {1--37},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39593-2_1},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  isbn = {978-3-642-39592-5 978-3-642-39593-2},
  file = {/home/riikoro/Zotero/storage/KZIAF9NB/Bengio - 2013 - Deep Learning of Representations Looking Forward.pdf}
}

@article{bhattacherjeePrinciplesDatasetVersioning2015,
  title = {Principles of Dataset Versioning: Exploring the Recreation/Storage Tradeoff},
  shorttitle = {Principles of Dataset Versioning},
  author = {Bhattacherjee, Souvik and Chavan, Amit and Huang, Silu and Deshpande, Amol and Parameswaran, Aditya},
  date = {2015-08},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {8},
  number = {12},
  pages = {1346--1357},
  issn = {2150-8097},
  doi = {10.14778/2824032.2824035},
  langid = {english},
  file = {/home/riikoro/Zotero/storage/PAXUSFPW/Bhattacherjee et al. - 2015 - Principles of dataset versioning exploring the re.pdf}
}

@book{bifetMachineLearningData2017,
  title = {Machine Learning for Data Streams: With Practical Examples in {{MOA}}},
  shorttitle = {Machine Learning for Data Streams},
  author = {Bifet, Albert},
  date = {2017},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{MIT Press}},
  location = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-03779-2},
  pagetotal = {262},
  keywords = {Data mining,Streaming technology (Telecommunications)}
}

@online{breuelBenchmarkingLSTMNetworks2015,
  title = {Benchmarking of {{LSTM Networks}}},
  author = {Breuel, Thomas M.},
  date = {2015-08-11},
  eprint = {1508.02774},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1508.02774},
  urldate = {2021-10-31},
  abstract = {LSTM (Long Short-Term Memory) recurrent neural networks have been highly successful in a number of application areas. This technical report describes the use of the MNIST and UW3 databases for benchmarking LSTM networks and explores the effect of different architectural and hyperparameter choices on performance. Significant findings include: (1) LSTM performance depends smoothly on learning rates, (2) batching and momentum has no significant effect on performance, (3) softmax training outperforms least square training, (4) peephole units are not useful, (5) the standard non-linearities (tanh and sigmoid) perform best, (6) bidirectional training combined with CTC performs better than other methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,K.3.2},
  file = {/home/riikoro/Zotero/storage/KRQQDQ6E/Breuel - 2015 - Benchmarking of LSTM Networks.pdf;/home/riikoro/Zotero/storage/KZQUMES6/1508.html}
}

@article{celikAdaptationStrategiesAutomated2021,
  title = {Adaptation {{Strategies}} for {{Automated Machine Learning}} on {{Evolving Data}}},
  author = {Celik, Bilge and Vanschoren, Joaquin},
  date = {2021-09-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {43},
  number = {9},
  pages = {3067--3078},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3062900},
  abstract = {Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of data stream challenges such as concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on different AutoML approaches. We do this for a variety of AutoML approaches for building machine learning pipelines, including those that leverage Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/R7QCGRW6/Celik and Vanschoren - 2021 - Adaptation Strategies for Automated Machine Learni.pdf;/home/riikoro/Zotero/storage/PPK8GY6S/2006.html}
}

@online{chetlurCuDNNEfficientPrimitives2014,
  title = {{{cuDNN}}: Efficient {{Primitives}} for {{Deep Learning}}},
  shorttitle = {{{cuDNN}}},
  author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  date = {2014-12-17},
  eprint = {1410.0759},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1410.0759},
  urldate = {2021-10-21},
  abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36\% on a standard model while also reducing memory consumption.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Computer Science - Neural and Evolutionary Computing},
  file = {/home/riikoro/Zotero/storage/MCC49HWZ/Chetlur et al. - 2014 - cuDNN Efficient Primitives for Deep Learning.pdf;/home/riikoro/Zotero/storage/LBPVS7IF/1410.html}
}

@incollection{chuFastLightBoosting2004,
  title = {Fast and {{Light Boosting}} for {{Adaptive Mining}} of {{Data Streams}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chu, Fang and Zaniolo, Carlo},
  editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Dai, Honghua and Srikant, Ramakrishnan and Zhang, Chengqi},
  date = {2004},
  volume = {3056},
  pages = {282--292},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24775-3_36},
  isbn = {978-3-540-22064-0 978-3-540-24775-3}
}

@online{courbariauxBinarizedNeuralNetworks2016,
  title = {Binarized {{Neural Networks}}: Training {{Deep Neural Networks}} with {{Weights}} and {{Activations Constrained}} to +1 or -1},
  shorttitle = {Binarized {{Neural Networks}}},
  author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  date = {2016-03-17},
  eprint = {1602.02830},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1602.02830},
  urldate = {2021-10-27},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/riikoro/Zotero/storage/YWHXQ6ZX/Courbariaux et al. - 2016 - Binarized Neural Networks Training Deep Neural Ne.pdf;/home/riikoro/Zotero/storage/E96BJ29X/1602.html}
}

@inproceedings{courbariauxBinaryConnectTrainingDeep2016,
 author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {BinaryConnect: Training Deep Neural Networks with binary weights during propagations},
 url = {https://proceedings.neurips.cc/paper/2015/file/3e15cc11f979ed25912dff5b0669f2cd-Paper.pdf},
 volume = {28},
 urldate = {2021-10-24},
 year = {2015}
}


@online{courbariauxTrainingDeepNeural2015,
  title = {Training Deep Neural Networks with Low Precision Multiplications},
  author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  date = {2015-09-22},
  eprint = {1412.7024},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1412.7024},
  urldate = {2021-10-27},
  abstract = {Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/riikoro/Zotero/storage/66JQ5XST/Courbariaux et al. - 2015 - Training deep neural networks with low precision m.pdf;/home/riikoro/Zotero/storage/SAM7VAAS/1412.html}
}

@inproceedings{DBLP:conf/icml/LeNCLPN11,
  title = {On Optimization Methods for Deep Learning},
  booktitle = {{{ICML}}},
  author = {Le, Quoc V. and Ngiam, Jiquan and Coates, Adam and Lahiri, Ahbik and Prochnow, Bobby and Ng, Andrew Y.},
  date = {2011},
  pages = {265--272},
  urldate = {2021-10-24},
  url = {https://icml.cc/2011/papers/210_icmlpaper.pdf},
  cdate = {1293840000000}
}

@online{desaHighAccuracyLowPrecisionTraining2018,
  title = {High-{{Accuracy Low}}-{{Precision Training}}},
  author = {De Sa, Christopher and Leszczynski, Megan and Zhang, Jian and Marzoev, Alana and Aberger, Christopher R. and Olukotun, Kunle and Ré, Christopher},
  date = {2018-03-08},
  eprint = {1803.03383},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.03383},
  urldate = {2021-10-27},
  abstract = {Low-precision computation is often used to lower the time and energy cost of machine learning, and recently hardware accelerators have been developed to support it. Still, it has been used primarily for inference - not training. Previous low-precision training algorithms suffered from a fundamental tradeoff: as the number of bits of precision is lowered, quantization noise is added to the model, which limits statistical accuracy. To address this issue, we describe a simple low-precision stochastic gradient descent variant called HALP. HALP converges at the same theoretical rate as full-precision algorithms despite the noise introduced by using low precision throughout execution. The key idea is to use SVRG to reduce gradient variance, and to combine this with a novel technique called bit centering to reduce quantization error. We show that on the CPU, HALP can run up to \$4 \textbackslash times\$ faster than full-precision SVRG and can match its convergence trajectory. We implemented HALP in TensorQuant, and show that it exceeds the validation performance of plain low-precision SGD on two deep learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/SPX2BKUR/De Sa et al. - 2018 - High-Accuracy Low-Precision Training.pdf;/home/riikoro/Zotero/storage/HQ4DR24F/1803.html}
}

@online{ericsonPerformanceNetworkParallel2017,
  title = {On the {{Performance}} of {{Network Parallel Training}} in {{Artificial Neural Networks}}},
  author = {Ericson, Ludvig and Mbuvha, Rendani},
  date = {2017-01-18},
  eprint = {1701.05130},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1701.05130},
  urldate = {2021-10-31},
  abstract = {Artificial Neural Networks (ANNs) have received increasing attention in recent years with applications that span a wide range of disciplines including vital domains such as medicine, network security and autonomous transportation. However, neural network architectures are becoming increasingly complex and with an increasing need to obtain real-time results from such models, it has become pivotal to use parallelization as a mechanism for speeding up network training and deployment. In this work we propose an implementation of Network Parallel Training through Cannon's Algorithm for matrix multiplication. We show that increasing the number of processes speeds up training until the point where process communication costs become prohibitive; this point varies by network complexity. We also show through empirical efficiency calculations that the speedup obtained is superlinear.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Performance,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/ZF2PZEZN/Ericson and Mbuvha - 2017 - On the Performance of Network Parallel Training in.pdf;/home/riikoro/Zotero/storage/TB852CCG/1701.html}
}

@thesis{faithfullUnsupervisedChangeDetection2018,
  title = {Unsupervised {{Change Detection}} in {{Multivariate Streaming Data}}},
  author = {Faithfull, Will},
  date = {2018},
  institution = {{Bangor University}},
  type = {phdthesis},
  url = {http://rgdoi.net/10.13140/RG.2.2.25121.66409},
  urldate = {2021-10-12},
  langid = {english}
}

@article{friedlanderHybridDeterministicStochasticMethods2012,
  title = {Hybrid {{Deterministic}}-{{Stochastic Methods}} for {{Data Fitting}}},
  author = {Friedlander, Michael P. and Schmidt, Mark},
  date = {2012-01},
  journaltitle = {SIAM Journal on Scientific Computing},
  shortjournal = {SIAM J. Sci. Comput.},
  volume = {34},
  number = {3},
  eprint = {1104.2373},
  eprinttype = {arxiv},
  pages = {A1380-A1405},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/110830629},
  abstract = {Many structured data-fitting applications require the solution of an optimization problem involving a sum over a potentially large number of measurements. Incremental gradient algorithms offer inexpensive iterations by sampling a subset of the terms in the sum. These methods can make great progress initially, but often slow as they approach a solution. In contrast, full-gradient methods achieve steady convergence at the expense of evaluating the full objective and gradient on each iteration. We explore hybrid methods that exhibit the benefits of both approaches. Rate-of-convergence analysis shows that by controlling the sample size in an incremental gradient algorithm, it is possible to maintain the steady convergence rates of full-gradient methods. We detail a practical quasi-Newton implementation based on this approach. Numerical experiments illustrate its potential benefits.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/P72DC3V2/Friedlander and Schmidt - 2012 - Hybrid Deterministic-Stochastic Methods for Data F.pdf;/home/riikoro/Zotero/storage/2NR6BB7D/1104.html}
}

@inproceedings{gamaLearningDriftDetection2004,
  title = {Learning with {{Drift Detection}}},
  booktitle = {Advances in {{Artificial Intelligence}} – {{SBIA}} 2004},
  author = {Gama, João and Medas, Pedro and Castillo, Gladys and Rodrigues, Pedro},
  editor = {Bazzan, Ana L. C. and Labidi, Sofiane},
  date = {2004},
  pages = {286--295},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  abstract = {Most of the work in machine learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the distribution that generate the examples changes over time. We present a method for detection of changes in the probability distribution of examples. The idea behind the drift detection method is to control the online error-rate of the algorithm. The training examples are presented in sequence. When a new training example is available, it is classified using the actual model. Statistical theory guarantees that while the distribution is stationary, the error will decrease. When the distribution changes, the error will increase. The method controls the trace of the online error of the algorithm. For the actual context we define a warning level, and a drift level. A new context is declared, if in a sequence of examples, the error increases reaching the warning level at example kw, and the drift level at example kd. This is an indication of a change in the distribution of the examples. The algorithm learns a new model using only the examples since kw. The method was tested with a set of eight artificial datasets and a real world dataset. We used three learning algorithms: a perceptron, a neural network and a decision tree. The experimental results show a good performance detecting drift and with learning the new concept. We also observe that the method is independent of the learning algorithm.},
  isbn = {978-3-540-28645-5}
}

@article{giraud-carrierNoteUtilityIncremental2000,
  title = {Note on the Utility of Incremental Learning},
  author = {Giraud-Carrier, C},
  date = {2000},
  volume = {13},
  number = {4},
  pages = {215--223},
  issn = {09217126}
}

@inproceedings{gomesLearningRecurringConcepts2011,
  title = {Learning Recurring Concepts from Data Streams with a Context-Aware Ensemble},
  booktitle = {Proceedings of the 2011 {{ACM Symposium}} on {{Applied Computing}} - {{SAC}} '11},
  author = {Gomes, João Bártolo and Menasalvas, Ernestina and Sousa, Pedro A. C.},
  date = {2011},
  pages = {994},
  publisher = {{ACM Press}},
  location = {{TaiChung, Taiwan}},
  doi = {10.1145/1982185.1982403},
  isbn = {978-1-4503-0113-8},
  langid = {english}
}

@online{goyalAccurateLargeMinibatch2018,
  title = {Accurate, {{Large Minibatch SGD}}: Training {{ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  date = {2018-04-30},
  eprint = {1706.02677},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1706.02677},
  urldate = {2021-10-27},
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  file = {/home/riikoro/Zotero/storage/PS8GMEIC/Goyal et al. - 2018 - Accurate, Large Minibatch SGD Training ImageNet i.pdf;/home/riikoro/Zotero/storage/W9PUN5UA/1706.html}
}

@online{goyalAccurateLargeMinibatch2018a,
  title = {Accurate, {{Large Minibatch SGD}}: Training {{ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  date = {2018-04-30},
  eprint = {1706.02677},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1706.02677},
  urldate = {2021-10-31},
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  file = {/home/riikoro/Zotero/storage/RXNI2K4U/Goyal et al. - 2018 - Accurate, Large Minibatch SGD Training ImageNet i.pdf;/home/riikoro/Zotero/storage/B5SQFAXW/1706.html}
}

@inproceedings{hazelwoodAppliedMachineLearning2018,
  title = {Applied {{Machine Learning}} at {{Facebook}}: A {{Datacenter Infrastructure Perspective}}},
  shorttitle = {Applied {{Machine Learning}} at {{Facebook}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong},
  date = {2018-02},
  pages = {620--629},
  publisher = {{IEEE}},
  location = {{Vienna}},
  doi = {10.1109/HPCA.2018.00059},
  eventtitle = {2018 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  isbn = {978-1-5386-3659-6}
}

@article{heAutoMLSurveyStateoftheart2021,
  title = {{{AutoML}}: A Survey of the State-of-the-Art},
  shorttitle = {{{AutoML}}},
  author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  date = {2021-01},
  journaltitle = {Knowledge-Based Systems},
  volume = {212},
  pages = {106622},
  issn = {09507051},
  doi = {10.1016/j.knosys.2020.106622},
  langid = {english}
}

@inproceedings{hofferTrainLongerGeneralize2018,
author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
title = {Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Background: Deep learning models are typically trained using stochastic gradient descent
or one of its variants. These methods update the weights using their gradient, estimated
from a small fraction of the training data. It has been observed that when using large
batch sizes there is a persistent degradation in generalization performance - known
as the "generalization gap" phenomenon. Identifying the origin of this gap and closing
it had remained an open problem.Contributions: We examine the initial high learning
rate training phase. We find that the weight distance from its initialization grows
logarithmically with the number of weight updates. We therefore propose a "random
walk on a random landscape" statistical model which is known to exhibit similar "ultra-slow"
diffusion behavior. Following this hypothesis we conducted experiments to show empirically
that the "generalization gap" stems from the relatively small number of updates rather
than the batch size, and can be completely eliminated by adapting the training regime
used. We further investigate different techniques to train models in the large-batch
regime and present a novel algorithm named "Ghost Batch Normalization" which enables
significant decrease in the generalization gap without increasing the number of updates.
To validate our findings we conduct several additional experiments on MNIST, CIFAR-10,
CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning
training of deep models and suggest they may not be optimal to achieve good generalization.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {1729–1739},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}



@online{hoiOnlineLearningComprehensive2018,
  title = {Online {{Learning}}: A {{Comprehensive Survey}}},
  shorttitle = {Online {{Learning}}},
  author = {Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
  date = {2018-10-22},
  eprint = {1802.02871},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.02871},
  urldate = {2021-10-05},
  abstract = {Online learning represents an important family of machine learning algorithms, in which a learner attempts to resolve an online prediction (or any type of decision-making) task by learning a model/hypothesis from a sequence of data instances one at a time. The goal of online learning is to ensure that the online learner would make a sequence of accurate predictions (or correct decisions) given the knowledge of correct answers to previous prediction or learning tasks and possibly additional information. This is in contrast to many traditional batch learning or offline machine learning algorithms that are often designed to train a model in batch from a given collection of training data instances. This survey aims to provide a comprehensive survey of the online machine learning literatures through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the learning type and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) supervised online learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) unsupervised online learning where there is no feedback available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/riikoro/Zotero/storage/URNXRIXE/Hoi et al. - 2018 - Online Learning A Comprehensive Survey.pdf;/home/riikoro/Zotero/storage/YX8WGMY3/Hoi et al. - 2018 - Online Learning A Comprehensive Survey.pdf;/home/riikoro/Zotero/storage/54QFL6W8/1802.html;/home/riikoro/Zotero/storage/V9TX4FRU/1802.html}
}

@thesis{huangEffectiveDataVersioning2019,
  type = {phdthesis},
  title = {Effective {{Data Versioning For Collaborative Data Analytics}}},
  author = {Huang, Silu},
  date = {2019},
  institution = {{University of Illinois}},
  location = {{Urbana, Illinois}},
  url = {https://www.ideals.illinois.edu/bitstream/handle/2142/105669/HUANG-DISSERTATION-2019.pdf?sequence=1&isAllowed=y},
  urldate = {2021-02-11},
  pagetotal = {137}
}

@online{hubaraQuantizedNeuralNetworks2016,
  title = {Quantized {{Neural Networks}}: Training {{Neural Networks}} with {{Low Precision Weights}} and {{Activations}}},
  shorttitle = {Quantized {{Neural Networks}}},
  author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  date = {2016-09-22},
  eprint = {1609.07061},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1609.07061},
  urldate = {2021-10-27},
  abstract = {We introduce a method to train Quantized Neural Networks (QNNs) --- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At train-time the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves \$51\textbackslash\%\$ top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/riikoro/Zotero/storage/6KVQYJA8/Hubara et al. - 2016 - Quantized Neural Networks Training Neural Network.pdf;/home/riikoro/Zotero/storage/HT76FBPH/1609.html}
}

@inproceedings{hultenMiningTimechangingData2001,
  title = {Mining Time-Changing Data Streams},
  booktitle = {Proceedings of the Seventh {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '01},
  author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
  date = {2001},
  pages = {97--106},
  publisher = {{ACM Press}},
  location = {{San Francisco, California}},
  doi = {10.1145/502512.502529},
  isbn = {978-1-58113-391-2},
  langid = {english}
}

@online{jiaHighlyScalableDeep2018,
  title = {Highly {{Scalable Deep Learning Training System}} with {{Mixed}}-{{Precision}}: Training {{ImageNet}} in {{Four Minutes}}},
  shorttitle = {Highly {{Scalable Deep Learning Training System}} with {{Mixed}}-{{Precision}}},
  author = {Jia, Xianyan and Song, Shutao and He, Wei and Wang, Yangzihao and Rong, Haidong and Zhou, Feihu and Xie, Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei and Chen, Tiegang and Hu, Guangxiao and Shi, Shaohuai and Chu, Xiaowen},
  date = {2018-07-30},
  eprint = {1807.11205},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.11205},
  urldate = {2021-10-27},
  abstract = {Synchronized stochastic gradient descent (SGD) optimizers with data parallelism are widely used in training large-scale deep neural networks. Although using larger mini-batch sizes can improve the system scalability by reducing the communication-to-computation ratio, it may hurt the generalization ability of the models. To this end, we build a highly scalable deep learning training system for dense GPU clusters with three main contributions: (1) We propose a mixed-precision training method that significantly improves the training throughput of a single GPU without losing accuracy. (2) We propose an optimization approach for extremely large mini-batch size (up to 64k) that can train CNN models on the ImageNet dataset without losing accuracy. (3) We propose highly optimized all-reduce algorithms that achieve up to 3x and 11x speedup on AlexNet and ResNet-50 respectively than NCCL-based training on a cluster with 1024 Tesla P40 GPUs. On training ResNet-50 with 90 epochs, the state-of-the-art GPU-based system with 1024 Tesla P100 GPUs spent 15 minutes and achieved 74.9\textbackslash\% top-1 test accuracy, and another KNL-based system with 2048 Intel KNLs spent 20 minutes and achieved 75.4\textbackslash\% accuracy. Our training system can achieve 75.8\textbackslash\% top-1 test accuracy in only 6.6 minutes using 2048 Tesla P40 GPUs. When training AlexNet with 95 epochs, our system can achieve 58.7\textbackslash\% top-1 test accuracy within 4 minutes, which also outperforms all other existing systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/JR32ZQP7/Jia et al. - 2018 - Highly Scalable Deep Learning Training System with.pdf;/home/riikoro/Zotero/storage/9VAPNEAY/1807.html}
}

@article{klinkenbergLearningDriftingConcepts2004,
  title = {Learning Drifting Concepts: Example Selection vs. Example Weighting},
  shorttitle = {Learning Drifting Concepts},
  author = {Klinkenberg, Ralf},
  date = {2004-08},
  journaltitle = {Intelligent Data Analysis},
  volume = {8},
  number = {3},
  pages = {281--300},
  issn = {15714128, 1088467X},
  doi = {10.3233/IDA-2004-8305}
}

@inproceedings{kolterDynamicWeightedMajority2007,
  title = {Dynamic {{Weighted Majority}}: An {{Ensemble Method}} for {{Drifting Concepts}}},
  booktitle = {Journal of {{Machine Learning Research}}},
  author = {Kolter, J and Maloof, Marcus A},
  date = {2007},
  url = {https://www.jmlr.org/papers/volume8/kolter07a/kolter07a.pdf},
  urldate = {2021-09-16}
}

@inproceedings{kolterUsingAdditiveExpert2005,
  title = {Using Additive Expert Ensembles to Cope with Concept Drift},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning - {{ICML}} '05},
  author = {Kolter, Jeremy Z. and Maloof, Marcus A.},
  date = {2005},
  pages = {449--456},
  publisher = {{ACM Press}},
  location = {{Bonn, Germany}},
  doi = {10.1145/1102351.1102408},
  isbn = {978-1-59593-180-1},
  langid = {english}
}

@inproceedings{koychevGradualForgettingAdaptation2000,
  title = {Gradual {{Forgetting}} for {{Adaptation}} to {{Concept Drift}}},
  booktitle = {Proceedings of {{ECAI}} 2000 {{Workshop}} on {{Current Issues}} in {{Spatio}}-{{Temporal Reasoning}}},
  author = {Koychev, Ivan},
  date = {2000},
  url = {http://hdl.handle.net/10506/57},
  urldate = {2021-09-16}
}

@online{krizhevskyOneWeirdTrick2014,
  title = {One Weird Trick for Parallelizing Convolutional Neural Networks},
  author = {Krizhevsky, Alex},
  date = {2014-04-26},
  eprint = {1404.5997},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1404.5997},
  urldate = {2021-10-27},
  abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/riikoro/Zotero/storage/T4YD4YQG/Krizhevsky - 2014 - One weird trick for parallelizing convolutional ne.pdf;/home/riikoro/Zotero/storage/CEF7RRWK/1404.html}
}

@online{liTernaryWeightNetworks2016,
  title = {Ternary {{Weight Networks}}},
  author = {Li, Fengfu and Zhang, Bo and Liu, Bin},
  date = {2016-11-18},
  eprint = {1605.04711},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1605.04711},
  urldate = {2021-10-28},
  abstract = {We introduce ternary weight networks (TWNs) - neural networks with weights constrained to +1, 0 and -1. The Euclidian distance between full (float or double) precision weights and the ternary weights along with a scaling factor is minimized. Besides, a threshold-based ternary function is optimized to get an approximated solution which can be fast and easily computed. TWNs have stronger expressive abilities than the recently proposed binary precision counterparts and are thus more effective than the latter. Meanwhile, TWNs achieve up to 16\$\textbackslash times\$ or 32\$\textbackslash times\$ model compression rate and need fewer multiplications compared with the full precision counterparts. Benchmarks on MNIST, CIFAR-10, and large scale ImageNet datasets show that the performance of TWNs is only slightly worse than the full precision counterparts but outperforms the analogous binary precision counterparts a lot.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/riikoro/Zotero/storage/U3JG86VZ/Li et al. - 2016 - Ternary Weight Networks.pdf;/home/riikoro/Zotero/storage/SVARJM9Z/1605.html}
}

@article{maddoxDecibelRelationalDataset2016,
  title = {Decibel: The Relational Dataset Branching System},
  shorttitle = {Decibel},
  author = {Maddox, Michael and Goehring, David and Elmore, Aaron J. and Madden, Samuel and Parameswaran, Aditya and Deshpande, Amol},
  date = {2016-05},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {9},
  number = {9},
  pages = {624--635},
  issn = {2150-8097},
  doi = {10.14778/2947618.2947619},
  abstract = {As scientific endeavors and data analysis become increasingly collaborative, there is a need for data management systems that natively support the               versioning               or               branching               of datasets to enable concurrent analysis, cleaning, integration, manipulation, or curation of data across teams of individuals. Common practice for sharing and collaborating on datasets involves creating or storing multiple copies of the dataset, one for each stage of analysis, with no provenance information tracking the relationships between these datasets. This results not only in wasted storage, but also makes it challenging to track and integrate modifications made by different users to the same dataset. In this paper, we introduce the Relational Dataset Branching System, Decibel, a new relational storage system with built-in version control designed to address these short-comings. We present our initial design for Decibel and provide a thorough evaluation of three versioned storage engine designs that focus on efficient query processing with minimal storage overhead. We also develop an exhaustive benchmark to enable the rigorous testing of these and future versioned storage engine designs.},
  langid = {english},
  file = {/home/riikoro/Zotero/storage/MTW6GH8I/Maddox et al. - 2016 - Decibel the relational dataset branching system.pdf}
}

@inproceedings{madridAutoMLPresenceDrift2019,
  TITLE = {{Towards AutoML in the presence of Drift: first results}},
  AUTHOR = {Madrid, Jorge G and Jair Escalante, Hugo  and Morales, Eduardo F and Tu, Wei-Wei and Yu, Yang and Sun-Hosoya, Lisheng and Guyon, Isabelle and Sebag, Mich{\`e}le},
  URL = {https://hal.inria.fr/hal-01966962},
  BOOKTITLE = {{Workshop AutoML 2018 @ ICML/IJCAI-ECAI}},
  ADDRESS = {Stockholm, Sweden},
  ORGANIZATION = {{Pavel Brazdil and Christophe Giraud-Carrier and Isabelle Guyon}},
  YEAR = {2018},
  MONTH = Jul,
  KEYWORDS = {AutoML ; Life Long Machine Learning ; Concept Drift ; AutoSKLearn},
  PDF = {https://hal.inria.fr/hal-01966962/file/drift-wkpICML2018.pdf},
  HAL_ID = {hal-01966962},
  HAL_VERSION = {v1},
  urldate = {2021-10-12}
}

@inproceedings{maiKungFuMakingTraining2020,
  title = {{{KungFu}}: Making {{Training}} in {{Distributed Machine Learning Adaptive}}},
  booktitle = {14th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}} ({{OSDI}} 20)},
  author = {Mai, Luo and Li, Guo and Wagenländer, Marcel and Fertakis, Konstantinos and Brabete, Andrei-Octavian and Pietzuch, Peter},
  date = {2020-11},
  pages = {937--954},
  publisher = {{USENIX Association}},
  url = {https://www.usenix.org/conference/osdi20/presentation/mai},
  isbn = {978-1-939133-19-9}
}

@article{maloofIncrementalLearningPartial2004,
  title = {Incremental Learning with Partial Instance Memory},
  author = {Maloof, Marcus A. and Michalski, Ryszard S.},
  date = {2004-04},
  journaltitle = {Artificial Intelligence},
  volume = {154},
  number = {1-2},
  pages = {95--126},
  issn = {00043702},
  doi = {10.1016/j.artint.2003.04.001},
  langid = {english}
}

@inproceedings{maloofMethodPartialmemoryIncremental1995,
  title = {A Method for Partial-Memory Incremental Learning and Its Application to Computer Intrusion Detection},
  booktitle = {Proceedings of 7th {{IEEE International Conference}} on {{Tools}} with {{Artificial Intelligence}}},
  author = {Maloof, M.A. and Michalski, R.S.},
  date = {1995},
  pages = {392--397},
  publisher = {{IEEE Comput. Soc. Press}},
  location = {{Herndon, VA, USA}},
  doi = {10.1109/TAI.1995.479784},
  isbn = {978-0-8186-7312-2}
}

@article{maloofSelectingExamplesPartial2000,
  title = {Selecting {{Examples}} for {{Partial Memory Learning}}},
  author = {Maloof, Marcus A. and Michalski, Ryszard S.},
  date = {2000},
  journaltitle = {Machine Learning},
  volume = {41},
  number = {1},
  pages = {27--52},
  issn = {08856125},
  doi = {10.1023/A:1007661119649}
}

@inproceedings{
micikeviciusMixedPrecisionTraining2018,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={International Conference on Learning Representations},
year={2018},
 urldate = {2021-10-27},
url={https://openreview.net/forum?id=r1gs9JgRZ},
}

@online{mikamiMassivelyDistributedSGD2019,
  title = {Massively {{Distributed SGD}}: {{ImageNet}}/{{ResNet}}-50 {{Training}} in a {{Flash}}},
  shorttitle = {Massively {{Distributed SGD}}},
  author = {Mikami, Hiroaki and Suganuma, Hisahiro and U-chupala, Pongsakorn and Tanaka, Yoshiki and Kageyama, Yuichi},
  date = {2019-03-05},
  eprint = {1811.05233},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.05233},
  urldate = {2021-10-27},
  abstract = {Scaling the distributed deep learning to a massive GPU cluster level is challenging due to the instability of the large mini-batch training and the overhead of the gradient synchronization. We address the instability of the large mini-batch training with batch-size control and label smoothing. We address the overhead of the gradient synchronization with 2D-Torus all-reduce. Specifically, 2D-Torus all-reduce arranges GPUs in a logical 2D grid and performs a series of collective operation in different orientations. These two techniques are implemented with Neural Network Libraries (NNL). We have successfully trained ImageNet/ResNet-50 in 122 seconds without significant accuracy loss on ABCI cluster.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/riikoro/Zotero/storage/U8D24XUR/Mikami et al. - 2019 - Massively Distributed SGD ImageNetResNet-50 Trai.pdf;/home/riikoro/Zotero/storage/FRK2QIER/1811.html}
}

@article{minkuDDDNewEnsemble2012,
  title = {{{DDD}}: A {{New Ensemble Approach}} for {{Dealing}} with {{Concept Drift}}},
  shorttitle = {{{DDD}}},
  author = {Minku, Leandro L. and Yao, Xin},
  date = {2012-04},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {24},
  number = {4},
  pages = {619--633},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2011.58}
}

@article{minkuImpactDiversityOnline2010,
  title = {The {{Impact}} of {{Diversity}} on {{Online Ensemble Learning}} in the {{Presence}} of {{Concept Drift}}},
  author = {Minku, L.L. and White, A.P. and {Xin Yao}},
  date = {2010-05},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {22},
  number = {5},
  pages = {730--742},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2009.156}
}

@article{murrayEnhancedMLPPerformance1994,
  title = {Enhanced {{MLP}} Performance and Fault Tolerance Resulting from Synaptic Weight Noise during Training},
  author = {Murray, A.F. and Edwards, P.J.},
  month = {9},
  year = {1994},
  journaltitle = {IEEE Transactions on Neural Networks},
  shortjournal = {IEEE Trans. Neural Netw.},
  volume = {5},
  number = {5},
  pages = {792--802},
  issn = {10459227},
  doi = {10.1109/72.317730}
}

@online{osawaLargeScaleDistributedSecondOrder2019,
  title = {Large-{{Scale Distributed Second}}-{{Order Optimization Using Kronecker}}-{{Factored Approximate Curvature}} for {{Deep Convolutional Neural Networks}}},
  author = {Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
  date = {2019-03-30},
  eprint = {1811.12019},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.12019},
  urldate = {2021-10-27},
  abstract = {Large-scale distributed training of deep neural networks suffer from the generalization gap caused by the increase in the effective mini-batch size. Previous approaches try to solve this problem by varying the learning rate and batch size over epochs and layers, or some ad hoc modification of the batch normalization. We propose an alternative approach using a second-order optimization method that shows similar generalization capability to first-order methods, but converges faster and can handle larger mini-batches. To test our method on a benchmark where highly optimized first-order methods are available as references, we train ResNet-50 on ImageNet. We converged to 75\% Top-1 validation accuracy in 35 epochs for mini-batch sizes under 16,384, and achieved 75\% even with a mini-batch size of 131,072, which took only 978 iterations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/2WX9BFK3/Osawa et al. - 2019 - Large-Scale Distributed Second-Order Optimization .pdf;/home/riikoro/Zotero/storage/2V2GTQ9N/1811.html}
}

@inproceedings{ozaExperimentalComparisonsOnline2001,
  title = {Experimental Comparisons of Online and Batch Versions of Bagging and Boosting},
  booktitle = {Proceedings of the Seventh {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '01},
  author = {Oza, Nikunj C. and Russell, Stuart},
  date = {2001},
  pages = {359--364},
  publisher = {{ACM Press}},
  location = {{San Francisco, California}},
  doi = {10.1145/502512.502565},
  isbn = {978-1-58113-391-2},
  langid = {english}
}

@incollection{pesaranghaderFastHoeffdingDrift2016,
  title = {Fast {{Hoeffding Drift Detection Method}} for {{Evolving Data Streams}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Pesaranghader, Ali and Viktor, Herna L.},
  editor = {Frasconi, Paolo and Landwehr, Niels and Manco, Giuseppe and Vreeken, Jilles},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}. Vol. 9852},
  pages = {96--111},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-46227-1_7},
  langid = {english}
}

@inproceedings{pmlr-v37-gupta15,
  title = {Deep Learning with Limited Numerical Precision},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  editor = {Bach, Francis and Blei, David},
  date = {2015-07-07/2015-07-09},
  series = {Proceedings of Machine Learning Research},
  volume = {37},
  pages = {1737--1746},
  urldate = {2021-10-24},
  publisher = {{PMLR}},
  location = {{Lille, France}},
  url = {https://proceedings.mlr.press/v37/gupta15.html},
  abstract = {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network’s behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding},
  pdf = {http://proceedings.mlr.press/v37/gupta15.pdf}
}

@inproceedings{pmlr-v37-ioffe15,
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  author = {Ioffe, Sergey and Szegedy, Christian},
  editor = {Bach, Francis and Blei, David},
  date = {2015-07-07/2015-07-09},
  series = {Proceedings of Machine Learning Research},
  volume = {37},
  pages = {448--456},
  publisher = {{PMLR}},
  location = {{Lille, France}},
  url = {https://proceedings.mlr.press/v37/ioffe15.html},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
  pdf = {http://proceedings.mlr.press/v37/ioffe15.pdf}
}

@article{polyzotisDataLifecycleChallenges2018,
  title = {Data {{Lifecycle Challenges}} in {{Production Machine Learning}}: A {{Survey}}},
  shorttitle = {Data {{Lifecycle Challenges}} in {{Production Machine Learning}}},
  author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
  date = {2018-12-11},
  journaltitle = {ACM SIGMOD Record},
  shortjournal = {SIGMOD Rec.},
  volume = {47},
  number = {2},
  pages = {17--28},
  issn = {0163-5808},
  doi = {10.1145/3299887.3299891},
  abstract = {Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.},
  langid = {english}
}

@incollection{rastegariXNORNetImageNetClassification2016,
  title = {{{XNOR}}-{{Net}}: {{ImageNet Classification Using Binary Convolutional Neural Networks}}},
  shorttitle = {{{XNOR}}-{{Net}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2016},
  author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {9908},
  pages = {525--542},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-46493-0_32},
  langid = {english},
  file = {/home/riikoro/Zotero/storage/KAJ9IWMH/Rastegari et al. - 2016 - XNOR-Net ImageNet Classification Using Binary Con.pdf}
}

@online{renggliDataQualityDrivenView2021,
  title = {A {{Data Quality}}-{{Driven View}} of {{MLOps}}},
  author = {Renggli, Cedric and Rimanic, Luka and Gürel, Nezihe Merve and Karlaš, Bojan and Wu, Wentao and Zhang, Ce},
  date = {2021-02-15},
  eprint = {2102.07750},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.07750},
  urldate = {2021-11-02},
  abstract = {Developing machine learning models can be seen as a process similar to the one established for traditional software development. A key difference between the two lies in the strong dependency between the quality of a machine learning model and the quality of the data used to train or perform evaluations. In this work, we demonstrate how different aspects of data quality propagate through various stages of machine learning development. By performing a joint analysis of the impact of well-known data quality dimensions and the downstream machine learning process, we show that different components of a typical MLOps pipeline can be efficiently designed, providing both a technical and theoretical perspective.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning},
  file = {/home/riikoro/Zotero/storage/PJWL85VB/Renggli et al. - 2021 - A Data Quality-Driven View of MLOps.pdf;/home/riikoro/Zotero/storage/AX5KMNXV/2102.html}
}

@article{schlimmerIncrementalLearningNoisy1986,
  title = {Incremental Learning from Noisy Data},
  author = {Schlimmer, Jeffrey C. and Granger, Richard H.},
  date = {1986-09},
  journaltitle = {Machine Learning},
  volume = {1},
  number = {3},
  pages = {317--354},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00116895},
  langid = {english}
}

@article{shallueMeasuringEffectsData2019,
  author  = {Christopher J. Shallue and Jaehoon Lee and Joseph Antognini and Jascha Sohl-Dickstein and Roy Frostig and George E. Dahl},
  title   = {Measuring the Effects of Data Parallelism on Neural Network Training},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {112},
  pages   = {1-49},
  urldate = {2021-10-27},
  url     = {http://jmlr.org/papers/v20/18-789.html}
}

@inproceedings{smithDonDecayLearning2018,
title={Don't Decay the Learning Rate, Increase the Batch Size},
author={Samuel L. Smith and Pieter-Jan Kindermans and Quoc V. Le},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1Yy1BxCZ},
urldate = {2021-10-28}
}

@online{smithDonDecayLearning2018a,
  title = {Don't {{Decay}} the {{Learning Rate}}, {{Increase}} the {{Batch Size}}},
  author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  date = {2018-02-23},
  eprint = {1711.00489},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1711.00489},
  urldate = {2021-10-31},
  abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate \$\textbackslash epsilon\$ and scaling the batch size \$B \textbackslash propto \textbackslash epsilon\$. Finally, one can increase the momentum coefficient \$m\$ and scale \$B \textbackslash propto 1/(1-m)\$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to \$76.1\textbackslash\%\$ validation accuracy in under 30 minutes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/DW6MGZ62/Smith et al. - 2018 - Don't Decay the Learning Rate, Increase the Batch .pdf;/home/riikoro/Zotero/storage/X98TSSWV/1711.html}
}

@article{szeEfficientProcessingDeep2017,
  title = {Efficient {{Processing}} of {{Deep Neural Networks}}: A {{Tutorial}} and {{Survey}}},
  shorttitle = {Efficient {{Processing}} of {{Deep Neural Networks}}},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  date = {2017-12},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {105},
  number = {12},
  pages = {2295--2329},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2017.2761740},
  file = {/home/riikoro/Zotero/storage/6TNGNIDR/Sze et al. - 2017 - Efficient Processing of Deep Neural Networks A Tu.pdf}
}

@inproceedings{vanderweideVersioningEndtoEndMachine2017,
  title = {Versioning for {{End}}-to-{{End Machine Learning Pipelines}}},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{Data Management}} for {{End}}-to-{{End Machine Learning}}},
  author = {van der Weide, Tom and Papadopoulos, Dimitris and Smirnov, Oleg and Zielinski, Michal and van Kasteren, Tim},
  options = {useprefix=true},
  date = {2017-05-14},
  pages = {1--9},
  publisher = {{ACM}},
  location = {{Chicago IL USA}},
  doi = {10.1145/3076246.3076248},
  eventtitle = {{{SIGMOD}}/{{PODS}}'17: International {{Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-5026-6},
  langid = {english}
}

@article{velosoHyperparameterSelftuningData2021,
  title = {Hyperparameter Self-Tuning for Data Streams},
  author = {Veloso, Bruno and Gama, João and Malheiro, Benedita and Vinagre, João},
  date = {2021-12},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {76},
  pages = {75--86},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.04.011},
  langid = {english}
}

@inproceedings{wangMiningConceptdriftingData2003,
  title = {Mining Concept-Drifting Data Streams Using Ensemble Classifiers},
  booktitle = {Proceedings of the Ninth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '03},
  author = {Wang, Haixun and Fan, Wei and Yu, Philip S. and Han, Jiawei},
  date = {2003},
  pages = {226},
  publisher = {{ACM Press}},
  location = {{Washington, D.C.}},
  doi = {10.1145/956750.956778},
  isbn = {978-1-58113-737-8},
  langid = {english}
}

@article{webbCharacterizingConceptDrift2016,
  title = {Characterizing Concept Drift},
  author = {Webb, Geoffrey I. and Hyde, Roy and Cao, Hong and Nguyen, Hai Long and Petitjean, Francois},
  date = {2016-07},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Min Knowl Disc},
  volume = {30},
  number = {4},
  pages = {964--994},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-015-0448-4},
  langid = {english},
  file = {/home/riikoro/Zotero/storage/XMWI3BYM/Webb et al. - 2016 - Characterizing concept drift.pdf}
}

@incollection{widmerEffectiveLearningDynamic1993,
  title = {Effective Learning in Dynamic Environments by Explicit Context Tracking},
  booktitle = {Machine {{Learning}}: {{ECML}}-93},
  author = {Widmer, Gerhard and Kubat, Miroslav},
  editor = {Siekmann, J. and Goos, G. and Hartmanis, J. and Brazdil, Pavel B.},
  date = {1993},
  volume = {667},
  pages = {227--243},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-56602-3_139},
  isbn = {978-3-540-56602-1 978-3-540-47597-2}
}

@online{youLargeBatchTraining2017,
  title = {Large {{Batch Training}} of {{Convolutional Networks}}},
  author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
  date = {2017-09-13},
  eprint = {1708.03888},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1708.03888},
  urldate = {2021-10-05},
  abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/riikoro/Zotero/storage/6XRZSNFS/You et al. - 2017 - Large Batch Training of Convolutional Networks.pdf;/home/riikoro/Zotero/storage/32JRGHMM/1708.html}
}

@inproceedings{youLargebatchTrainingLSTM2019,
  title = {Large-Batch Training for {{LSTM}} and Beyond},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {You, Yang and Hseu, Jonathan and Ying, Chris and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  date = {2019-11-17},
  pages = {1--16},
  publisher = {{ACM}},
  location = {{Denver Colorado}},
  doi = {10.1145/3295500.3356137},
  eventtitle = {{{SC}} '19: The {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}}, and {{Analysis}}},
  isbn = {978-1-4503-6229-0},
  langid = {english},
  file = {/home/riikoro/Zotero/storage/HGL2ATJ8/You et al. - 2019 - Large-batch training for LSTM and beyond.pdf}
}

@online{youLimitBatchSize2020,
  title = {The {{Limit}} of the {{Batch Size}}},
  author = {You, Yang and Wang, Yuhui and Zhang, Huan and Zhang, Zhao and Demmel, James and Hsieh, Cho-Jui},
  date = {2020-06-15},
  eprint = {2006.08517},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.08517},
  urldate = {2021-10-05},
  abstract = {Large-batch training is an efficient approach for current distributed deep learning systems. It has enabled researchers to reduce the ImageNet/ResNet-50 training from 29 hours to around 1 minute. In this paper, we focus on studying the limit of the batch size. We think it may provide a guidance to AI supercomputer and algorithm designers. We provide detailed numerical optimization instructions for step-by-step comparison. Moreover, it is important to understand the generalization and optimization performance of huge batch training. Hoffer et al. introduced "ultra-slow diffusion" theory to large-batch training. However, our experiments show contradictory results with the conclusion of Hoffer et al. We provide comprehensive experimental results and detailed analysis to study the limitations of batch size scaling and "ultra-slow diffusion" theory. For the first time we scale the batch size on ImageNet to at least a magnitude larger than all previous work, and provide detailed studies on the performance of many state-of-the-art optimization schemes under this setting. We propose an optimization recipe that is able to improve the top-1 test accuracy by 18\% compared to the baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/riikoro/Zotero/storage/SDMNT6CR/You et al. - 2020 - The Limit of the Batch Size.pdf;/home/riikoro/Zotero/storage/XDFC6SAN/2006.html}
}

@thesis{zliobaiteAdaptiveTrainingSet2010,
  title = {Adaptive Training Set Formation},
  type = {phdthesis},
  author = {Žliobaitė, Indrė},
  date = {2010},
  institution = {{Vilnius University}},
  location = {{Vilnus}},
  url = {https://epublications.vu.lt/object/elaba:1932399/},
  urldate = {2021-10-12}
}

@online{zliobaiteLearningConceptDrift2010,
  title = {Learning under {{Concept Drift}}: An {{Overview}}},
  shorttitle = {Learning under {{Concept Drift}}},
  author = {Žliobaitė, Indrė},
  date = {2010-10},
  eprint = {1010.4784},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1010.4784},
  urldate = {2021-09-13},
  abstract = {Concept drift refers to a non stationary learning problem over time. The training and the application data often mismatch in real life problems. In this report we present a context of concept drift problem 1. We focus on the issues relevant to adaptive training set formation. We present the framework and terminology, and formulate a global picture of concept drift learners design. We start with formalizing the framework for the concept drifting data in Section 1. In Section 2 we discuss the adaptivity mechanisms of the concept drift learners. In Section 3 we overview the principle mechanisms of concept drift learners. In this chapter we give a general picture of the available algorithms and categorize them based on their properties. Section 5 discusses the related research fields and Section 5 groups and presents major concept drift applications. This report is intended to give a bird's view of concept drift research field, provide a context of the research and position it within broad spectrum of research fields and applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}


