


@inproceedings{wang_mining_2003,
	address = {Washington, D.C.},
	title = {Mining concept-drifting data streams using ensemble classifiers},
	isbn = {978-1-58113-737-8},
	url = {http://portal.acm.org/citation.cfm?doid=956750.956778},
	doi = {10.1145/956750.956778},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the ninth {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining  - {KDD} '03},
	publisher = {ACM Press},
	author = {Wang, Haixun and Fan, Wei and Yu, Philip S. and Han, Jiawei},
	year = {2003},
	pages = {226},
	annote = {1750 cites. Identifies the main challenges in stream mining to be handling voluminous data, handling concept drifts and (more minor point) ease of use. Argues that ensembles are better in both first 2 respects; ensembles perform in general better and are also better parallelizable and more efficient to build. Other results: bigger ensembles have less error, smaller windows are more drift sensitive.},
}

@inproceedings{kolter_dynamic_2007,
	title = {Dynamic {Weighted} {Majority}: {An} {Ensemble} {Method} for {Drifting} {Concepts}},
	url = {https://www.jmlr.org/papers/volume8/kolter07a/kolter07a.pdf},
	urldate = {2021-09-16},
	booktitle = {Journal of {Machine} {Learning} {Research}},
	author = {Kolter, J and Maloof, Marcus A},
	year = {2007},
	annote = {1099 cites. Presents a new method of ensembles; old weighted majority with the possibility of adding and removing models. Empirical experiments conducted in drifting and static environments with STAGGER, AQ-PM, AQ11-PM, AQ11-PM+WAH. DVM outperforms every other in drifting and is equally good in a static environment.},
}

@article{minku_impact_2010,
	title = {The {Impact} of {Diversity} on {Online} {Ensemble} {Learning} in the {Presence} of {Concept} {Drift}},
	volume = {22},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5156502/},
	doi = {10.1109/TKDE.2009.156},
	number = {5},
	urldate = {2021-09-17},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Minku, L.L. and White, A.P. and {Xin Yao}},
	month = may,
	year = {2010},
	pages = {730--742},
	annote = {465 cites. Considers how the diversity of ensemble models affects performance and thus tries to go deeper on why ensembles help. Results: less diversity is better when no drift is present, high diversity is better in case of drifting. This paper is quite "deep in ensemble", should be gone into deeper only if ensemble seems like the one n only good thing to do.},
}

@article{minku_ddd_2012,
	title = {{DDD}: {A} {New} {Ensemble} {Approach} for {Dealing} with {Concept} {Drift}},
	volume = {24},
	issn = {1041-4347},
	shorttitle = {{DDD}},
	url = {http://ieeexplore.ieee.org/document/5719616/},
	doi = {10.1109/TKDE.2011.58},
	number = {4},
	urldate = {2021-09-17},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Minku, Leandro L. and Yao, Xin},
	month = apr,
	year = {2012},
	pages = {619--633},
	annote = {413 cites. An algorithm that takes into account the considerations in diversity study (The Impact of Diversity on Online Ensemble Learning in the Presence of Concept Drift). States that it mostly outperforms DWM and some other, but this is left a little vague. Also considers false positives in drift detection. Has a section with references to incremental approaches and arguments on why ensembles are better (sect 3). Otherwise, contains more deep stuff on ensembles and should be gone into if ensemble feels the best-only.},
	file = {Full Text:/home/riikoro/Zotero/storage/3454L87I/Minku and Yao - 2012 - DDD A New Ensemble Approach for Dealing with Conc.pdf:application/pdf},
}

@inproceedings{oza_experimental_2001,
	address = {San Francisco, California},
	title = {Experimental comparisons of online and batch versions of bagging and boosting},
	isbn = {978-1-58113-391-2},
	url = {http://portal.acm.org/citation.cfm?doid=502512.502565},
	doi = {10.1145/502512.502565},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining  - {KDD} '01},
	publisher = {ACM Press},
	author = {Oza, Nikunj C. and Russell, Stuart},
	year = {2001},
	pages = {359--364},
	annote = {250 cites. Considers the ensemble creating approaches of bagging and boosting (refs 3 and 4 in this paper explain them). These are traditionally done in batch mode. This study presents their online versions ans benchmarks their performance agains the batch versions. Ensemble details.},
}

@inproceedings{kolter_using_2005,
	address = {Bonn, Germany},
	title = {Using additive expert ensembles to cope with concept drift},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102408},
	doi = {10.1145/1102351.1102408},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning  - {ICML} '05},
	publisher = {ACM Press},
	author = {Kolter, Jeremy Z. and Maloof, Marcus A.},
	year = {2005},
	pages = {449--456},
	annote = {303 cites. Introduces the AddExp ensemble approach that can be used with any online learner. Includes empirical comparisons to other approaches on the STAGGER data set and pruning strategy (when to drop experts) comparisons. No considerations on if ensemble is better than incremental, so this is ensemble-specific.},
}

@incollection{kanade_fast_2004,
	address = {Berlin, Heidelberg},
	title = {Fast and {Light} {Boosting} for {Adaptive} {Mining} of {Data} {Streams}},
	volume = {3056},
	isbn = {978-3-540-22064-0 978-3-540-24775-3},
	url = {http://link.springer.com/10.1007/978-3-540-24775-3_36},
	urldate = {2021-09-17},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chu, Fang and Zaniolo, Carlo},
	editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Dai, Honghua and Srikant, Ramakrishnan and Zhang, Chengqi},
	year = {2004},
	doi = {10.1007/978-3-540-24775-3_36},
	pages = {282--292},
	annote = {177 cites. Presents an online boosting ensemble that has the novelties of simpler base learners and a change detection scheme to meet main requirements of performance and ability to adapt. Uses a kind of dynamic weight assignment. The method is compared to and outperforms bagging and weighted bagging. Seems to consider classifying, is not ideal for dealing with gradual drifts.},
	file = {Submitted Version:/home/riikoro/Zotero/storage/WVGC9YW2/Chu and Zaniolo - 2004 - Fast and Light Boosting for Adaptive Mining of Dat.pdf:application/pdf},
}

@inproceedings{bach_paired_2008,
	address = {Pisa, Italy},
	title = {Paired {Learners} for {Concept} {Drift}},
	isbn = {978-0-7695-3502-9},
	url = {http://ieeexplore.ieee.org/document/4781097/},
	doi = {10.1109/ICDM.2008.119},
	urldate = {2021-09-17},
	booktitle = {2008 {Eighth} {IEEE} {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE},
	author = {Bach, Stephen H. and Maloof, Marcus A.},
	month = dec,
	year = {2008},
	pages = {23--32},
	annote = {145 cites. Uses 2 models, a stable and a reactive one. Accuracy difference between those is used as value for trigger. Empirical study conducted with Stagger and Sea concepts and the DWM, accuracy weighted ensemble and sea algorithms. Suggests comparable to better performance with less time and space requirements.},
}

@inproceedings{gomes_learning_2011,
	address = {TaiChung, Taiwan},
	title = {Learning recurring concepts from data streams with a context-aware ensemble},
	isbn = {978-1-4503-0113-8},
	url = {http://portal.acm.org/citation.cfm?doid=1982185.1982403},
	doi = {10.1145/1982185.1982403},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the 2011 {ACM} {Symposium} on {Applied} {Computing} - {SAC} '11},
	publisher = {ACM Press},
	author = {Gomes, João Bártolo and Menasalvas, Ernestina and Sousa, Pedro A. C.},
	year = {2011},
	pages = {994},
	annote = {55 cites. Considers how context information can be used in updating an ensemble and pruning outdated models. Empirical tests prove that this performs better compared to a single-model approach. However, leaves many gaps open like how to set the required parameters, so seems a somewhat unexplored approach that might be immature. References regarding context: 6, 16, 17.},
}

@article{schlimmer_incremental_1986,
	title = {Incremental learning from noisy data},
	volume = {1},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00116895},
	doi = {10.1007/BF00116895},
	language = {en},
	number = {3},
	urldate = {2021-09-17},
	journal = {Machine Learning},
	author = {Schlimmer, Jeffrey C. and Granger, Richard H.},
	month = sep,
	year = {1986},
	pages = {317--354},
	annote = {707 cites. Schlimmer \& Granger introducing STAGGER, the 1st existing solution (according to DWM paper) to address concept drift. STAGGER deals with pre-labeled binary classification only, is probably not useful as is, but as a comparison point that other, more modern technologies use. This is a single-model incremental solution, there are 2 processes updating a single model},
	file = {Full Text:/home/riikoro/Zotero/storage/DDKSFP26/Schlimmer and Granger - 1986 - Incremental learning from noisy data.pdf:application/pdf},
}

@inproceedings{hulten_mining_2001,
	address = {San Francisco, California},
	title = {Mining time-changing data streams},
	isbn = {978-1-58113-391-2},
	url = {http://portal.acm.org/citation.cfm?doid=502512.502529},
	doi = {10.1145/502512.502529},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining  - {KDD} '01},
	publisher = {ACM Press},
	author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
	year = {2001},
	pages = {97--106},
	annote = {2088 cites. CVFDT algo for windowing-outperforming fast classification. A decision tree model, updated from the previous algorithm, VFDT. From 2001, outdated approach in terms of data volume.},
}

@article{klinkenberg_learning_2004,
	title = {Learning drifting concepts: {Example} selection vs. example weighting},
	volume = {8},
	issn = {15714128, 1088467X},
	shorttitle = {Learning drifting concepts},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IDA-2004-8305},
	doi = {10.3233/IDA-2004-8305},
	number = {3},
	urldate = {2021-09-17},
	journal = {Intelligent Data Analysis},
	author = {Klinkenberg, Ralf},
	month = aug,
	year = {2004},
	pages = {281--300},
	annote = {584 cites. Presents a support vector machine approach for adapting to drift by adjusting training window size, and selecting and weighting examples. SVM is for classification only so it is unsure if this has any applicability for our case.},
}

@article{maloof_selecting_2000,
	title = {Selecting {Examples} for {Partial} {Memory} {Learning}},
	volume = {41},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1007661119649},
	doi = {10.1023/A:1007661119649},
	number = {1},
	urldate = {2021-09-17},
	journal = {Machine Learning},
	author = {Maloof, Marcus A. and Michalski, Ryszard S.},
	year = {2000},
	pages = {27--52},
	annote = {237 cites, maloof\&michalski 2000 (old). Presents how to choose examples for a partial memory learner (uses a combination of old and new data examples to adjust to concepts) by choosing examples close to concept boundaries and forgetting old examples. Compares to IB2 and FLORAs using STAGGER concepts.},
	file = {Full Text:/home/riikoro/Zotero/storage/LT6GIA5E/Maloof and Michalski - 2000 - [No title found].pdf:application/pdf},
}

@incollection{siekmann_effective_1993,
	address = {Berlin, Heidelberg},
	title = {Effective learning in dynamic environments by explicit context tracking},
	volume = {667},
	isbn = {978-3-540-56602-1 978-3-540-47597-2},
	url = {http://link.springer.com/10.1007/3-540-56602-3_139},
	urldate = {2021-09-17},
	booktitle = {Machine {Learning}: {ECML}-93},
	publisher = {Springer Berlin Heidelberg},
	author = {Widmer, Gerhard and Kubat, Miroslav},
	editor = {Siekmann, J. and Goos, G. and Hartmanis, J. and Brazdil, Pavel B.},
	year = {1993},
	doi = {10.1007/3-540-56602-3_139},
	pages = {227--243},
	annote = {234 cites, '05. Presents FLORA3, an improvement to FLORA2 that has as additions the ability to adjust window size and store old concepts (FLORAs deal in general mainly with windowing for classificaiton). Also context information is utilized. This gains better performance in dealing with abrupt drifts.},
	file = {Full Text:/home/riikoro/Zotero/storage/Z3L9NN37/Widmer and Kubat - 1993 - Effective learning in dynamic environments by expl.pdf:application/pdf},
}

@article{giraud-carrier_note_2000,
	title = {Note on the utility of incremental learning},
	volume = {13},
	issn = {09217126},
	number = {4},
	author = {Giraud-Carrier, C},
	year = {2000},
	pages = {215--223},
	annote = {cites 232. Defines incremental learning tasks (task where not all data is available at once and waiting for all is impossible or impractical) and incremental algorithms (algorithms that update their hypotheses while examples arrive). Argues that incrementality should be used only for incremental tasks.},
}

@inproceedings{koychev_gradual_2000,
	title = {Gradual {Forgetting} for {Adaptation} to {Concept} {Drift}},
	url = {http://hdl.handle.net/10506/57},
	urldate = {2021-09-16},
	booktitle = {Proceedings of {ECAI} 2000 {Workshop} on {Current} {Issues} in {Spatio}-{Temporal} {Reasoning}},
	author = {Koychev, Ivan},
	year = {2000},
}

@article{maloof_incremental_2004,
	title = {Incremental learning with partial instance memory},
	volume = {154},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370203001498},
	doi = {10.1016/j.artint.2003.04.001},
	language = {en},
	number = {1-2},
	urldate = {2021-09-17},
	journal = {Artificial Intelligence},
	author = {Maloof, Marcus A. and Michalski, Ryszard S.},
	month = apr,
	year = {2004},
	pages = {95--126},
	annote = {168 cites. "Sequel" to Selecting Examples for Partial Memory Learning, also by Maloof \& Mihalski. In the previous one they used batch learning algorithms, now they extend to incremental algorithms. Conducts empirical comparison with stagger concepts to AQ11 and GEM, as previously partial memory reduces accuracy a bit but saves a lot of memory. Additionally, this paper has a good summary and bibliography of memory schemes of adaptive learners.},
}

@inproceedings{maloof_method_1995,
	address = {Herndon, VA, USA},
	title = {A method for partial-memory incremental learning and its application to computer intrusion detection},
	isbn = {978-0-8186-7312-2},
	url = {http://ieeexplore.ieee.org/document/479784/},
	doi = {10.1109/TAI.1995.479784},
	urldate = {2021-09-17},
	booktitle = {Proceedings of 7th {IEEE} {International} {Conference} on {Tools} with {Artificial} {Intelligence}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Maloof, M.A. and Michalski, R.S.},
	year = {1995},
	pages = {392--397},
	annote = {26 cites. Maloof \& michalski partial memory applied to intrusion detection. Again, as result, compared to batch learners there is better learning speed and memory efficiency at the expense of prediction accuracy.},
	file = {Submitted Version:/home/riikoro/Zotero/storage/TRNKBEMU/Maloof and Michalski - 1995 - A method for partial-memory incremental learning a.pdf:application/pdf},
}
