SECT 2.1 neural nets & approaches intro
outline

start: nonestablished terminology, terms are chosen.
terms
the method: neural networks. gradient descent. recurrent + deep net meaning. trained using batches (batch more clearly in systems, move here?)

	problems chek
		concept drift
		subtypes: gradual/abrupt, real/virtual
		delayed feedback

	ml paradigms chek
	    
		approaches that are not this: transfer learning is changing learning problem, lifelong learning is dnn changing problem or environment, adaptive learning usually classification / regression adjustment to concept drift. continual learning = methods that enable longer-term operation. online and incremental learning mean that not all data is known at first, our case can be interpreted as both of them but continual will be used.

		
note
% in case a neural networks introduction is needed: \cite{ben-nun_demystifying_2019} sect 4.1 has a good description. include here intro to neural nets & rnn. move before sect 2.1 2.1 assumes nets introduced. introduce stochastic gradient descent, kungfu is a possible ref... also\cite{szeEfficientProcessingDeep2017} has a good intr

clarify
	we deal with global/partial gradual + partial abrupt concept drift
	we do continual learning with delayed labels to cope with concept drift

SECT 2.2: systems intro

Best practices in big data systems design: organization and principles
ingressi: miksi tää on tässä, mitä tullaan kertoo
luettele bd systemin yleiset osat: data ingestion, prep, storage, trianing, registry, inference

esittele bd systemin yleiset osat osa kerrallaan

	model development workflow is omitted, assumption: model is developed. (development lifecycle is nontrivial for organization but is out of scope) assumption: model had ready is a neural net, affecs training
	DATA INGESTION
	description of taking in sensor data
	batch and stream: what are they (batch as collected chunk of stream)
	the general schemes: lambda and kappa (shortly)

	DATA PREPARATION
	what it is: existing text is good
	include: data needs to be validated (from mlops guide)

	DATA STORAGE
	common ways of storing
	note on what is able to store: 
		not like dump all there and use when you have time
		memory is very scarce: only a few days data can be saved, one-pass data is common practice in streaming
		
	MODEL TRAINING
	intro to federated learning
	(eg DATE n kung fu papers (1st & 2nd from lucy) state that in ML systems distribution is of high importance)
	
	MODEL EVALUATION & VALIDATION
	here models are made sure that they work. like model testing phase

	MODEL REGISTRY
	quick naming: the storage of trained models, memory-wise small

	MODEL INFERENCE
	mention the fact that many devices are possible, this is big iot research field

näytä kuva: the components are summarized in Figure...

kerro asiat joita käsitellään: trigger = under which conditions retraining happens
training: the actual retraining

näytä kuva: tää kuva visualisoi missä osassa järjestelmää ollaan (google mlops se toinen kuva). sano et tehään model maintaining to enable continual learning!

kerro bestpracticeist: simple mature automated (automl mlops)

sect conclusion

BATCH LEARNING INGRESSI

the question here is a more general one: how to train efficiently

Recurrent neural networks can be either trained from scratch or fed with new data as such, model does not need to be 're-feeding' capable

 
states that dnn training can take hours to days. This means that in order to meet decent update cycle times, optimization is necessary. Also: in training speedup is nice, but lots of research focuses on being energy efficient as usually in inference the resources are most constrained

the time demands of the system are so large that the 100s of millisecs for moving stuff to the cloud is irrelevant -> training can be conducted in the cloud -> edge device resource constraints do not play a role

most expensive parts for rnns: memory accesses and data moving (nykynen versio ei mainitse tätä)

approaches to discuss: large batch training, distribution and parallelization, optimized computational primitives (matrix multiplication?), reduced precision networks (under 32b numbers up to binary nets)

methodology note: the approaches chosen here are those that are the most mature. There is a lot of other research going on as well in the field but these were seen as the most mature for real world implementation. a crucial matter is that many methods are tested on different learning tasks. Also only approaches that suit recurrent neural networks are considered.

DRIFT DETECTION

% outline / what i wanna say here....
% intro points: global gradual or abrupt partial is important, gradual is bad for detection
% methods: windowing comparison, statistics: do two samples come from the same distribution?, ddm eddm fhddm and other model performance based: slow but popular
% my argument: abrupt needs to be detected in a federated manner
% so statistics need to be used as they are mem efficient and fast
% for gradual something feedback based can be useful but likely is not useful.

AUTOML

% structure: 1. state of literature (little to no research)
% 2. existing possibilities: automated pipeline optimization and automated adaptation strategy choice
% both of these: first makes full recovery possible but is quite immature, second requires to have the mechanisms. further improvements but not the main ones
% other areas that have works: preprocessing automation can enhance simplicity


